# 第二部分 分布式系统

​	没有分布式系统，我们将无法拨打电话、转账或远距离交换信息。我们每天都在使用分布式系统。有时候，即使没有明确说明，任何客户端/服务器架构的应用程序其实都是分布式系统。

​	对于许多现代软件系统，垂直扩展(将软件运行在更大更快的机器上，配备更多的CPU、RAM或更快的磁盘)是不现实的。更大的机器也更贵，更难以置换，而且可能需要特殊的维护。一个替代选项是<u>水平扩展：将软件运行在多个用网络相连的机器上，而在逻辑上视为单个实体</u>。

​	分布式系统有各种规模，少则几台机器，多则上百台机器。系统参与者的特性也各不相同，可以是手持式或传感器设备，也可能是高性能计算机。

​	数据库系统运行在单个节点上的时代已经过去很久了，大多数现代数据库系统拥有多个以集群方式相互连接的节点，以增加存储容量、提升性能和增强可用性。

​	尽管某些分布式计算的理论突破不是新生事物，但大部分的实际应用还是在近期才出现的。今天，这一主题越来越受到人们的关注，我们也看到更多的相关研究与新的发展正在进行。

---

+ 第二部分基本定义

  ​	分布式系统有若干个参与者(participant，有时也称为进程、节点或副本)，每个参与者都有其自己的本地状态。参与者通过在通信链路上交换消息来进行彼此间的通信。

  ​	进程可以用时钟来获取时间，它可以是逻辑的，也可以是物理的。<u>逻辑时钟是用单调递增的计数器实现的。物理时钟也被称为挂钟(wall clock)，它与物理世界的时间概念紧密相连，可以通过进程本地的方式(例如通过操作系统)获取到</u>。

  ​	说到分布式系统就不得不提到一点：系统中的各个部分彼此分开放置，这本身就带来了很大的困难。远程进程间的通信链路可能既慢又不可靠，这导致确定远程进程的状态变得更复杂。

  ​	**分布式系统领域中的大多数研究都与"没有什么是完全可靠的"这一事实有关：通信信道可能会延迟、乱序甚至无法传递消息；进程可能会暂停、变慢、崩溃、失控或突然停止响应**。

  ​	并行编程和分布式编程领域中有许多共同主题，因为CPU也可以看作一个具备通信链路、进程和通信协议的微型的分布式系统。你将在11.5节中看到分布式编程和并发编程的许多相似之处。但是多数的原语无法在二者之间直接复用，这是因为远程通信的成本要大得多，并且通信链路和进程是不可靠的。

  ​	为了克服分布式环境的困难，我们需要用到一类特殊的算法——分布式算法，它定义了本地和远程的状态以及执行的概念，即便在不稳定的网络或发生组件故障的情况下也能工作。为了解释这些算法，我们会用到术语状态和步骤(或阶段， phase)，并描述它们之间的转换(transition)。毎个进程都在本地执行算法步骤，而本地执行以及进程之间的交互构成了分布式算法。

  ​	分布式算法描述了本地的行为和多个独立节点的交互过程。节点通过相互发送消息进行通信。算法定义了参与者的角色、要交换的消息、状态、转换、执行步骤、传递介质的特性、时序假设、故障模型，以及其他描述进程和进程间交互的特性。

  ​	分布式算法有很多不同的用途:

  + **协调**

    监督若干工作者的动作和行为的进程。

  + **合作**

    多个参与者互相依靠，共同完成任务。

  + **分发**

    进程相互配合，将信息快速而可靠地分发给感兴趣的进程。

  + **共识**

    在多个进程间达成共识。

  ​	本书中，我们将站在使用的角度讨论算法，相比纯学术的材料，我们更倾向于实用的方法。首先，我们会介绍所有必要的抽象，包括进程和它们间的联系，并逐步构建出更复杂的通信模式。我们将会从UDP开始：使用UDP时，发送者无法确认它的消息是否已送达目的地。最终，在系统中达成共识，即多个进程都认同某个特定值。

# 8. 简介与概述

## 8.1 并发执行

​	一旦两个执行线程都能访问变量，除非我们在线程间同步这些步骤，否则这些并发步骤的执行结果是无法预知的。

​	即便仅在单个节点上，我们就已经遇到了分布式系统中的第一个问题：并发。每个并发程序都具有分布式系统的某些特性。线程访问共享状态，在本地执行一些运算，再将结果传回共享变量。

​	为了精确定义执行历史并减少可能的结果数量，我们需要一致性模型。一致性模型描述并发执行的过程，并且确定了运算执行以及对其他参与者可见的顺序。使用不同的一致性模型，我们可以约束或放松系统可能的状态数量。

​	分布式系统和并发计算在术语和学术研究上有许多重叠之处，但也存在一些差异。并发系统中存在共享内存，进程可以用它来交换信息。在分布式系统中，各个进程拥有自己的本地状态，参与者之间通过传递消息进行通信。

> **并发与并行**
>
> ​	我们经常互换使用并发和并行计算这两个术语，但是这两个概念在语义上有细微的差异。当两个步骤序列并发执行时，二者都在进行中，但任意时刻都只有其中一个在执行。当两个步骤序列并行执行时，它们的步骤可以(在某一时刻)同时执行。并发的操作时间上存在重叠，而并行的操作由多个处理器执行[WEIKUM01]
>
> ​	Erlang编程语言的创建者 Joe Armstrong举过一个例子：并发执行就像一台咖啡机前排了两队，而并行执行就像两台咖啡机前排了两队。即便如此，绝大部分资料都用术语"并发"来描述拥有多个并行执行线程的系统，而"并行"这个词则很少见。

---

+ 分布式系统中的共享状态

  ​	我们可以尝试在分布式系统中引入共享内存的概念，例如，单一信息源(比如数据库)。

  ​	即使我们解决了并发访问的问题，我们依然无法保证所有进程都是同步的。

  ​	为了访问数据库，进程需要通过通信介质发送和接受消息，以查询或修改状态。但是，如果一个进程很久都没有从数据库得到响应会如何？为了回答这个问题，我们首先要定义什么是很久。为此，必须<u>从同步性的角度来描述系统：通信是否是全异步的？是否存在某些时序假设？如果存在的话，这些时序假设将允许我们引入操作超时和重试机制</u>。

  ​	我们无从知晓数据库没有响应是因为过载、不可用、响应太慢还是网络问题。这描述了崩溃的本质——进程可能以各种方式崩溃：可能因某种原因无法继续执行后面的算法步骤；可能遇到了临时性的故障；也可能是消息丟失。我们需要定义一个**故障模型**并描述故障可能发生的方式，然后再决定如何处理它们。

  ​	<u>如果系统在故障发生时仍然能继续正常运行，我们将这样的特性称为**容错性**</u>。故障是不可避免的，所以我们需要构建出具有可靠组件的系统。消除单点故障，比如前文提到的单节点数据库，可能是我们朝此方向迈出的第一步。我们可以引入一些冗余，增设备份数据库。然而这就引出了另一个问题：如何使共享状态的多个副本保持同步?

  ​	到目前为止，在我们这个简单系统中引入共享状态所带来的问题比答案还多。现在我们知道，共享状态不像引入数据库那样简单，还必须采取更细化的方法，即基于消息传递来描述各个独立进程之间的交互。

## 8.2 分布式计算的误区

​	理想情况下，当两台计算机在网络上通信时，一切都能正常工作：进程开启一个连接、发送数据、收到响应，每个人都很开心。但是假设所有操作总会成功并且没有任何错误是很危险的，因为当某些东西出问题时，我们的假设也就不成立了，那时系统的行为将变得难以预测。

​	大多数时候，假设网络可靠是合理的。网络至少在一定程度上可靠才能有用。我们都曾经历过这样的情况，当我们尝试连接到远程服务器时，却收到了一个"网络不可达"的错误。即使能建立连接，一个成功的初始连接也无法保证这条链路是稳定的，连接随时可能中断。消息可能送达了对端，但对端的响应却可能丢失了，也有可能在对端的响应发送之前连接就中断了。

​	网络交换机会有故障，电缆可能断开，网络配置也随时可能发生变化。我们构建系统时需要适当地处理所有这些情况。

​	连接可以是稳定的，但我们不能期望远程调用能像本地调用一样快。我们应尽可能少地对延迟做出假设，并且永远不要假设延迟为零。一条消息要想到达远程服务器，需要穿过若干个软件层和一个物理媒介(比如光纤或电缆)，所有这些操作都不是瞬间完成的。

​	Michael Lewis在他所著的书Flash Boys(Simon and Schuster公司出版)中讲述了这样个故事，公司花费数百万美元把延迟降低几毫秒，从而能比竞争对手更快地访问交易所。这是一个把延迟作为竞争优势的绝佳例子，然而值得一提的是，根据其他一些研究，比如文献[BARTLETTI6]，过时报价套利(通过比竞争对手更快地得知价格并执行交易来获取利润)并不能使快速交易者从市场中获利。

​	从上述教训当中学习，我们增加了重试和重连机制，并去掉了关于瞬间执行的假设，但是事实证明这还不够：**当我们增加消息的数量、发送速率和大小，并向现有网络中添加新的进程时，我们不应该假设带宽是无限的**。

> 1994年， Peter Deutsch发布过一个如今很有名的断言列表，标题为"分布式计算的误区"，描述了分布式计算中易被忽视的一些方面。除了网络可靠性、延迟和带宽假设，它还提到了其他问题，比如，网络的安全性、可能存在的攻击者、有意或无意的拓扑变化都可能打破我们的一些假设，这些假设包括：某一资源存在性和所在位置，网络传输所消耗的时间和资源，以及最后——存在一个拥有整个网络的知识和控制权的权威个体。

​	Deutsch的列表可以说非常详尽，但它侧重于通过链路传递消息时可能出错的地方。这些担忧是合理的，而且描述了最通用、最底层的复杂性，但不幸的是，在设计和实现分布式系统时，我们还做出了很多其他假设，这些假设也可能在运行中导致问题。

### 8.2.1 处理

​	在远程进程响应刚刚收到的消息之前，它还需要在本地执行一些工作，因此我们不能假定处理是瞬时完成的。只考虑网络延迟还不够，因为远程进程执行的操作也不是立即完成的。

​	此外，我们还无法保证消息送达后会立刻被处理。消息可能会进入远程服务器的等待队列中，等到所有更早到达的消息处理完后才被处理。

​	节点可能相距很近，也可能很远，各节点可能有不同的CPU、内存和磁盘配置，可能运行不同的软件版本和配置。我们不能期望它们以相同的速度处理请求。<u>如果完成一项任务需要等待几个并行工作的远程服务器响应，则**整个执行的完成时间取决于最慢的服务器**</u>。

​	与普遍存在的看法相反，队列容量并非是无限的，堆积更多的请求不会对系统有任何好处。当生产者产生消息的速度大于消费者能够处理的速度时，我们可以使用**背压(backpressure)**策略减慢生产者的速度。背压是分布式系统中人们了解和应用最少的概念之一，通常是事后才建立，而不是将其视为系统设计必需的一个组成部分。

​	尽管增加队列容量听起来像是个好主意——可以帮助我们管道化、并行化以及有效地调度请求，但是，如果消息仅仅是停在队列中等待处理，什么也不会发生。**增大队列大小可能对延迟产生负面影响，因为这并不会改善处理速度**。

​	通常，进程本地队列是用于实现以下目标：

+ **解耦**

  使接收和处理在时间上分开，并各自独立发生。

+ **流水线化**

  不同阶段的请求由系统中独立的部分处理。负责接收消息的子系统不用阻塞到上条消息处理完成。

+ **吸收瞬时突发流量**

  系统负载可能经常变化，但是请求到达的间隔时间对负责处理请求的组件是隐藏的。总体的系统延迟会由于排队而增加，但这通常仍比响应失败并重试请求更好。

​	队列大小取决于工作负载和应用程序。对于相对稳定的工作负载，我们可以通过测量任务处理时间以及各任务的平均排队时间来确定队列大小，从而确保在提升吞吐量的同时，延迟仍保持在可接受的范围内。在这种情况下，队列大小相对较小。对于不可预测的工作负载，可能会出现任务提交的突发流量，这时队列大小也应当考虑突发流量和高负载。

​	即使远程服务器可以快速地处理请求，也并不意味着我们总是能获得正面的响应。它也可能回应一个失败：无法进行写操作、要查找的值不存在或是触发了bug。总之，即使是最顺利的情况也需要我们的关注。

### 8.2.2 时钟和时间

​	假设不同的远程计算机上的时钟都同步也很危险。再加上延迟为零以及处理是瞬时的这些假设，将会导致不同的特质，尤其是在时序和实时数据处理中。例如，当从时间感知不同的参与者收集和聚合数据时，你必须了解它们之间的时间漂移并相应地对时间进行归一化，而不是依赖源时间戳。除非使用特殊的髙精度时间源，否则不能依赖时间戳进行同步或排序。当然，这并不意味着我们完全不能或不该依赖时间：说到底，<u>任何同步系统都依靠本地时钟实现超时</u>。

​	**我们必须始终注意进程之间可能存在的时间误差，以及传递和处理消息所需的时间**。例如，Spanner(参见13.5节)使用特殊的时间API，该API返回时间戳和不确定性界限以施加严格的事务顺序。一<u>些故障检测算法依赖于共享的时间概念，要求时钟漂移始终在允许的范围内才能确保正确性</u>[GUPTA01]。

​	除了分布式系统中的时钟同步非常困难之外，当前时间也在不断变化：你可以从操作系统请求当前的POSIX时间戳，并在执行几个步骤后请求另一个当前时间戳，两次结果是不同的。尽管这是一个明显的现象，但是了解时间的来源以及时间戳捕获的确切时刻至关重要。

​	了解时钟源是否是单调的(即永远不会后退)，以及与调度时间相关的操作可能偏移多少，可能也会有所帮助。

### 8.2.3 状态一致性

​	之前说到的假设大多属于"几乎总是错的"一类，但是，还有一些假设最好归入"并非总是对的"一类。这类假设帮助我们走思维捷径，通过以特定方式思考来简化模型，忽略某些棘手的边缘情形。

​	分布式算法并不总是保证状态严格一致。<u>一些方法具有较宽松的约束，允许各副本之间的状态存在分歧，并依赖冲突解决(检测和解决系统内分歧状态的能力)和读取时数据修复(读取期间，当各副本响应不同结果时，使副本恢复同步的能力)</u>。有关这些概念的更多信息参见第12章。假定状态在节点间完全一致可能会导致难以察觉的bug。

​	最终一致的分布式数据库可能具有这样的逻辑：读取时通过查询Quorum的节点来处理副本不一致，但是假定数据库表结构和集群视图是强一致的。除非我们确保这些信息的一致性，否则依赖该假设可能会造成严重的后果。

​	例如， Apache Cassandra曾有一个bug，其原因是表结构变更在不同时刻传播到各个服务器。如果在表结构传播过程中尝试从数据库读取数据，则可能会读到损坏的数据，因为一台服务器以某种表结构进行编码，而另一台服务器使用不同的表结构对其进行解码。

​	另一个例子是由环的视图分歧引起的bug：如果一个节点假定另一个节点保存了某个键的数据记录，但另一个节点具有不同的集群视图，此时读写数据可能会导致数据记录被错误放置，或是获得一个空的响应，虽然数据实际上好端端地存放在另一个节点上。

​	即使完全的解决方案成本很高，我们也最好事先考虑各种可能的问题。通过了解和处理这些情况，你能以更自然的方式解决问题，比如内置防护措施或修改设计。

### 8.2.4 本地和远程执行

​	将复杂性隐藏在API内部可能很危险。例如，对于本地数据集上的一个迭代器，即使你对存储引擎不熟悉，也可以合理地推测内部行为。理解远程数据集上的迭代过程则是个完全不同的问题：你需要理解一致性、传递语义、数据协调、分页、合并、并发访问含义以及许多其他事情。

​	简单地将两者隐藏在同一个接口后，即便有用，也可能会产生误导。调试、配置和可观察性可能需要额外的API参数。我们应该始终牢记，**本地执行和远程执行是不同的**[ WALDO96]。

​	**隐藏远程调用最明显的问题是延迟：远程调用的成本比本地调用高很多倍，因为它涉及<u>双向网络传输、序列化/反序列化</u>以及许多其他步骤**。交错使用本地调用和阻塞的远程调用可能会导致性能下降和预期之外的副作用[VINOSKIO08]。

### 8.2.5 处理故障的需要

​	刚开始构建系统的时候，我们可以假设所有节点都可以正常工作，但如果总是这么想就很危险了。在长时间运行的系统中，节点可能会关机维护(通常会有个优雅关闭的过程)或因为种种原因(例如软件问题、内存耗尽(out-of-memory killer [KERRISK10])、运行时bug、硬件问题等)而崩溃。<u>进程会发生故障，而你能做的最好的事情就是做好准备并知道如何处理它们</u>。

​	如果远程服务器没有响应，我们并不总是知道确切的原因。这可能是由系统崩溃、网络故障、远程进程或中间链路太慢导致的。<u>一些分布式算法使用**心跳协议**和**故障检测**机制来确定哪些参与者还活着且可达</u>。

### 8.2.6 网络分区和部分故障

​	<u>当两个或更多服务器无法相互通信时，我们称这种情况为**网络分区**</u>。Seth Gilbert和Nancy Lynch在 Perspectives on the CAP Theorem[GILBERT12]中区分了以下两种情况：两个参与者无法相互通信；几组参与者彼此隔开，无法交换消息并继续运行算法。

​	网络的总体不可靠性(数据包丢失、重传、延迟难以预测)令人烦恼但尚可容忍，而网络分区则会造成更多的麻烦，因为各个独立的分组可以继续执行并产生冲突的结果。**网络链路的故障也可能是不对称的：消息仍然能从一个进程传递到另一个进程，反之则不行**。

​	为了构建在一个或多个进程出现故障的情况下仍健壮的系统，我们必须考虑**部分故障**的情况[TANENBAUM06]，如何让系统在部分不可用或运行不正常的情况下仍能继续工作。

​	故障很难检测，并且在系统的不同部分看来，不总是以相同的方式可见。设计高可用性系统时，我们应该始终考虑边缘情形：如果我们确实复制了数据却没有收到确认该怎么办？要重试吗？在发送了确认的节点上，数据仍可用于读取吗？

​	**墨菲定律告诉我们故障一定会发生**。编程界又补充道，故障将以最坏的方式发生。因此，作为分布式系统工程师，我们的工作是尽可能减少可能出现错误的场景，并为故障做好准备——包括这些故障可能导致的破坏。

​	避免一切故障是不可能的，但我们仍可以构建一个弹性的系统，使之在故障出现时仍然能正常运行。**<u>设计应对故障的最佳方式是进行故障测试</u>**。我们无法考虑清楚毎种可能的故障场景，并预测多个进程的行为。最好的解决方法就是通过测试工具来制造网络分区、模拟比特位腐烂[GRAY05]、增加延迟、使时钟发生偏移以及放大相对处理速度。现实世界中分布式系统的设置可能是对抗性的、不友好的，甚至是“有创造性的”(然而以非常敌对的方式)，因此测试工作应当尝试覆盖尽可能多的场景。

> 过去几年中出现了一些开源项目，它们能帮助我们构造出各种故障场景。
>
> + Toxiproxy用于<u>模拟网络问题</u>：<u>限制带宽、引入延迟、超时</u>等。
> +  Chaos Monkey的方法更为激进，它通过<u>随机关闭服务使工程师直面生产环境故障的风险</u>。
> + CharybdeFS<u>模拟文件系统及硬件错误与故障</u>。你可以用这些工具来测试软件以确保在这些故障出现时软件仍能正确工作。 
>
> + CrashMonkey是一个与文件系统无关的记录-重放-测试框架，用于测试持久性文件的数据及元数据一致性。

​	设计分布式系统时，我们必须认真考虑容错性、弹性，以及可能的故障场景和边缘情形。类似于"足够多的眼睛，就可让所有问题浮现"，我们可以说足够大的集群最终定会命中所有可能的问题。与此同时，只要有足够多的测试，我们最终能够发现每个存在的问题。

### * 8.2.7 级联故障

​	我们做不到总是完全隔离故障：**被高负载压垮的进程会增加集群其余部分的负载，从而使其他节点更有可能发生故障。级联故障能够从系统的一部分传播到另一部分，扩大了问题的范围**。

​	有时，级联故障甚至可能来源于完全善意的目的。例如，某个节点离线了一段时间，因而没有接收到最近的更新。当它恢复在线时，乐于助人的其他节点希望帮助它追赶上最近的变化，于是开始向它发送缺失的数据，而这又导致网络资源耗尽，或是导致该节点启动后短时间内再次发生故障。

> ​	为了防止系统的故障扩散并妥善处理故障场景，我们可以使用**断路器(或熔断机制)**。在电气工程中，断路器可通过中断电流来保护昂贵且难以更换的部件，使其免受电流过载或短路的影响。在软件开发中，熔断机制会监视故障，并使用**回退(fallback)**机制保护整个系统：避免使用出故障的服务，给它一些时间进行恢复，并妥善处理失败的调用。

​	<u>当与某一台服务器的连接失败或服务器没有响应时，客户端将开始循环重连。那时候，过载的服务器已经难以应付新的连接请求，因而客户端的循环重试也无济于事。为了避免这一情况，我们可以使用**退避(backoff)**策略，客户端不要立即重试，而是等待一段时间</u>。

​	**退避通过合理安排重试、增加后续请求之间的时间窗口来避免问题扩大**。

​	退避用于增加单个客户端的请求间隔。但是，<u>使用相同退避策略的多个客户端也会产生大量负载</u>。为了防止多个客户端在退避期之后同时重试，我们可以引入**抖动(Jitter)**。<u>抖动在退避上增加了一个小的随机时间间隔，从而降低了多个客户端同时醒来并重试的可能性</u>。

​	<u>硬件故障、比特位腐烂和软件错误都会导致数据损坏，而损坏的数据会通过标准的传递机制传播</u>。如果没有适当的验证机制，系统可能将损坏的数据传播到其他节点，甚至可能覆盖未损坏的数据记录。为了避免这一情况，我们应该采用**校验和(checksum)**以及验证机制，来验证节点之间交换的任何内容的完整性。

​	**通过计划和协调执行可以避免过载和热点问题。<u>相比于让各个对等节点独立执行操作步骤，我们可以用协调器来依据可用资源准备一份执行计划，并根据过去的执行数据来预测负载</u>**。

​	总之，我们应该始终考虑这样的情形：系统某一部分的故障可能导致其他地方也出现问题。我们应该为系统装备上**熔断**、**退避**、**验证**和**协调**机制。<u>处理被隔离的小问题总比从大规模故障中恢复更简单</u>。

​	我们用整整一节讨论了分布式系统中的问题和潜在的故障场景，但是我们应当将其视为警告，而不是被它们吓跑。

​	了解什么会出问题，并仔细设计和测试我们的系统，可以让它更健壮、更具弹性。了解这些问题可以帮助你在开发过程中识别、发现潜在的问题根源，也能帮助你在生产环境中调试

## * 8.3 分布式系统抽象

​	讨论编程语言时，我们使用<u>通用术语</u>并用<u>函数</u>、<u>运算符</u>、<u>类</u>、<u>变量</u>和<u>指针</u>来定义我们的程序。通用的词汇可以帮助我们避免每次都为了描述某些东西而发明新词。我们的定义越精确、越没有歧异，听众也就越容易理解。

​	在开始学习算法之前，我们首先要了解分布式系统中的词汇：这些定义你会经常在演讲书籍和论文中遇到。

---

+ **链路**

  ​	网络是不可靠的：消息会丢失、延迟或被打乱。记住这一点之后，我们来尝试构建几种通信协议。我们从最不可靠的协议开始，确定它们可能处于的状态，然后找出可以为协议增加的东西使它提供更好的保证。

  **公平损失链路**

  ​	我们可以从两个进程开始，它们之间以**链路**相连。进程可以相互发送消息。任何通信介质都是不完美的，消息可能丢失或延迟。

  ​	看看我们能得到什么样的保证。消息M被发送之后(从发送方的角度来看)，它可能处于以下状态之一：

  + 还未送达进程B(但会在某个时间点送达)
  + 在途中丢失且不可恢复
  + 成功送达远程进程

  ​	注意，发送方没有任何方法确定消息是否已经送达。在分布式系统的术语中，这种链路称为**公平损失(fair-loss)**。这种链路具有以下属性：

  **公平损失**

  ​	如果发送方和接收方都是正确的，且发送方无限多次重复发送，则消息最终会被送达。

  **有限重复**

  ​	发送的消息不会被送达无限次。

  **不会无中生有**

  ​	链路不会自己生成消息。换句话说，它不会传递一个从未发送过的消息。

  ​	公平损失链路是一种很有用的抽象，它是构建具有更强保证的通信协议的基石。我们可以假设该链路不会在通信双方之间*系统性地*丢弃消息，也不会创建新消息。但与此同时，我们也不能完全依靠它。这可能让你想起了用户数据报协议(UDP)，UDP允许我们从一个进程发送消息到另一个进程，但在协议层面上不提供可靠的传输语义。

  **消息确认**

  ​	为了改善这一情况、更清晰地获得消息状态，我们可以引入**确认(acknowledgment)机制**：接收方通知发送方消息已送达。为此，我们需要双向通信信道，并增加一些措施以区分不同的消息，例如序列号——单调递增的唯一消息标识符。

  > ​	每个消息只要有唯一标识符就足够了。序列号只是唯一标识符的一种特殊情况，即使用计数器来获取标识符，从而实现唯一性。<u>当使用哈希算法来唯一地标识消息时，我们应当考虑可能的冲突，并确保能消除歧义</u>。

  ​	现在，进程A可以发送消息M(n)，其中n是单调递增的消息计数器。B收到消息后立即向A发送确认ACK(n)。

  ​	确认消息，就像原始消息一样，也有可能在途中丟失。消息可能处于的状态数会稍有变化。在A收到确认之前，该消息仍处于我们前面提到的三种状态之一，但是，一旦A收到确认，就可以确信该消息已送达B。

  **消息重传**

  ​	增加确认机制仍不足以保证通信协议完全可靠：发送的消息仍可能会丢失，远程进程也可能在确认之前发生故障。为了解决该问题并提供送达保证，我们可以尝试**重传**(retransmit)。重传是指发送方重试可能失败的操作。我们之所以说可能失败，是因为发送方并不能真的知道有没有失败，因为我们要讨论的链路<u>不使用确认机制</u>。

  ​	进程A发送消息M之后，它将等到超时T被触发，然后尝试再次发送同一条消息。假设进程之间的链路完好无损，进程间的网络分区不会无限持续下去，并且并非所有数据包都丢失，我们可以认为，从发送方的角度看，消息要么尚未送达进程B，要么已经成功送达。由于A一直在尝试发送消息，可以认为传输过程中不会发生不可恢复的消息丢失。

  ​	在分布式系统的术语中，这种抽象称为**顽固链路(stubborn link)**。之所以称为顽固，是因为发件人会<u>无限期地反复发送消息</u>，但是，由于这种抽象非常不切实际，因此我们需要将重试与确认结合起来。

  **重传的问题**

  ​	每当我们发送消息时，在收到远程进程的确认之前，我们无从得知消息的状态：可能已被处理，可能马上就要处理，也可能已经丟失，甚至可能在收到消息之前远程进程就崩溃了——上述的任意状态都是可能的。我们可以重试操作、再次发送消息，但这可能导致消息重复。<u>只有当我们要执行的操作是幂等时，处理重复消息才是安全的</u>。

  ​	<u>幂等(dempotent)的操作可以执行多次而产生相同的结果，且不会产生其他副作用</u>。例如，服务器关机操作可以是幂等的，第一次调用将发起关机，而所有后续调用都不会产生任何其他影响。

  ​	如果毎个操作都是幂等的，那我们可以少考虑一些传递语义，更多地依赖重传来实现容错，并以完全反应式的方式构建系统：为某些信号触发相应的操作，而不会引起预期之外的副作用。但是，操作不一定是幂等的，简单地假设它们幂等可能会导致集群范围的副作用。例如，向客户的信用卡收费不是幂等操作，绝对不可以重复收费多次。

  ​	在存在部分故障和网络分区的情况下，幂等性尤其重要，因为我们无法总是确定远程操作的确切状态——是成功还是失败，还是会马上被执行——我们只能等待更长的时间。<u>**保证毎个操作都是幂等的是不切实际的，因此我们需要在不改变实际操作语义的情况下，提供与幂等性等价的保证**。为此，我们可以使用去重来避免多次处理消息</u>。

  **消息顺序**

  ​	不可靠的网络给我们带来了两个问题：一是消息可能会乱序到达；二是由于重传某些消息可能会多次送达。我们已经引入了序列号，利用这些消息标识符我们可以在接收方确保先进先出(FIFO)的顺序。由于每条消息都有一个序列号，因此接收方可以跟踪下列信息：

  + n<sub>consecutive</sub>表示最大连续序列号：所有小于或等于该序列号的消息都已经收到，这些消息可以按顺序放到正确的位置上。
  + n<sub>processed</sub>表示最大已处理序列号：所有小于或等于该序列号的消息都已经按照原来的顺序被处理。此序列号可以用于去重。

  ​	如果收到的消息序列号不连续，接收方会将其放入重新排序缓冲区。例如，它在接收到序列号为3的消息后收到消息5，那我们就知道4还是缺失的，因此我们将5放在一旁，直到4到来，然后就能构造出原本的消息顺序。<u>由于通信构建在公平损失链路之上，可以认为n<sub>consecutive</sub>和n<sub>max_seen</sub>之间的消息最终一定会送达</u>。

  ​	接收方可以安全地丢弃收到的序列号小于等于n<sub>consecutive</sub>的消息，因为这些消息确定已经送达了。去重的工作原理是检查带有序列号n的消息是否已被处理(已被传给网络栈的更上层)，丢弃已处理的消息。

  ​	去重的工作原理是检査带有序列号n的消息是否已被处理(已被传给网络栈的更上层)，丢弃已处理的消息。

  ​	在分布式系统的术语中，这种类型的链路称为**完美链路**，它提供以下保证[CACHIN11]：

  **可靠传递**

  ​	正确的进程A发送一次到正确的进程B的每个消息**最终**都会被传递。

  **没有重复**

  ​	消息不会被传送多次。

  **不会无中生有**

  ​	与其他种类的链路一样，它只能传递实际由发送者发送过的消息。

  ​	这可能会让你想起TCP协议(但是，TCP仅在单个会话内保证可靠传递)。当然，上述模型仅仅是一种用于说明原理的简化表示。TCP中处理消息确认的模型更为复杂，它按组进行确认以减少协议层面的开销。另外，TCP具有选择性确认、流控、拥塞控制错误检测等很多其他功能，这些不在我们的讨论范围之内。

  **严格一次传递**

  ​	<u>关于是否可以做到严格一次传递(exactly-once delivery)这个问题已经有很多讨论。这里，语义和精确的措辞非常重要。**由于链路故障可能导致传递消息的第一次尝试无法成功，因此大多数实际的系统都采用至少一次传递(at-least-once delivery)，它确保了发送方将重试直到收到确认为止，否则就认为对方没有收到该消息**。还有一种传递语义是最多一次(at-most-once)：发送方仅仅发送消息而不期待得到任何确认</u>。

  ​	TCP协议的原理是将消息分成数据包，一个一个传输，然后在接收端将它们拼接到起。TCP可能会尝试重传某些数据包，并且可能有不止一次的传输会成功。由于TCP用序列号标记毎个数据包，即使某些数据包被发送多次，它也可以对其进行去重，确保接收方只会看到并处理一次该消息。<u>在TCP中，此保证仅对单个会话有效：如果消息被确认并处理，但是发送方在收到确认消息前连接就中断了，则应用程序并不知道此传递成功，取决于其逻辑，它可能会尝试再次发送消息</u>。

  ​	这意味着严格一次处理是个有趣的问题，因为重复的传送(或数据包传输)没有副作用，仅仅是链路尽力而为的产物。举个例子，<u>如果数据库节点仅接收到记录但还没将它持久化。在这种情况下传递已经完成了，但除非该记录可以被査到(换句话说，除非消息被传递并且处理了)，否则这次传递毫无用处</u>。

  ​	为了确保严格一次传递，各节点需要一个共同知识[HALPERN90]：每个节点都知道某件事，每个节点都知道其他所有节点也都知道这件事。<u>用简化的术语来说，**节点必须在记录状态上达成共识**：两个节点都认为该记录已经或者还未被持久化。正如本章之后会说的，这在**理论上是不可能的**，但在实践中，我们仍通过放宽协调的要求来使用这一概念</u>。

  ​	各种关于是否是严格一次发送的误解，大多是因为从不同协议和抽象层次上考虑该问题，以及对“传递”的不同定义。**要想建立可靠的链路，不可能不重复传送某些消息**。但是，<u>我们可以通过仅处理毎个消息一次并忽略重复消息，使得从发送方的角度来看是严格一次发送</u>。

  ​	现在，在建立了实现可靠通信的方法之后，我们可以继续前进，探寻实现分布式系统中进程间一致性和共识的方法。

## 8.4 两将军问题

> [【翻译】两军问题（Two Generals’ Problem） | 知研片语 (liuzhaocn.com)](https://www.liuzhaocn.com/?p=1235)

​	一个被广泛称为两将军问题的思想实验，是对分布式系统一致性的最著名的描述之一。

​	这个思想实验表明，**<u>如果链路可能发生故障并且通信是异步的，则不可能在通信的双方之间达成共识</u>**。尽管TCP具有完美链路的性质，但是务必记住：完美链路尽管被称为完美链路，并不能保证完美的传递。它们也不能保证参与方一直活着，而只关心传输本身。

​	想象现在有两支军队，分别由两位将军领导，准备进攻一座要塞城市。两支军队分别位于城市的两侧，只有在同时进攻的情况下才能获胜。

​	两位将军通过信使进行通信。他们已经制定了攻击计划，现在唯一需要达成共识的就是是否执行计划。该问题的变体包括：其中一位将军的级别较高，但需要确保攻击是有协调的；或者两位将军需要就确切时间达成共识。这些细节不会改变问题的定义：将军们需要达成一项共识。

​	将军们只需要对“他们都会发起进攻”这一事实达成共识。否则，攻击将无法成功。将军A发出一条消息MSG(N)，表明如果对方也同意的话，就在指定的时间发起进攻。

​	将军A送出信使之后，他不知道信使是否已经到达：信使可能会被抓而无法传达消息。当将军B收到消息时，他必须发送确认ACK(MSG(N))。一条消息由一方发送并由另一方确认。

​	传递确认消息的信使也可能会被抓而无法传达消息。B无从得知信使是否已成功送达确认消息。

​	为了确认这一点，B必须等待ACK(ACK(MSG(N)))，一个二阶的确认，用于确认A收到了确认。

​	**<u>无论将军们互相发送多少确认，他们始终距离安全地发起攻击还差一个ACK。将军们注定要怀疑最后一个确认消息是否已送达目的地</u>**。

​	注意我们没有做任何时序上的假设:将军间的通信是完全异步的。并没有一个上限约束将军必须在多长时间内做出回应。

## 8.5 FLP不可能定理

​	Fisher、Lynch和Paterson在论文中描述了一个著名的问题：FLP不可能问题[FISCHERI85] (FLP是作者姓氏的首字母)，论文讨论了一种共识形式：各进程启动时有一个初始值，并尝试就新值达成共识。算法完成后，所有正常进程上的新值必须相同。

​	如果网络完全可靠，很容易对特定值达成共识。但实际上，系统容易出现各式各样的故障，例如消息丟失、重复、阒络分区，以及进程缓慢或崩溃。

​	共识协议描述了这样一个系统：给定初始状态的多个进程，它将所有进程带入决定状态。一个正确的共识协议必须具备以下三个属性：

+ 一致性

  ​	协议达成的决定必须是一致的：每个进程都做出了决定且所有进程决定的值是相同的。否则我们就尚未达成共识。

+ 有效性

  ​	达成共识的值必须由某一个参与者提出，这意味着系统本身不能“提出”值。这也意味着这个值不是无关紧要(trivial)的：进程不能总是决定某个预定义的默认值。

+ 终止性

  ​	只有当所有进程都达到决定状态时，协议才算完成。

​	<u>文献[FISCHER85]假定处理过程是完全异步的，进程之间没有共享的时间概念。这样的系统中的算法不能基于超时，并且一个进程无法确定另一个进程是崩溃了还是仅仅运行太慢。论文表明，在这些假设下，不存在任何协议能保证在有限时间内达成共识。**完全异步的共识算法甚至无法容忍一个远程进程无通知地突然崩溃**</u>。

​	**<u>如果我们不给进程完成算法步骤设定一个时间上限，那么就无法可靠地检测出进程故障，也不存在确定性的共识算法</u>**。

​	但是，FLP不可能定理并不意味着我们要收拾东西回家(由于达成共识是不可能的)。**<u>它仅仅意味着我们不能总是在有限的时间内在一个异步系统中达成共识</u>**。实践中，系统至少会表现出一定程度的同步性，而要想解决共识问题还需要一个更完善的模型。

## 8.6 系统同步性

​	从FLP不可能定理中可以看出<u>**时序假设**是分布式系统的关键特征之一</u>。在异步系统中，我们不知道进程运行的相对速度，也不能保证在有限时间内或以特定顺序传递消息。进程可能要花无限长的时间来响应，而且无法总是可靠地检测到进程故障。

​	对异步系统的主要批评在于上述假设不切实际：进程不可能具有任意不同的处理速度，链路传递消息的时间也不会无限长。依赖时间能够简化推理，并提供时间上限的保证。

​	在异步模型中不一定能解决共识问题[FISCHER85]。而且，不一定能设计出高效的异步算法。<u>对于某些任务，切实可行的解决方案很可能需要依赖时间</u>[ ARJOMANDIS83]。

​	我们可以放宽一些假设，认为系统是同步的。为此我们引入了时间的概念。在同步模型下对系统进行推理要容易得多。它假定各进程的处理速度相近、传输延迟是有限的，并且消息传递不会花任意长的时间。

​	同步系统也可以表示为同步的进程本地时钟：两个进程本地时间源之间的时间差存在上限[CACHIN11]。

​	**在同步模型中设计系统可以使用超时机制**。我们可以构建更复杂的抽象，例如**领导者选举、共识、故障检测**以及基于它们的其他抽象。<u>这使得最佳情况的场景更加健壮，但是如果时序假设不成立则可能导致故障</u>。

​	例如：Raft共识算法(参见14.4节)中，可能最终有多个进程认为它们是领导者，为了解决该问题，我们强制滞后的进程接受其他进程成为领导者；故障检测算法(参见第9章)。可能会错误地将活动进程标记为故障，反之亦然。设计系统时，我们必须考虑这些可能性。

​	异步和同步模型的性质可以组合使用，我们可以将系统视为部分同步的。部分同步的系统具有同步系统的某些属性，但是消息传递、时钟漂移和相对处理速度的边界范围可能并不精确，并且仅在大多数时候成立[DWORK88]。

​	同步是分布式系统的基本属性：它对性能、扩展性和一般可解性有影响，并且有许多对系统正常工作来说是必要的因素。本书中讨论的一些算法就工作在同步系统的假设下。

## 8.7 故障模型

​	我们一直在提到故障这个词，但到目前为止，它还是一个十分宽泛的概念，可能包含多种含义。就像我们可以做出不同的时序假设那样，我们也可以假设存在不同种类的故障。故障模型准确地描述了分布式系统中的进程可能以怎样的方式崩溃，并基于这些假设来开发算法。例如，我们可以假设进程可能崩溃并且永远无法恢复，或者可以预期它将在一段时间后恢复，或者它可能会失控并且产生错误的值。

​	分布式系统中，进程互相依赖以共同执行算法，因此故障可能导致整个系统的执行错误。

​	我们将讨论分布式系统中现有的多种故障模型，例如崩溃、遗漏和任意故障。这个列表并非面面俱到，但它涵盖了在实际中的大多数重要场景。

### 8.7.1 崩溃故障

​	通常，我们期望进程正确执行算法的所有步骤。最简单的崩溃方式是进程停止执行接下来的算法步骤，并且不再发送任何消息给其他进程。换句话说，该进程崩溃了。大多数情况下，我们使用**崩溃-停止(crash-stop)**进程抽象的假设，它规定一旦进程崩溃就会保持这种状态。

​	该模型不假定该进程无法恢复，也不阻拦或试图阻止恢复。这仅仅意味着该算法的正确性或活动性不依赖于恢复过程。实际上，并没有什么东西会去阻止进程恢复、追上系统状态以及参与下一次的算法执行。

​	失败的进程无法再继续参与当前这一轮的协作。为恢复的进程分配一个新的、不同的ID不会使模型等价于崩溃-恢复模型(之后会讨论)，因为大多数算法使用预定义的进程列表，并且依据最多可容忍的故障数明确定义了故障的语义[CACHIN11]。

​	**崩溃-恢复(crash-recovery)**是另一种的进程抽象。在这个抽象中，进程停止执行算法步骤，但会在稍后恢复并尝试执行剩下的步骤。要想让恢复成为可能，需要在系统中引入持久状态以及恢复协议[SKEEN83]。允许崩溃-恢复的算法需要考虑所有可能的恢复状态，因为恢复的进程会尝试从最后一个已知的步骤开始继续执行。

​	想利用恢复的算法必须同时考虑状态和进程ID。在这种情况下，崩溃恢复也可以看作是遗漏故障的一种特殊情况，因为<u>从另一个进程的角度看，不可达的进程与崩溃再恢复的进程没什么区别</u>。

### 8.7.2 遗漏故障

​	另一个故障模式是**遗漏故障(omission fault)**。该模型假设故障进程跳过了某些算法步骤，或者无法执行这些步骤，或者执行过程对其他参与者不可见，或者无法与其他参与者通信。遗漏故障中包含了由于网络链路故障、交换机故障或网络拥塞而导致的网络分区。网络分区可以表示为单个进程或进程组之间的消息遗漏。进程崩溃可以模拟为遗漏所有该进程收发的消息。

​	如果进程的运行速度慢于其他参与者，发送响应比预期迟得多，那么对于系统的其余部分来说，这个节点看起来丢三落四的。慢节点没有完全停止，而是发送结果太慢，常常与其他节点不同步。

​	<u>如果本应执行某些步骤的算法跳过了这些步骤或者执行结果不可见时，就发生了遗漏故障</u>。例如，消息在送往接收方的途中丢失，而发送方就像消息发送成功时那样，没有再次发送而是继续运行，即使消息已经不可恢复地丟失了。遗漏故障也可能是由间歇性停顿、网络过载、队列满等引起的。

### 8.7.3 任意故障

​	最难以解决的故障种类是**任意故障**或**拜占庭故障(Byzantine fault)**：<u>进程继续执行算法步骤，但是以与违背算法的方式(例如，共识算法中的进程决定一个从未由任何参与者提出过的值)</u>。

​	此类故障可能是由于软件bug或运行不同版本算法的进程，在这种情况下，故障很容易被发现和理解。如果我们无法控制所有进程，并且其中一个进程有意地误导其他进程，则发现和理解故障会变得非常困难。

​	你可能在航空航天工业中听说过拜占庭式的容错：飞机和航天器的系统不会直接使用子部件传来的值，而是会对结果进行交叉验证。另一个广泛的应用是加密货币[GILAD17]，那里没有中央权威，节点被多方控制，并且敌对的参与者有强烈的动机通过提供错误响应来欺骗系统。

### 8.7.4 故障处理

​	我们可以通过构成进程组、在算法中引入冗余来掩盖故障：即使其中一个进程发生故障，用户也不会注意到[CHRISTIAN91]。

​	故障可能会带来一些性能损失：正常的执行依赖于进程可响应，而且系统必须回退到较慢的执行路径来处理故障和纠正错误。故障往往可以通过一些方式来避免，例如：代码审査、广泛的测试、引入超时重试机制确保消息送达，以及确保各算法步骤在本地按顺序执行。

​	我们这里介绍的大多数算法都基于崩溃-故障模型，并通过**引入冗余来解决故障**。这些假设帮助我们创造性能更好、更易于理解和实现的算法

## 8.8 本章小结

​	本章中，我们讨论了一些分布式系统的术语，并介绍了一些基本概念。我们讨论了分布式系统的固有困难和复杂性，这是由于系统组件不可靠性导致的：链路可能无法传递消息、进程可能崩溃、网络可能发生分区。

​	这些术语应该足够让我们继续讨论。本书的剩余部分将讨论分布式系统中常见的解决方案:我们将先回想下哪些地方可能会出问题，然后看看有哪些可用的选项。

# * 9. 故障检测

​	为了使系统对故障做出适当的反应，应该及时检测故障。其他进程可能联系发生错误的进程，即使它无法响应，这会增加延迟并降低整个系统的可用性。

​	在异步分布式系统中检测故障(即不做任何时序假设)是极其困难的，因为我们无法判断进程是崩溃了，还是运行缓慢而需要无限长的时间来响应。我们在8.5节中讨论了与此相关的问题。

​	诸如死亡(dead)、失效(failed)和崩溃(crashed)等术语通常用于描述完全停止执行其步骤的进程。而<u>诸如无响应(unresponsive)、有故障(faulty)和缓慢(slow)等术语用于描述可疑进程，这些进程可能实际上已经死亡</u>。

​	故障可能发生在链路层上(进程之间的消息丢失或传递缓慢)，或者发生在进程层上(进程崩溃或运行缓慢)，而缓慢可能不一定能与故障区分开来。这意味着在如下两个方面总是面临一个杈衡：

+ 将活着的进程错误地怀疑为死的(产生假阳性)
+ 推迟将无响应的进程标记为死的并期望它最终做出响应(产生假阴性)。

​	**故障检测器(failure detector)**是一个本地子系统，其负责识别故障或无法到达的进程，将它们排除在算法之外，并在维持安全性的同时保证算法的活动性。

​	<u>**活动性**和**安全性**是描述算法解决特定问题的能力及其输出正确性的属性</u>。

​	更正式地说，**活动性(liveness)是一种保证特定预期事件必须发生的属性**。例如，如果其中一个进程发生故障，则故障检测器必须检测到该故障。**安全性(safety)保证意外事件不会发生**，例如，如果一个故障检测器已经将一个进程标记为死亡，那么这个进程实际上必须是死亡的[LAMPORT77， RAYNAL99， FREILING11]。

​	从实际的角度来看，排除发生故障的进程有助于避免不必要的工作，防止错误传播和级联故障，而排除疑似故障的活动进程会降低系统可用性。

​	故障检测算法应该表现出几个基本特性。首先，每一个没有问题的成员最终都应该注意到进程故障，并且算法应该能够向前推进并最终得出结果。这种特性称为**完备性（completeness）**。

​	我们可以通过算法的效率来判断其优劣：故障检测器识别进程故障的速度有多快。另一种方法是观察算法的准确性：是否精确地检测到了进程故障。换句话说，如果一个算法错误地认为一个活着的进程发生了故障或者不能检测出实际已发生的故障，那么它就是不准确的。

​	我们可以把效率和准确度之间的关系看作是一个可调参数：一个更高效的算法可能更不准确，而一个更准确的算法通常更不髙效。**建立既准确又高效的故障检测器被证明是不可能的**。同时，<u>故障检测器是允许产生假阳性的(即，错误地将活着的进程识别为故障，或者反过来)</u>[CHANDRA96]。

​	故障检测器是许多共识和原子广播算法的必要前提和组成部分，我们将在本书后面讨论这些算法。

​	许多分布式系统使用**心跳(heartbeat)**来实现故障检测器。由于其简单性和很强的完备性，这种方法非常普遍。我们在这里讨论的算法<u>假设不存在拜占庭式故障：进程不会试图故意谎报它们自己及相邻进程的状态</u>。

## 9.1 心跳和ping

​	我们可以通过触发如下两个周期性过程之一来查询远程进程的状态：

+ 我们可以触发一个**ping**，它将消息发送到远程进程，通过在指定的时间段内是否得到响应来检查它们是否仍处于活动状态。
+ 我们可以触发一个**心跳**，即进程通过向其对等方发送消息来主动通知其仍在运行。

​	在这里我们将使用ping作为例子，使用心跳也可以解决相同的问题并产生相似的结果。

​	每个进程维护一个其他进程的列表(存活、死亡和疑似死亡)，并且用每个进程最新的响应时间对这个列表进行更新。如果一个进程在较长的时间内无法响应一个ping消息，它会被标记为疑似死亡(suspected)。

​	许多故障检测算法都是基于心跳和超时的。例如，用于构建分布式系统的流行框架Akka实现了一个截止时间故障检测器(deadline failure detector)，这一检测器使用心跳机制，如果进程在某一固定时间间隔内未能成功注册，它将报告进程故障。

​	<u>这种方法有几个潜在的缺点：它的精度依赖于对ping频率和超时的仔细挑选，并且它不能从其他进程的角度捕获进程的可见性(参见9.1.2节)</u>。

### * 9.1.1 无超时的故障检测器

​	一些算法避免依赖超时来检测故障。例如Heartbeat，一种**无超时(timeout-free)**故障检测器[AGUILERA97]，该算法仅对心跳计数并允许应用程序基于心跳计数器向量中的数据来检测进程故障。由于该算法是无超时的，因此它能在异步系统假设下运行。

​	该算法假设任意两个正确的进程用公平路径(fair path)相互连接，该路径只包含公平链路(即如果无限频繁地通过该链路发送一条消息，该消息也会无限频繁地被接收)，并且每个进程都能意识到网络中所有其他进程的存在。

​	每个进程维护一个邻居列表和与其相关联的计数器。首先，进程向邻居发送心跳消息，毎个消息都包含心跳到目前为止所经过的路径。初始消息包含路径中的第一个发件人和一个唯一标识符，该标识符可用于避免同一消息被广播多次。

​	当进程接收到新的心跳消息时，它会递增路径中所有参与者的计数器，将心跳信号发送到尚未参与的进程，并将其自身追加到路径中。进程一旦看到所有已知进程已经接收到消息(换句话说，进程ID出现在路径中)，就会停止传播该消息。

​	<u>由于消息是通过不同的进程传播的，并且心跳路径包含从相邻进程接收的聚合信息，因此即使两个进程之间的直接链路出现故障，我们也可以(正确地)将无法到达的进程标记为活动进程</u>。

​	心跳计数器表示系统的全局和归一化视图。这个视图捕获了心跳是如何在节点间传播的，让我们可以对进程进行比较。然而，<u>这种方法的一个缺点是，对心跳计数器进行解释可能相当棘手：**需要选择一个能够产生可靠结果的阈值**。除非我们能做到这一点，否则算法会错误地将活动进程标记为疑似死亡</u>。

### * 9.1.2 外包心跳

​	可扩展弱一致性感染式进程组成员协议(Scalable Weakly Consistent Infection-style Process Group Membership Protocol，SWIM)[GUPTA01使用的是另一种方法。它使用外包心跳(outsourced heartbeat)来提高可靠性，**<u>利用的是从其相邻进程的角度查看到的进程活动性(liveness)信息。这种方法不需要进程知道网络中的所有其他进程，只需要知道其连接的对等进程的子集</u>**。

​	进程P1向进程P2发送ping消息。P2不响应该消息，因此P1继续选择多个随机成员(P3和P4)发送ping消息。这些随机成员会尝试向P2发送心跳消息，如果P2响应，则将确认转发回P1。

​	这允许我们将直接和间接可达性都考虑在内。例如，如果有进程P1、P2和P3，我们可以同时从P1和P2的角度检查P3的状态。

​	通过将决策责任分布到成员组中，外包心跳可以做到可靠的故障检测。这种方法不需要向很多对等进程广播消息。由于外包心跳请求可以并行触发，这种方法可以快速收集更多关于疑似死亡进程的信息，进而让我们做出更准确的决策。

## * 9.2 phi增量故障检测器

> 某种意义上有点类似Google TCP的BBR拥塞控制算法。
>
> [谈TCP BBR拥塞控制算法_yang_oh的博客-CSDN博客_bbr拥塞控制算法](https://blog.csdn.net/u013032097/article/details/96212770)

​	phi增量(φ-accrual)故障检测器[HAYASHIBARA04]不是将节点故障视为二元判断问题(即进程只能处于两种状态：在线或宕机)，而是<u>用连续范围来捕获被监视进程崩溃的概率</u>。它的工作方式是**维护一个滑动窗口，从对等进程收集最近心跳的到达时间**。该信息用于估算下一个心跳的到达时间，将该近似值与实际到达时间进行比较，并计算可疑程度φ：代表在给定当前网络条件下，故障检测器对故障的置信度。

​	该算法的原理是：<u>收集和采样到达时间，创建出一个可用于对节点健康状况做出可靠判断的视图，然后使用这些釆样结果计算φ的值：如果该值达到阈值，则节点被标记为宕机。**通过调整标记节点为疑似死亡的阈值，这种故障检测器能够动态地适应变化的网络条件**</u>。

​	从架构的角度来看，phi增量故障检测器可以看作三个子系统的组合。

+ **监控**

  通过ping、心跳或请求-响应采样来收集进程存活信息。

+ **解释**

  决定是否将该进程标记为疑似死亡。

+ **行动**

  每当标记进程为疑似死亡时执行的回调。

​	监控进程将数据样本(假定是正态分布)收集并储存在心跳到达时间的固定大小窗口中。新到达的心跳被添加到窗口中，同时最早的心跳数据点被丢弃。

​	<u>通过确定样本的均值和方差，可以从采样窗口估算出分布参数。该信息用于计算在前个消息到达之后t个时间单位内消息到达的概率。基于这个信息我们能计算出φ，它描述了我们对一个进程活动性做出正确决定的可能性。换句话说，有多大的可能性犯错——接收到一个与计算出的假设相矛盾的心跳</u>。

​	这种方法是由日本高级科学技术研究所的研究人员开发的，现在已用于许多分布式系统中，例如，Cassandra和Akka(连同前面提到的截止时间故障检测器)。

## * 9.3 Gossip和故障检测

> 有点像OSPF动态路由协议。
>
> [动态路由协议_百度百科 (baidu.com)](https://baike.baidu.com/item/动态路由协议/2915884?fr=aladdin)

​	另一种避免依赖单节点视图做出决策的方法是Gossip式的故障检测服务[VANRENESSE98]它使用Gossip(参见12.6节)来收集和分发相邻进程的状态。

​	<u>每个成员维护一个其他成员的列表：它们的心跳计数器(heartbeat counter)和时间戳</u>。

​	<u>时间戳列出了心跳计数器上次递增的时间。每个成员定期递增其心跳计数器，并将其列表分发给随机相邻节点。在接收到消息时，相邻节点将列表与它自己的列表进行合并，更新其他相邻节点的心跳计数器</u>。

​	节点还定期检查状态列表和心跳计数器。如果任何节点在足够长的时间里没有更新其计数器，就认为它发生了故障。超时时间应当谨慎选择，以将误报的概率降至最低。<u>成员之间通信的频率(换句话说，最坏情况下所使用的带宽)是有上限的，并且最多可以随着系统中的进程数线性增长</u>。

​	通过这样的方式，我们就可以检测出崩溃的以及任何其他集群成员都无法访问的节点。这个决策是可靠的，因为集群的视图是来自多个节点的聚合。如果两台主机之间的链路出现故障，心跳仍然可以通过其他进程传播。**使用Gossip来传播系统状态增加了系统中的消息数量，但使得信息传播更可靠**。

## * 9.4 反向故障检测

​	由于并不总是能传播故障的信息，并且通过通知每个成员来进行传播可能成本较高，因此出现了一种称为FUSE(Failure Notification Service，故障通知服务)[DUNAGANO4]的方法，它专注于可靠且廉价的故障传播，即使在网络分区的情况下也能工作。

​	<u>为了检测进程故障，该方法将所有活动进程进行分组。**如果其中一组变得不可用，则所有参与者都能检测到该故障**。换句话说，每次检测到单个进程故障时，它被转换并传播为**组故障**</u>。它可以检测任何形式的网络中断、网络分区和节点故障。

​	<u>组中的进程定期向其他成员发送ping消息，以查询它们是否仍处于活动状态。**如果其中一个成员由于崩溃、网络分区或链路故障而无法响应此消息，则发出这个ping的成员本身将停止响应ping消息**</u>。

​	展示了四个通信进程：

+ 初始状态：所有进程都处于活动状态并可以通信。
+ P2崩溃并停止响应ping消息。
+ P4检测到P2的故障并停止响应自己收到的ping消息。
+ 最终，P1和P3注意到P4和P2都没有响应，将进程故障传播到整个组。

​	**所有故障都通过系统从故障源传播到所有其他参与者**。<u>参与者逐渐停止响应ping消息，将单个节点故障转换为组故障</u>。

​	在这里，我们**<u>利用不通信作为一种传播的手段</u>**。这种方法的一个优点是保证每个成员都能了解组的故障并对其做出充分的反应。它的一个缺点是：<u>将单个进程与其他进程分开的链路故障也可能会被转换为组故障，但这其实也可以被看作是一个优点，应由具体的用例所决定</u>。应用程序可以使用其自身对故障传播的定义来应对这种情况。

## 9.5 本章小结

​	故障检测器在任何分布式系统中都是一个重要的组成部分。正如FLP不可能定理所说的，**<u>异步系统中没有协议能够保证一致性</u>**。

​	<u>故障检测器有助于扩展模型，允许我们通过在**准确性**和**完备性**之间进行权衡来解决一致性问题</u>。文献[CHANDRA96]描述了这一领域的一个重要发现，证明了故障检测器是有用的，它表明，<u>**即使使用一个犯了无限多个错误的故障检测器，解决共识问题仍然是可能的**</u>。

​	我们介绍了几种故障检测算法，每种算法都使用不同的方法：一些专注于通过直接通信来检测故障，而一些则使用广播或Gossip来传播信息，还有一些使用沉默(换句话说，不再通信)作为传播手段。现在，我们知道可以使用心跳、ping、截止时间、连续范围等方法，毎一种方法都有自己的优点:简单性、准确性或精确性。

# * 10. 领导者选举

​	同步的代价可能会非常大：如果算法的每一个步骤都需要联系其他参与者，那么结果定是产生相当显著的通信开销。在大型且地理分布的网络中尤其如此。<u>为了减少同步开销和达成决定所需消息的往返次数，一些算法依赖于**领导者(有时称为协调者)进程**的存在，该进程负责执行或协调分布式算法的各个步骤</u>。

​	一般来说，分布式系统中的进程是对等的，任何进程都可以接管领导者的角色。进程可以长期担任领导者，但这不是一个永久的角色。通常情况下，进程可以一直担任领导者直到崩溃为止。崩溃后，任何其他进程都可以开始新一轮的选举，如果其当选，就可以担任领导者并继续执行上个领导者遗留的工作。

​	选举算法的**活动性(liveness)**保证了大多数时候会有一个领导者，选举最终会完成(即<u>系统不应该无限期地处于选举状态</u>)。

​	理想情况下，我们也希望获得安全性，保证一次最多只能有一个领导者，并完全消除<u>脑裂(两个目的相同的领导者被选举出来，但彼此不知情)</u>的可能性。然而在实践中，许多领导者选举算法违反了这一协定。

​	可以使用领导者进程来实现广播中消息的全序。领导者收集并保存全局状态，接收消息，并将消息分发给各个进程。它还可以用于协调发生在系统故障后、初始化期间或重要状态变更时的系统重组。

​	系统初始化时将会触发选举，第一次选择领导者。当上一个领导者崩溃或通信失败时也会触发选举。选举必须是确定性的：<u>选举过程中必须只产生一个领导者。这一决定需要对所有参与者都有效</u>。

​	<u>尽管领导者选举和分布式锁(即对共享资源的独占所有权)从理论角度看可能很相似，但它们略有不同。如果一个进程因为执行**临界区**而持有锁，那么对于其他进程来说，知道现在到底是谁持有锁并不重要，只要满足活动性(即锁最终将被释放并允许其他人获得它)就可以了。**相比之下，选出的进程具有一些特殊性质，必须让所有其他参与者都知道，因此新当选的领导者必须将其角色通知所有对等进程**</u>。

​	<u>如果分布式锁算法对某个进程或进程组有任何偏好，不被偏好的进程最终将产生对共享资源的饥饿，这与活动性相矛盾。相比之下，领导者可以保持领导的角色直到停止或崩溃，长期存活的领导者是更好的</u>。

​	**在系统中具有稳定的领导者有助于避免远程参与者之间的状态同步，减少交换消息的数量，并能通过单进程(而不是对等进程间的协调)来驱动执行**。

​	在具有领导权的系统中，一个潜在的问题是，**领导者可能会成为瓶颈**。<u>为了克服这种情况，许多系统将数据划分在不相交的独立副本集中(参见13.6节)。每个副本集都有自己的领导者，而不是在整个系统范围上使用一个领导者</u>。使用这种方法的系统之一是Spanner(参见13.5节)。

​	因为毎一个领导者进程最终都会发生故障，所以故障必须被检测、报告和处理：系统必须选择另一个领导者来替换故障领导者。

​	一些算法，例如ZAB(参见14.2.2节)、 Multi-Paxos(参见14.34节)或Raft(参见14.4节)，<u>使用**临时领导者**来减少参与者达成一致所需的消息数量</u>。然而，这些算法都使用算法特有的手段来进行领导者选举、故障检测以及解决竞争领导者的进程之间的冲突。

## 10.1 霸道选举算法

​	有一个被称为霸道选举算法(bully algorithm)的领导者选举算法，<u>它使用进程排名来认定新的领导者。每个进程都有一个唯一的排名。在选举过程中，排名最高的进程成为领导者[MOLINA82]</u>。

​	这种算法以其简单性而闻名。之所以被命名为霸道(bully)，是因为排名最高的节点"霸道"地强迫其他节点接受它。它有时也被称为君主领导者选举：在前一个君主不复存在后，排名最高的兄弟姐妹成为君主。

​	如果一个进程注意到系统中没有领导者(该系统从未被初始化)或以前的领导者已停止响应，则按如下三个步骤进行选举：

1. 进程将选举消息发送到具有较高标识符的进程。
2. 进程等待较高排名的进程进行响应。如果没有排名更高的进程响应，则继续执行步骤3。否则，进程通知它所了解到的排名最高的进程，让它继续执行步骤3。
3. 进程假定没有排名更高的活动进程了，因此将新的领导者通知给所有排名更低的进程。

​	霸道领导者选举算法：

+ a）进程3注意到前一个领导者6已崩溃，于是向具有更高标识符的进程发送选举消息来开始新的选举。
+ b）4和5回应Alive(存活)消息，因为它们具有比3更高的排名。
+ c）3通知在此轮响应中排名最高的进程5。
+ d）5被选为新的领导者，它广播Elected(选举完成)消息，将选举结果通知排名较低的进程。

​	这种算法的一个明显问题是，在存在网络分区的情况下，它违反了安全性保证(一次最多只能选举一个领导者)。很容易出现这样的情况：<u>节点将被分成两个或多个独立运行的子集，每个子集选举出了这个子集的领导者。这种情况被称为**脑裂(split brain)**</u>。

​	该算法的另一个问题是对排名较高节点有强烈的偏向性，<u>如果这些节点不稳定，可能导致算法一直处于重新选举状态</u>。一个不稳定的高排名节点提议出任领导者，此后不久发生故障，稍后又再次赢得选举，然后再次发生故障，整个过程重复进行。可以通过分发主机质量的度量并在选举时将其纳入考虑因素来解决这一问题。

## 10.2 依次故障转移

​	霸道算法有许多改进其各种性质的版本。例如，我们可以使用多个依次(next-in-line)替代进程作为故障转移候选来缩短重选过程[GHOLIPOUR09]。

​	<u>每个选举出的领导者都提供一个故障转移节点的列表。当其中一个进程检测到领导者故障时，它通过向列表中排名最高的备选进程发送消息来发起新一轮选举。如果提议的备选进程中有一个是活动的，它就会成为一个新的领导者，而不必经过整个选举回合</u>。

​	如果检测到领导者故障的进程本身是列表中排名最高的进程，则它可以立即通知其他进程新领导者的信息。

优化过的流程：

+ a）6是具有指定备选进程{5，4}的领导者，它崩溃了。3注意到此故障并联系5，它是列表中排名最高的备选进程。
+ b）5响应3，它处于活动状态，以防止它联系备选进程列表中的其他节点
+ c）5通知其他节点它是新的领导者。

​	因此，如果下一个进程仍然活着，我们在选举期间需要的步骤就更少。

## 10.3 候选节点/普通节点优化

​	另一种算法试图通过将节点分成两个子集，即**候选节点(candidate)**和**普通节点(ordinary)**，来降低所需的消息数量，只有候选节点的其中之一最终可以成为领导者[MURSHED12]。

​	普通进程通过联系候选节点来发起选举，收集它们的响应，选择排名最高的活动候选节点作为新的领导者，然后通知其余节点选举结果。

​	**为了解决同时发生多个选举的问题，该算法建议使用一个特定于进程的延迟变量δ(各进程的δ差异很大)，使得其中一个节点可以在其他节点之前发起选举**。δ通常大于消息的往返时间。高优先级的节点具有较低的δ，反之则具有较高的δ。

​	选举过程的步骤（假设1、2、6都是候选节点；3、4、5都是普通节点）：

+ a）普通进程集合里的进程4注意到领导者进程6的故障。它通过联系候选节点集合里的所有剩余进程来发起新的选举回合。
+ b）候选进程发送响应通知4它们仍然活动。
+ c）4将新的领导者2通知给所有进程。

## * 10.4 邀请算法

​	邀请算法(invitation algorithm)允许进程"邀请"其他进程加入它们的组，而不是试图超越它们的排名。这种算法从定义上就**允许多个领导者存在**，因为每个组都有自己的领导者。

​	<u>每个进程一开始都是一个新组的领导者，组内唯一的成员是这个进程本身。组领导者联系不属于该组的对等进程，邀请其加入。如果对等进程本身是领导者，则合并两个组。否则，被联系的进程会回复组领导者ID，从而让两个组的领导者以较少的步骤建立联系并合并两个组</u>。

​	邀请算法的执行步骤：

+ a）开始时，有四个进程成为各有一名成员的组的领导者。1邀请2加入它所在组，3邀请4加入它所在组。
+ b）2加入进程1的组，4加入进程3的组。1作为第一组的领导者去联系另一组的领导者3。随后，该组剩余的成员(在本例中为4)被通知关于新的组领导的信息。
+ c）两个组被合并，并且1成为扩展后组的领导者。

​	因为组被合并了，是建议组合并的进程还是另一个进程成为新的领导者都无关紧要。为了将合并组所需的消息数量保持在最低限度，较大组的领导者可以成为新组的领导者。这样，只需要把领导者变更的消息通知给较小组的进程。

​	与其他讨论过的算法类似，该算法允许各个进程处于多个组中，并允许多个领导者存在。邀请算法允许创建进程组并合并它们，而不必从头开始触发新的选举，这减少了完成选举所需的消息数量。

## 10.5 环算法

​	在环算法(ring algorithm)[CHANG79]中，系统中所有的节点形成一个环，并且知道环拓扑(即它们在环中的前驱和后继)。当进程检测到领导者故障时，它发起新的选举。选举消息沿着环向下转发：每个进程联系它的后继节点(环中离它最近的下一个节点)。如果该节点不可用，则进程跳过不可达的节点，并尝试联系环中之后的节点，直到最终有一个节点响应为止。

​	节点联系其兄弟节点，沿着环收集活动节点的集合，并在将集合传递到下一个节点之前将其自身添加到该集合中，类似于9.1.1节中描述的故障检测算法那样，节点在将其传递到下一个节点之前将其标识符附加到路径中。

​	该算法通过完全遍历环来进行。当消息返回到开始选举的节点时，从活动节点集中选择排名最高的节点作为领导者。一个遍历的例子（假设1-2-3-4-5-6～1成环，6故障后，5和1成相邻节点）：

+ a）先前的领导者6发生了故障，并且每个进程都具有该环的视图。
+ b）3通过开始遍历来发起选举。每一步中，都维护一个在路径上遍历过的节点集合。5无法达到6，所以它跳过6直接联系1。
+ c）由于5是排名最高的节点，3会发起另一轮消息传递来广播关于新领导者的信息。

​	**这种算法的变体包括收集单个排名最高的标识符，而不是一组活动节点，以节省空间**：因为max函数是可交换的，所以知道当前遍历过的节点的排名最大值就足够了。当算法返回到已经开始选举的节点时，最后已知的最高标识符会再次在环上循环传递。

​	**由于环可以划分为两个或更多的部分，因此可能出现脑裂这种不安全的情况**。

​	正如你所看到的，一个具有领导者的系统要正确行使职能，我们就需要知道目前领导者的状况(它是否还活着)，因为，要想将进程组织起来继续执行算法，领导者必须是活动且可达的。要检测领导者崩溃，我们可以使用故障检测算法(参见第9章)。

## 10.6 本章小结

​	领导者选举是分布式系统中的一个重要课题，使用特定领导者可以减少协调开销并提高算法的性能。每轮选举的成本可能很高，但由于它们不经常发生，因此不会对整个系统的性能产生负面影响。<u>单个领导者可能会成为瓶颈，但大多数时候可以通过对数据进行分区来解决(对不同分区或不同的动作使用不同的领导者)</u>。

​	<u>不幸的是，我们在本章中讨论的所有算法都容易出现**脑裂**的问题</u>：**我们可能会在独立的子网中选出两个领导者，它们彼此都不知道对方的存在**。<u>为了避免脑裂，我们必须获得整个集群范围内的多数票</u>。

​	<u>许多共识算法，包括Multi-paxos和Raft，都依赖于一个领导者来进行协调。但领导者选举不就是共识本身吗?要选举一个领导者，我们需要就其身份达成共识。如果我们能就领导者的身份达成共识，我们就能用同样的方法在其他任何事情上达成共识[ABRAHAM13]</u>。

​	一个领导者的身份可能会在其他进程不知晓的情况下发生变化，因此进程本地关于领导者的知识是否仍然有效是个问题。为了解决这个问题，<u>我们需要将领导者选举与故障检测相结合</u>。例如，稳定领导者选举(stable leader election)算法使用具有唯一稳定领导者的回合和基于超时的故障检测，以保证领导者只要不崩溃且可访问，就能维持其地位[AGUILERA01]。

​	**依赖于领导者选举的算法通常允许存在多个领导者，并试图尽可能快地解决领导者之间的冲突**。例如，对于 Multi-Paxos(参见14.3.4节)就是这样的，其中只有两个冲突的领导者(提议者)中的一个可以继续执行，这些冲突通过收集第二个Quorum的投票来解决，保证来自两个不同提议者的值不会被同时接受。

​	在Raft(参见14.4节)中，领导者可以发现其任期过时(这意味着系统中存在不同的领导者)，并将其任期更新为最新的任期。

​	在这两种情况下，拥有一个领导者是确保活动性的一种方式(如果当前的领导者发生故障，我们需要一个新的领导者)，而且其他进程不应该花费无限长的时间来了解领导者是否真的发生了故障。安全性的缺失和允许多个领导者是一种性能优化：算法可以继续进行复制阶段，而安全性则通过检测和解决冲突来保证。

​	我们将会在第14章里更深入地讨论共识和共识范畴内的领导者选举。

# 11. 复制和一致性

​	在继续讨论共识和原子提交算法之前，让我们学习最后一块缺失的部分：一致性模型(consistency model)。一致性模型非常重要，因为它们解释了多数据副本系统的可见性语义和行为。

​	容错(fault tolerance)描述了这样一种特性：当系统中的部分组件发生故障时，系统仍然能继续正确地运行。使系统具有容错性并不是一件容易的事情，而使现有的系统具备容错能力则更加困难。<u>容错的主要目标是从系统中消除单点故障，并确保关键任务组件有冗余</u>。通常，冗余对用户来说是完全透明的。

​	系统可以存储多个数据副本，当其中一台机器发生故障时，通过另一台机器可以进行**故障转移(failover)**，这样系统就可以继续正确运行。<u>在具有单一真相来源(source of truth)的系统中(例如，主/副本数据库)，可以通过将副本晋升为新的主数据库来显式地完成故障转移</u>。有些系统则不需要显式的重新配置，它们通过在读写查询期间收集多个参与者的响应来确保一致性。

​	**数据复制(replication)**通过在系统中维护多个数据副本来引入冗余。然而，由于原子地更新数据的多个副本是一个等同于共识的问题[MILOSEVIC11]，因此<u>**数据库中的每个操作都执行共识操作可能成本相当高**</u>。我们可以探索一些性价比更高且更灵活的方法，允许参与者之间存在某种程度的差异，但使数据从用户的角度看起来是一致的。

​	在多数据中心部署中，复制尤为重要。在这种情况下，**跨地域复制(geo-replication)**有多种用途：它通过提供冗余来提高可用性，增强抵御一个或多个数据中心的故障的能力。它还<u>可以将数据副本放置在离客户端更近的物理位置以减少延迟</u>。

​	当数据记录被修改时，其副本必须被相应地更新。在谈论复制时，我们最关心这三种事件： **写入、副本更新和读取**。这些操作触发了由客户端发起的一系列事件。在某些情况下，从客户端角度看，更新副本可能发生在写操作完成之后，但这仍然不能改变这样个事实：<u>客户端必须能够以特定的顺序观察到发生过的操作</u>。

## 11.1 实现可用性

​	我们已经讨论了分布式系统的误区，并确定了许多可能出错的事情。在现实世界中，节点不一定处于活动状态或能够相互通信。然而，**间歇性故障不应影响可用性**：从用户的角度来看，系统作为一个整体会继续运行，就像什么都没有发生一样。

​	系统可用性是一个非常重要的属性：在软件工程中，我们总是努力实现高可用性，并尽量减少停机时间。工程团队夸耀他们软件的正常运行时间指标。我们之所以那么关心可用性是出于如下几个原因：软件已经成为我们社会不可分割的一部分；没有它，许多重要的事情都难以进行，例如银行业务、通信、旅行等。

​	对于公司来说，缺乏可用性可能意味着失去客户或金钱：如果电商系统出现故障，你就无法在那里购物；如果你的银行网站没有响应，你就无法转账。

​	为了使系统具有高可用性，我们需要以这样一种方式设计我们的系统——允许它优雅地处理一个或多个参与者出现故障或不可用的情况。为此，我们需要引入**冗余**和**复制**。然而，<u>一旦我们添加冗余，就会面临**多数据副本同步**的问题，并且必须实现**恢复机制**</u>。

## 11.2 臭名昭著的CAP理论

​	**可用性**是一个衡量系统成功响应请求能力的属性。可用性的理论定义提到了最终响应，但是，在现实世界的系统中，我们当然希望避免等待无限长的时间。

​	理想情况下，我们希望毎个操作都是一致的。**一致性在这里定义为原子性或可线性化linearizable)的一致性**(参见11.5.2节)。<u>可线性化历史能够表示为一个可以保持原始操作顺序的瞬时操作序列</u>。可线性化简化了对可能的系统状态的推理，使分布式系统看起来就像是在单机系统上运行一样。

​	我们希望在容忍网络分区的同时实现一致性和可用性。网络可能被分裂为几个部分，在这些部分之间，进程不能相互通信：被分隔的节点之间发送的一些消息将无法到达目的地。

​	<u>**可用性**要求任何无故障的节点交付结果，而**一致性**要求结果是可线性化的</u>。 Eric Brewer提出的CAP猜想讨论了一致性(consistency)、可用性(availablity)和分区容忍性(partition tolerance)之间的权衡[BREWER00]。

​	<u>在一个异步系统中，可用性要求是不可能被满足的，而在网络分区存在的情况下，我们无法实现一个同时保证可用性和一致性的系统[GILBERT02]</u>。构建系统时，我们可以在提供尽力而为(best effort)可用性的同时保证强一致性，或者在提供尽力而为一致性的同时保证可用性[GILBERT12]。在这里，尽力而为意味着：如果一切正常，系统将不会故意违反任何保证，但是在网络分区的情况下，允许系统削弱和违反保证。

​	换句话说，CAP描述了一系列潜在的可选项，而在这些选项的两端是以下两种系统：

+ **一致性和分区容忍系统(CP系统)**

  CP系统更倾向拒绝请求，而不是提供可能不一致的数据。

+ **可用性和分区容忍系统(AP系统)**

  AP系统放松了一致性要求，允许在请求期间提供可能不一致的值。

​	CP系统的一个例子是共识算法的实现，它要求多数派节点的参与才能进行：系统总是一致的，但在网络分区的情况下可能不可用。

​	而AP系统的一个例子是，只要有一个副本活着，数据库就一定能进行读写，这最终可能导致数据丢失或返回不一致的结果。

​	PACELC猜想[ABADI12]是CAP的一个扩展，它指出在网络分区(P)存在的情况下可用性和一致性(AC)之间存在一个选择。否则(Else，E)，即使在没有网络分区的情况下系统运行正常，我们仍然要在延迟(Latency，L)和一致性(C)之间做出选择。

### 11.2.1 小心使用CAP

​	需要注意的是，<u>CAP讨论的是网络分区，而不是节点崩溃或任何其他类型的故障(如崩溃恢复)。与集群其他节点分隔开的节点可以做出不一致的响应，但崩溃的节点根本不会响应</u>。一方面，这意味着没有必要考虑任何宕掉的节点需要面对一致性问题。而另一方面，现实世界中的情况并非如此：还有许多不同的故障场景(其中一些可以用网络分区来模拟)。

​	CAP意味着，即使所有节点都启动了，**<u>只要它们之间有连接性问题，我们仍可能面临一致性问题</u>**。这是因为<u>我们期望毎个没有故障的节点都能正确响应，而不考虑有多少节点可能宕掉</u>。

​	CAP猜想有时用一个三角形来表示，就好像我们可以转动一个旋钮，或多或少地得到所有这三个参数中的两个。然而，尽管我们可以转动旋钮用一致性换取可用性，但**分区容忍性是一个实际上我们无法调节或用任何东西来交换的属性**[HALE10]。

> CAP中的一致性定义与ACID(参见第5章)定义的一致性完全不同。ACID致性描述了事务一致性：事务将数据库从一个有效状态带到另一个有效状态，保持所有数据库的约束(如唯一性约束和引用完整性)。而在CAP中，它意味着操作是**原子的**(操作全部成功或失败)和**一致的**(操作从不让数据处于不一致的状态)。

​	CAP中的可用性也不同于前面提到的高可用性[KLEPPMANN15]。CAP的定义对执行延迟没有限制。另外，与CAP相反，数据库中的可用性并不要求每个非故障节点响应每个请求。

​	CAP猜想用于解释分布式系统、推理故障场景和评估可能的情况，放弃一致性并不意味着系统可以提供不可预测的结果，它们之间有所区别。

​	如果使用正确，声称具备可用性的数据库仍然能够提供来自副本的一致结果，前提是要存在足够多的活着的副本。当然，还有更复杂的故障场景，**CAP猜想只是一条经验法则，并不一定能说明全部事实**。

### 11.2.2 收成与产量

​	CAP猜想仅以它们最强的形式讨论一致性和可用性：可线性化和系统最终响应每一个请求的能力。

​	这迫使我们在这两个属性之间做出艰难的权衡。然而，有些应用程序可以从稍微放松的假设中获益，我们可以用它们较弱的形式来思考这些属性。

​	系统不一定非得在一致或可用中二选一，也可以提供更宽松的保证。我们可以定义两个可调度量：收成(harvest)和产量(yield)，在两者之间进行选择仍然可以形成正确的行为[FOX99]：

+ 收成

  收成定义査询的完成程度：如果查询必须返回100行，但由于某些节点不可用而只能获取99行，这仍然比查询完全失败而不返回任何内容要好。

+ 产量

  产量指成功完成的请求数与尝试请求总数之比。产量与正常运行时间(uptime)不同，例如一个繁忙的节点没有宕机，但仍然可能无法响应某些请求。

​	这就把权衡的重点从绝对条件变成了相对条件。我们可以用收成换取产量，并允许某些请求返回不完整的数据。<u>提高产量的一个方法是只从可用的分区返回查询结果</u>(参见13.6节)。例如，如果存储某些用户记录的节点子集宕机，我们仍然可以继续为其他用户处理请求。或者，我们可以要求，对关键应用程序的数据必须完整返回，但对其他请求允许出现一些偏离。

​	定义和权衡收成与产量，并在二者之间做出慎重的决定，有助于我们建立容错性更好的系统。

## 11.3 共享内存

​	对于客户端来说，分布式系统在储存数据时仿佛拥有共享的存储，类似于单节点的系统。节点间通信和消息传递则被抽象出来并发生在幕后。这就造成了一种共享内存的假象。

​	可通过读写操作来访问的单个存储单元通常被称为寄存器(register)。我们可以将分布式数据库中的共享内存视为一个寄存器阵列。

​	我们用调用((invocation)和完成(completion)事件来标识每个操作。如果调用该操作的进程在操作完成之前崩溃，则将该操作定义为失败操作。<u>如果一个操作的调用和完成事件都发生在另一个操作被调用之前，我们说这个操作在另一个操作之前，并且这两个操作是顺序的(sequential)。否则说它们是并发的</u>。

​	进程P1和P2执行不同的操作：

+ a)进程P2执行的操作在进程P1执行的操作已经完成之后才开始，两个操作是顺序
+ b)两个操作之间有重叠，因此这两个操作是并发的。
+ c)进程P2执行的操作在P1执行的操作之后开始，并在P1执行的操作完成之前结束，这两个操作也是并发的。

​	多个读取者或写入者可以同时访问寄存器。对寄存器的读写操作不是即时的，即需要些时间。由不同进程执行的并发读写操作不是串行的(serial)：根据操作重叠时寄存器的行为，它们的顺序可能不同，并可能产生不同的结果。根据寄存器在并发操作下的行为，我们将寄存器分为以下三类：

+ 安全寄存器

  在并发写操作期间，对安全寄存器的读取可能返回寄存器范围内的任意值(这听起来不太实用，但可能描述出了异步系统的语义——不限定执行的顺序)。**具有二元值的安全寄存器在读写过程中可能会出现闪烁**(flickering，即结果交替返回两个值)。

+ 常规寄存器

  常规寄存器有稍强一点的保证：<u>读操作只能返回最近完成的写操作所写的值，或者与当前读操作重叠的写操作所写的值</u>。在这种情况下，**系统有一些顺序的概念，但是写结果并不同时对所有读取者可见**(例如，在复制数据库中可能会发生这样的情况：主进程接受写操作，并将其复制给提供读服务的工作者进程)。

+ 原子寄存器

  **原子寄存器保证可线性化：每个写操作都对应一个时刻，在此之前，每个读操作返回一个旧值，在此之后，每个读操作返回一个新值。原子性是简化系统状态推断的基本性质**。

## * 11.4 顺序

​	当我们看到一系列事件时，我们对它们的执行顺序会有直观感受。然而，在分布式系统中，理解顺序并不总是那么容易，因为很难准确地知道什么时候发生了什么事情，并且也很难在整个集群中立即获得这些信息。每个参与者可能都有自己的状态视图，因此我们必须査看毎个操作，并根据其调用和完成事件定义顺序，并描述其操作边界。

​	让我们这样定义一个系统，在其中的进程可以对共享寄存器执行read(register)和write(register， value)操作，每个进程按顺序执行自己的一组操作(即，必须完成每个操作之后才能启动下一个操作)。顺序进程执行的组合形成了一个全局历史，这其中，操作可能并发执行。

​	**考虑一致性模型的最简单方法是讨论读和写操作及其重叠的方式：读操作没有副作用，而写操作会改变寄存器状态**。这能帮助我们推断写入后数据何时变得可读。例如，考虑两个进程并发执行如下事件的历史记录：

```pseudocode
进程1:				进程2:
write(x,1)		read(x)
							read(x)
```

​	单看这些事件无法确定这两种情况下read(x)运算的结果是什么。有几种可能的历史：

+ 写操作在两次读操作之前完成。
+ 写操作和两次读操作交错进行，而且可能在两次读操作之间执行。
+ 两次读操作都在写操作之前完成。

​	即使我们只有一份数据，也并不能很容易地回答会发生什么。在一个复制系统中，我们会有更多可能的状态组合，当多个进程读写数据时情况将变得更加复杂。

​	如果所有这些操作都是由单个进程执行的，我们可以强制一个严格的事件顺序，但是对于多个进程则很难这样做。潜在的困难可以分为两组：

+ 操作可能会重叠。
+ 不重叠调用的影响可能不会立即显现。

​	**为了推理操作顺序并明确描述可能的结果，必须定义一致性模型**。我们用共享内存和并发系统来讨论分布式系统中的并发性，因为大多数一致性定义和规则仍然是适用的。尽管并发系统和分布式系统之间存在着许多重叠的术语，但由于通信方式、性能和可靠性等方面的差异，我们无法直接应用大多数并发算法。

## 11.5 一致性模型

​	由于共享内存寄存器上的操作允许重叠，我们需要定义清楚语义：如果多个客户端同时(或在短时间内)读取或修改数据的不同副本将会发生什么。这个问题没有唯一正确的答案，因为这些语义根据应用程序的不同而不同，但是在一致性模型的上下文中它们都得到了很好的研究。

​	一致性模型(consistency model)提供不同的语义和保证。你可以将一致性模型看作是参与者之间的契约：每个副本要做什么才能满足所需的语义，以及用户在发出读写操作时可以期望得到什么结果。

​	一致性模型描述了在存在多份数据和并发访问的情况下可能岀现的返回结果。在本节，我们将讨论单一操作(single-operation)一致性模型。

​	毎个模型都描述了系统行为与我们期望的(或感觉自然的)行为之间存在的差距。它帮助我们区分交错操作的"所有可能的历史"和"在模型X下允许的历史"，这显著地简化了关于状态变化可见性的推理。

​	我们可以从状态的角度考虑一致性，描述哪些状态不变式是可以接受的，并建立不同副本上的数据拷贝之间所准许的关系。或者，我们也可以考虑操作一致性，它提供一个数据存储的外部视图，描述了操作并约束它们发生的顺序[TANENBAUM06，AGUILERA16]。

​	<u>如果没有全局时钟，很难给分布式操作一个精确且确定的顺序</u>。这就像是数据的狭义相对论：每个参与者对状态和时间都有自己的视角。

​	**<u>从理论上讲，每当我们想要改变系统状态时，可以先获取一个系统范围的锁，但这是非常不实际的。相反，我们使用一组规则、定义和约束来限制可能的历史和结果的数量</u>**。

​	一致性模型为我们在11.2节中讨论的内容增加了另一个维度。现在我们不仅要兼顾一致性和可用性，还要从**同步代价**的方面考虑一致性[ATTIYA94]。同步代价可能包括时延执行额外操作所花费的**CPU周期**、**用于持久化恢复信息的磁盘I/O**、**等待时间**、**网络I/O**，以及一切可以通过避免同步而节省的代价。

​	首先，我们将重点关注操作结果的可见性和传播。回到并发读写的例子，通过将写入按照依赖关系一个接一个地放置，或定义一个新值被传播出去的时间点，我们就能限制可能的历史数量。

​	我们从执行数据读写操作的进程(客户端)的角度来讨论一致性模型。由于我们在数据复制的上下文中讨论一致性，因此我们假设数据库可以有多个副本。

### 11.5.1 严格一致性

​	严格一致性(strict consistency)相当于完全透明的复制：**<u>任何进程的任何写入都可以立即被任何进程的后续的读操作读取</u>**。<u>它涉及全局时钟的概念，如果在时刻t1有write(x，1)，则任何read(x)的操作将在时刻t2>t1时返回新写入的值t1</u>。

​	不幸的是，**<u>这只是一个理论模型，且不可能实现</u>**，因为物理定律和分布式系统的工作方式限制了事情发生的速度[SINHA97]。

### * 11.5.2 可线性化

​	可线性化(linearizability)是最强的单对象、单操作一致性模型。**该模型下，在写操作开始和结束之间的某个时间点，写操作的效果严格一次性地对所有读取者可见，没有客户端会观察到状态转移、部分(即未完成的、仍在进行中的)或不完全(即在完成之前中断的)的写入操作**[LEE15]。

​	并发操作表现为可见性属性所支持的可能的顺序历史记录之一。可线性化有一定的不确定性，因为可能存在不止一种方式来对事件进行排序[HERLIHY90]。

​	如果两个操作重叠，它们可以按任何顺序生效。<u>在写操作完成之后发生的所有读操作都可以观察到该操作的结果</u>。**一旦一个读操作返回一个特定的值，在它之后的所有读操作返回的值至少都与它返回的那个值一样新**[BAILIS14a]。

​	**在全局历史中，并发事件发生的顺序有一定的灵活性，但它们不能被任意地重新排序**。操作的结果生效不能早于操作的开始时间，因为谁也不能预测未来。同时，结果必须在完成之前生效，否则，我们无法定义一个线性化点。

​	可线性化既尊重顺序性进程局部的操作顺序，也尊重相对于其他进程的并行操作的顺序，因此它定义了事件的全序关系(total order)。

​	这个顺序应该是一致的，这意味着共享值的每次读取都应该返回在此之前写入此共享变量的最新值，或者返回与此读取重叠的写操作所写入的值。**对共享变量的可线性化写访问也意味着互斥：在两个并发写之间，只有一个可以先进行**。

​	即使操作是并发并存在重叠，它们的效果也会以一种看似连续的方式变得可见。当然，没有一个操作是瞬间发生的，但操作看起来仍然是原子的。

**线性化点**

​	<u>可线性化最重要的一个特征是可见性：一旦操作完成，每个参与者都必须能看到它，并且系统不能“穿越到过去”还原它或使它对某些参与者不可见。换句话说，**可线性化禁止读取过时的数据**，它要求读取是单调的</u>。

​	这种一致性模型最好用原子的(即：不间断、不可分割)操作来解释。<u>操作不一定是瞬时的(也因为并不存在瞬时的东西)，但是它们的效果必须在某个时间点变得可见，从而造成操作是瞬时的错觉。这个时刻称为**线性化点(linearization point)**</u>。

​	越过写操作的线性化点(换句话说，当该值对于其他进程变得可见时)，每个进程必然看到该操作所写的值或更新的值(如果在该值之后还有其他的写操作)。<u>一个可见值应保持稳定，直到下一个值变得可见，寄存器不应在这两个临近的状态之间反复</u>。

> 当下大多数编程语言都提供了原子原语，允许原子写操作和原子比较-交换(CAS)操作。原子写操作不考虑当前寄存器值，这与CAS不同，CAS仅在前一个值没有变化时才从一个值变成另一个值[HERLIHY94]。读取值后修改它，并随后用CAS写入，这比简单地检查和设置值要复杂得多，因为可能存在**ABA问题**[DECHEⅥ10]：如果CAS期望A出现在寄存器中，那么值A将最终被设置，即使通过其他两个并发写入操作设置了值B并切换回值A。换句话说，值A本身不变并不能保证自上次读取以来该值没有被改过。

​	<u>线性化点起到切分点的作用，在此之后，操作效果变得可见。我们可以通过使用**锁**来保护临界区、**原子读/写**或**读-修改-写(read-modify-write)原语**来实现它</u>。

**可线性化的代价**

​	**<u>当今许多系统避免实现可线性化</u>**。

​	**在默认情况下，即使是CPU在访问主存时也不提供可线性化一致性**。这是因<u>为**同步指令开销大、速度慢，并且涉及跨节点CPU流量和缓存失效**</u>。

​	然而，**可以使用底层原语来实现可线性化**[MCKENNEY05a，MCKENNEY05b]。

​	在并发编程中，你可以使用CAS操作来引入可线性化。**<u>许多算法的工作方式都是先准备结果，然后使用CAS交换指针使结果可见</u>**。例如，并发队列可以这样实现：创建一个链表节点，然后原子性地将其追加到列表尾部[KHANCHANDANI8]。

​	在分布式系统中，可线性化需要协调冲突和顺序。它可以使用**共识算法**来实现：客户端使用消息与复制存储进行交互，**共识模块**负责确保应用的操作在整个集群中是一致且相同的。**毎个写操作将在它的调用和完成事件之间的某个时间点瞬间出现且仅出现一次**[HOWARD14]。

​	有趣的是，可线性化在其传统理解中被视为*局部性质*，并且意味着其是独立实现和验证的单元组合。**<u>合并可线性化的历史将产生一个同样可线性化的历史</u>**[HERLIHY90]。换句话说，其中所有对象都是可线性化的系统，也是可线性化的。这是一个非常有用的属性，但是我们应该记住，**它的范围仅限于单个对象**，而且即使对两个独立对象的操作是可线性化的，<u>涉及两个对象的操作还必须依赖于额外的同步手段</u>。

> **可重用可线性化基础设施**
>
> ​	**可重用可线性化基础设施(Reusable Infrastructure For Linearizability，RIFL)是一种用于实现可线性化远程过程调用(RPC)的机制**[LEE15]。在RIFL中，消息用客户端ID和客户端本地单调递增的序列号唯一标识。
>
> ​	为了分配客户端ID，RIFL使用由系统层面的服务颁发的租约——规避重复序列号的唯一标识符。如果发生故障的客户端试图使用过期的租约执行操作，它的操作将不会被提交：客户端必须接收新的租约，然后重试。
>
> ​	<u>如果服务器在写操作回复确认消息前崩溃，客户端可能会尝试重试此操作，而不知道此操作已被应用</u>。甚至会出现这样的情况：客户端C1写入值V1，但没有收到确认。同时，客户端C2写入值Ⅴ2。如果C1重试其操作并成功写入V1，则C2的写入操作将丢失。为了避免这种情况，系统需要防止重复执行重试操作。**当客户端重试操作时，<u>RIFL返回一个完成对象(completion object)来指示与它相关联的操作已经被执行并返回结果，而不是再次应用操作</u>**。
>
> ​	<u>**完成对象**与**实际数据记录**一起储存在持久存储中</u>。但是，它们的生存期是不同的：在发出请求的客户端保证它不会重试相关联的操作之前，或者服务器检测到客户端崩溃之前，完成对象都应该存在。而在这两种情况下，所有与它相关联的完成对象都可以被安全地删除。**创建完成对象的操作应该与它所关联数据记录的变化在一起原子地执行**。
>
> ​	**客户端必须定期续订租约，以表明它们的活动性**。<u>如果客户端未能续订其租约，它将被标记为已崩溃的，并且所有与其租约相关联的数据都将被垃圾回收</u>。租约有个有限的生存期，以确保属于故障进程的操作不会被永远保留在日志中。**如果出现故障的客户端试图使用到期的租约继续运行，其结果将不会被提交，客户端将不得不从头开始**。
>
> ​	<u>**RIFL的优点在于，通过保证RPC不会被多次执行**，一个操作可以通过确保其**结果原子可见**而**实现可线性化**，并且它的大部分实现细节独立于底层存储系统</u>。

### 11.5.3 顺序一致性

P198