# 第二部分 分布式系统

​	没有分布式系统，我们将无法拨打电话、转账或远距离交换信息。我们每天都在使用分布式系统。有时候，即使没有明确说明，任何客户端/服务器架构的应用程序其实都是分布式系统。

​	对于许多现代软件系统，垂直扩展(将软件运行在更大更快的机器上，配备更多的CPU、RAM或更快的磁盘)是不现实的。更大的机器也更贵，更难以置换，而且可能需要特殊的维护。一个替代选项是<u>水平扩展：将软件运行在多个用网络相连的机器上，而在逻辑上视为单个实体</u>。

​	分布式系统有各种规模，少则几台机器，多则上百台机器。系统参与者的特性也各不相同，可以是手持式或传感器设备，也可能是高性能计算机。

​	数据库系统运行在单个节点上的时代已经过去很久了，大多数现代数据库系统拥有多个以集群方式相互连接的节点，以增加存储容量、提升性能和增强可用性。

​	尽管某些分布式计算的理论突破不是新生事物，但大部分的实际应用还是在近期才出现的。今天，这一主题越来越受到人们的关注，我们也看到更多的相关研究与新的发展正在进行。

---

+ 第二部分基本定义

  ​	分布式系统有若干个参与者(participant，有时也称为进程、节点或副本)，每个参与者都有其自己的本地状态。参与者通过在通信链路上交换消息来进行彼此间的通信。

  ​	进程可以用时钟来获取时间，它可以是逻辑的，也可以是物理的。<u>逻辑时钟是用单调递增的计数器实现的。物理时钟也被称为挂钟(wall clock)，它与物理世界的时间概念紧密相连，可以通过进程本地的方式(例如通过操作系统)获取到</u>。

  ​	说到分布式系统就不得不提到一点：系统中的各个部分彼此分开放置，这本身就带来了很大的困难。远程进程间的通信链路可能既慢又不可靠，这导致确定远程进程的状态变得更复杂。

  ​	**分布式系统领域中的大多数研究都与"没有什么是完全可靠的"这一事实有关：通信信道可能会延迟、乱序甚至无法传递消息；进程可能会暂停、变慢、崩溃、失控或突然停止响应**。

  ​	并行编程和分布式编程领域中有许多共同主题，因为CPU也可以看作一个具备通信链路、进程和通信协议的微型的分布式系统。你将在11.5节中看到分布式编程和并发编程的许多相似之处。但是多数的原语无法在二者之间直接复用，这是因为远程通信的成本要大得多，并且通信链路和进程是不可靠的。

  ​	为了克服分布式环境的困难，我们需要用到一类特殊的算法——分布式算法，它定义了本地和远程的状态以及执行的概念，即便在不稳定的网络或发生组件故障的情况下也能工作。为了解释这些算法，我们会用到术语状态和步骤(或阶段， phase)，并描述它们之间的转换(transition)。毎个进程都在本地执行算法步骤，而本地执行以及进程之间的交互构成了分布式算法。

  ​	分布式算法描述了本地的行为和多个独立节点的交互过程。节点通过相互发送消息进行通信。算法定义了参与者的角色、要交换的消息、状态、转换、执行步骤、传递介质的特性、时序假设、故障模型，以及其他描述进程和进程间交互的特性。

  ​	分布式算法有很多不同的用途:

  + **协调**

    监督若干工作者的动作和行为的进程。

  + **合作**

    多个参与者互相依靠，共同完成任务。

  + **分发**

    进程相互配合，将信息快速而可靠地分发给感兴趣的进程。

  + **共识**

    在多个进程间达成共识。

  ​	本书中，我们将站在使用的角度讨论算法，相比纯学术的材料，我们更倾向于实用的方法。首先，我们会介绍所有必要的抽象，包括进程和它们间的联系，并逐步构建出更复杂的通信模式。我们将会从UDP开始：使用UDP时，发送者无法确认它的消息是否已送达目的地。最终，在系统中达成共识，即多个进程都认同某个特定值。

# 8. 简介与概述

## 8.1 并发执行

​	一旦两个执行线程都能访问变量，除非我们在线程间同步这些步骤，否则这些并发步骤的执行结果是无法预知的。

​	即便仅在单个节点上，我们就已经遇到了分布式系统中的第一个问题：并发。每个并发程序都具有分布式系统的某些特性。线程访问共享状态，在本地执行一些运算，再将结果传回共享变量。

​	为了精确定义执行历史并减少可能的结果数量，我们需要一致性模型。一致性模型描述并发执行的过程，并且确定了运算执行以及对其他参与者可见的顺序。使用不同的一致性模型，我们可以约束或放松系统可能的状态数量。

​	分布式系统和并发计算在术语和学术研究上有许多重叠之处，但也存在一些差异。并发系统中存在共享内存，进程可以用它来交换信息。在分布式系统中，各个进程拥有自己的本地状态，参与者之间通过传递消息进行通信。

> **并发与并行**
>
> ​	我们经常互换使用并发和并行计算这两个术语，但是这两个概念在语义上有细微的差异。当两个步骤序列并发执行时，二者都在进行中，但任意时刻都只有其中一个在执行。当两个步骤序列并行执行时，它们的步骤可以(在某一时刻)同时执行。并发的操作时间上存在重叠，而并行的操作由多个处理器执行[WEIKUM01]
>
> ​	Erlang编程语言的创建者 Joe Armstrong举过一个例子：并发执行就像一台咖啡机前排了两队，而并行执行就像两台咖啡机前排了两队。即便如此，绝大部分资料都用术语"并发"来描述拥有多个并行执行线程的系统，而"并行"这个词则很少见。

---

+ 分布式系统中的共享状态

  ​	我们可以尝试在分布式系统中引入共享内存的概念，例如，单一信息源(比如数据库)。

  ​	即使我们解决了并发访问的问题，我们依然无法保证所有进程都是同步的。

  ​	为了访问数据库，进程需要通过通信介质发送和接受消息，以查询或修改状态。但是，如果一个进程很久都没有从数据库得到响应会如何？为了回答这个问题，我们首先要定义什么是很久。为此，必须<u>从同步性的角度来描述系统：通信是否是全异步的？是否存在某些时序假设？如果存在的话，这些时序假设将允许我们引入操作超时和重试机制</u>。

  ​	我们无从知晓数据库没有响应是因为过载、不可用、响应太慢还是网络问题。这描述了崩溃的本质——进程可能以各种方式崩溃：可能因某种原因无法继续执行后面的算法步骤；可能遇到了临时性的故障；也可能是消息丟失。我们需要定义一个**故障模型**并描述故障可能发生的方式，然后再决定如何处理它们。

  ​	<u>如果系统在故障发生时仍然能继续正常运行，我们将这样的特性称为**容错性**</u>。故障是不可避免的，所以我们需要构建出具有可靠组件的系统。消除单点故障，比如前文提到的单节点数据库，可能是我们朝此方向迈出的第一步。我们可以引入一些冗余，增设备份数据库。然而这就引出了另一个问题：如何使共享状态的多个副本保持同步?

  ​	到目前为止，在我们这个简单系统中引入共享状态所带来的问题比答案还多。现在我们知道，共享状态不像引入数据库那样简单，还必须采取更细化的方法，即基于消息传递来描述各个独立进程之间的交互。

## 8.2 分布式计算的误区

​	理想情况下，当两台计算机在网络上通信时，一切都能正常工作：进程开启一个连接、发送数据、收到响应，每个人都很开心。但是假设所有操作总会成功并且没有任何错误是很危险的，因为当某些东西出问题时，我们的假设也就不成立了，那时系统的行为将变得难以预测。

​	大多数时候，假设网络可靠是合理的。网络至少在一定程度上可靠才能有用。我们都曾经历过这样的情况，当我们尝试连接到远程服务器时，却收到了一个"网络不可达"的错误。即使能建立连接，一个成功的初始连接也无法保证这条链路是稳定的，连接随时可能中断。消息可能送达了对端，但对端的响应却可能丢失了，也有可能在对端的响应发送之前连接就中断了。

​	网络交换机会有故障，电缆可能断开，网络配置也随时可能发生变化。我们构建系统时需要适当地处理所有这些情况。

​	连接可以是稳定的，但我们不能期望远程调用能像本地调用一样快。我们应尽可能少地对延迟做出假设，并且永远不要假设延迟为零。一条消息要想到达远程服务器，需要穿过若干个软件层和一个物理媒介(比如光纤或电缆)，所有这些操作都不是瞬间完成的。

​	Michael Lewis在他所著的书Flash Boys(Simon and Schuster公司出版)中讲述了这样个故事，公司花费数百万美元把延迟降低几毫秒，从而能比竞争对手更快地访问交易所。这是一个把延迟作为竞争优势的绝佳例子，然而值得一提的是，根据其他一些研究，比如文献[BARTLETTI6]，过时报价套利(通过比竞争对手更快地得知价格并执行交易来获取利润)并不能使快速交易者从市场中获利。

​	从上述教训当中学习，我们增加了重试和重连机制，并去掉了关于瞬间执行的假设，但是事实证明这还不够：**当我们增加消息的数量、发送速率和大小，并向现有网络中添加新的进程时，我们不应该假设带宽是无限的**。

> 1994年， Peter Deutsch发布过一个如今很有名的断言列表，标题为"分布式计算的误区"，描述了分布式计算中易被忽视的一些方面。除了网络可靠性、延迟和带宽假设，它还提到了其他问题，比如，网络的安全性、可能存在的攻击者、有意或无意的拓扑变化都可能打破我们的一些假设，这些假设包括：某一资源存在性和所在位置，网络传输所消耗的时间和资源，以及最后——存在一个拥有整个网络的知识和控制权的权威个体。

​	Deutsch的列表可以说非常详尽，但它侧重于通过链路传递消息时可能出错的地方。这些担忧是合理的，而且描述了最通用、最底层的复杂性，但不幸的是，在设计和实现分布式系统时，我们还做出了很多其他假设，这些假设也可能在运行中导致问题。

### 8.2.1 处理

​	在远程进程响应刚刚收到的消息之前，它还需要在本地执行一些工作，因此我们不能假定处理是瞬时完成的。只考虑网络延迟还不够，因为远程进程执行的操作也不是立即完成的。

​	此外，我们还无法保证消息送达后会立刻被处理。消息可能会进入远程服务器的等待队列中，等到所有更早到达的消息处理完后才被处理。

​	节点可能相距很近，也可能很远，各节点可能有不同的CPU、内存和磁盘配置，可能运行不同的软件版本和配置。我们不能期望它们以相同的速度处理请求。<u>如果完成一项任务需要等待几个并行工作的远程服务器响应，则**整个执行的完成时间取决于最慢的服务器**</u>。

​	与普遍存在的看法相反，队列容量并非是无限的，堆积更多的请求不会对系统有任何好处。当生产者产生消息的速度大于消费者能够处理的速度时，我们可以使用**背压(backpressure)**策略减慢生产者的速度。背压是分布式系统中人们了解和应用最少的概念之一，通常是事后才建立，而不是将其视为系统设计必需的一个组成部分。

​	尽管增加队列容量听起来像是个好主意——可以帮助我们管道化、并行化以及有效地调度请求，但是，如果消息仅仅是停在队列中等待处理，什么也不会发生。**增大队列大小可能对延迟产生负面影响，因为这并不会改善处理速度**。

​	通常，进程本地队列是用于实现以下目标：

+ **解耦**

  使接收和处理在时间上分开，并各自独立发生。

+ **流水线化**

  不同阶段的请求由系统中独立的部分处理。负责接收消息的子系统不用阻塞到上条消息处理完成。

+ **吸收瞬时突发流量**

  系统负载可能经常变化，但是请求到达的间隔时间对负责处理请求的组件是隐藏的。总体的系统延迟会由于排队而增加，但这通常仍比响应失败并重试请求更好。

​	队列大小取决于工作负载和应用程序。对于相对稳定的工作负载，我们可以通过测量任务处理时间以及各任务的平均排队时间来确定队列大小，从而确保在提升吞吐量的同时，延迟仍保持在可接受的范围内。在这种情况下，队列大小相对较小。对于不可预测的工作负载，可能会出现任务提交的突发流量，这时队列大小也应当考虑突发流量和高负载。

​	即使远程服务器可以快速地处理请求，也并不意味着我们总是能获得正面的响应。它也可能回应一个失败：无法进行写操作、要查找的值不存在或是触发了bug。总之，即使是最顺利的情况也需要我们的关注。

### 8.2.2 时钟和时间

​	假设不同的远程计算机上的时钟都同步也很危险。再加上延迟为零以及处理是瞬时的这些假设，将会导致不同的特质，尤其是在时序和实时数据处理中。例如，当从时间感知不同的参与者收集和聚合数据时，你必须了解它们之间的时间漂移并相应地对时间进行归一化，而不是依赖源时间戳。除非使用特殊的髙精度时间源，否则不能依赖时间戳进行同步或排序。当然，这并不意味着我们完全不能或不该依赖时间：说到底，<u>任何同步系统都依靠本地时钟实现超时</u>。

​	**我们必须始终注意进程之间可能存在的时间误差，以及传递和处理消息所需的时间**。例如，Spanner(参见13.5节)使用特殊的时间API，该API返回时间戳和不确定性界限以施加严格的事务顺序。一<u>些故障检测算法依赖于共享的时间概念，要求时钟漂移始终在允许的范围内才能确保正确性</u>[GUPTA01]。

​	除了分布式系统中的时钟同步非常困难之外，当前时间也在不断变化：你可以从操作系统请求当前的POSIX时间戳，并在执行几个步骤后请求另一个当前时间戳，两次结果是不同的。尽管这是一个明显的现象，但是了解时间的来源以及时间戳捕获的确切时刻至关重要。

​	了解时钟源是否是单调的(即永远不会后退)，以及与调度时间相关的操作可能偏移多少，可能也会有所帮助。

### 8.2.3 状态一致性

​	之前说到的假设大多属于"几乎总是错的"一类，但是，还有一些假设最好归入"并非总是对的"一类。这类假设帮助我们走思维捷径，通过以特定方式思考来简化模型，忽略某些棘手的边缘情形。

​	分布式算法并不总是保证状态严格一致。<u>一些方法具有较宽松的约束，允许各副本之间的状态存在分歧，并依赖冲突解决(检测和解决系统内分歧状态的能力)和读取时数据修复(读取期间，当各副本响应不同结果时，使副本恢复同步的能力)</u>。有关这些概念的更多信息参见第12章。假定状态在节点间完全一致可能会导致难以察觉的bug。

​	最终一致的分布式数据库可能具有这样的逻辑：读取时通过查询Quorum的节点来处理副本不一致，但是假定数据库表结构和集群视图是强一致的。除非我们确保这些信息的一致性，否则依赖该假设可能会造成严重的后果。

​	例如， Apache Cassandra曾有一个bug，其原因是表结构变更在不同时刻传播到各个服务器。如果在表结构传播过程中尝试从数据库读取数据，则可能会读到损坏的数据，因为一台服务器以某种表结构进行编码，而另一台服务器使用不同的表结构对其进行解码。

​	另一个例子是由环的视图分歧引起的bug：如果一个节点假定另一个节点保存了某个键的数据记录，但另一个节点具有不同的集群视图，此时读写数据可能会导致数据记录被错误放置，或是获得一个空的响应，虽然数据实际上好端端地存放在另一个节点上。

​	即使完全的解决方案成本很高，我们也最好事先考虑各种可能的问题。通过了解和处理这些情况，你能以更自然的方式解决问题，比如内置防护措施或修改设计。

### 8.2.4 本地和远程执行

​	将复杂性隐藏在API内部可能很危险。例如，对于本地数据集上的一个迭代器，即使你对存储引擎不熟悉，也可以合理地推测内部行为。理解远程数据集上的迭代过程则是个完全不同的问题：你需要理解一致性、传递语义、数据协调、分页、合并、并发访问含义以及许多其他事情。

​	简单地将两者隐藏在同一个接口后，即便有用，也可能会产生误导。调试、配置和可观察性可能需要额外的API参数。我们应该始终牢记，**本地执行和远程执行是不同的**[ WALDO96]。

​	**隐藏远程调用最明显的问题是延迟：远程调用的成本比本地调用高很多倍，因为它涉及<u>双向网络传输、序列化/反序列化</u>以及许多其他步骤**。交错使用本地调用和阻塞的远程调用可能会导致性能下降和预期之外的副作用[VINOSKIO08]。

### 8.2.5 处理故障的需要

​	刚开始构建系统的时候，我们可以假设所有节点都可以正常工作，但如果总是这么想就很危险了。在长时间运行的系统中，节点可能会关机维护(通常会有个优雅关闭的过程)或因为种种原因(例如软件问题、内存耗尽(out-of-memory killer [KERRISK10])、运行时bug、硬件问题等)而崩溃。<u>进程会发生故障，而你能做的最好的事情就是做好准备并知道如何处理它们</u>。

​	如果远程服务器没有响应，我们并不总是知道确切的原因。这可能是由系统崩溃、网络故障、远程进程或中间链路太慢导致的。<u>一些分布式算法使用**心跳协议**和**故障检测**机制来确定哪些参与者还活着且可达</u>。

### 8.2.6 网络分区和部分故障

​	<u>当两个或更多服务器无法相互通信时，我们称这种情况为**网络分区**</u>。Seth Gilbert和Nancy Lynch在 Perspectives on the CAP Theorem[GILBERT12]中区分了以下两种情况：两个参与者无法相互通信；几组参与者彼此隔开，无法交换消息并继续运行算法。

​	网络的总体不可靠性(数据包丢失、重传、延迟难以预测)令人烦恼但尚可容忍，而网络分区则会造成更多的麻烦，因为各个独立的分组可以继续执行并产生冲突的结果。**网络链路的故障也可能是不对称的：消息仍然能从一个进程传递到另一个进程，反之则不行**。

​	为了构建在一个或多个进程出现故障的情况下仍健壮的系统，我们必须考虑**部分故障**的情况[TANENBAUM06]，如何让系统在部分不可用或运行不正常的情况下仍能继续工作。

​	故障很难检测，并且在系统的不同部分看来，不总是以相同的方式可见。设计高可用性系统时，我们应该始终考虑边缘情形：如果我们确实复制了数据却没有收到确认该怎么办？要重试吗？在发送了确认的节点上，数据仍可用于读取吗？

​	**墨菲定律告诉我们故障一定会发生**。编程界又补充道，故障将以最坏的方式发生。因此，作为分布式系统工程师，我们的工作是尽可能减少可能出现错误的场景，并为故障做好准备——包括这些故障可能导致的破坏。

​	避免一切故障是不可能的，但我们仍可以构建一个弹性的系统，使之在故障出现时仍然能正常运行。**<u>设计应对故障的最佳方式是进行故障测试</u>**。我们无法考虑清楚毎种可能的故障场景，并预测多个进程的行为。最好的解决方法就是通过测试工具来制造网络分区、模拟比特位腐烂[GRAY05]、增加延迟、使时钟发生偏移以及放大相对处理速度。现实世界中分布式系统的设置可能是对抗性的、不友好的，甚至是“有创造性的”(然而以非常敌对的方式)，因此测试工作应当尝试覆盖尽可能多的场景。

> 过去几年中出现了一些开源项目，它们能帮助我们构造出各种故障场景。
>
> + Toxiproxy用于<u>模拟网络问题</u>：<u>限制带宽、引入延迟、超时</u>等。
> +  Chaos Monkey的方法更为激进，它通过<u>随机关闭服务使工程师直面生产环境故障的风险</u>。
> + CharybdeFS<u>模拟文件系统及硬件错误与故障</u>。你可以用这些工具来测试软件以确保在这些故障出现时软件仍能正确工作。 
>
> + CrashMonkey是一个与文件系统无关的记录-重放-测试框架，用于测试持久性文件的数据及元数据一致性。

​	设计分布式系统时，我们必须认真考虑容错性、弹性，以及可能的故障场景和边缘情形。类似于"足够多的眼睛，就可让所有问题浮现"，我们可以说足够大的集群最终定会命中所有可能的问题。与此同时，只要有足够多的测试，我们最终能够发现每个存在的问题。

### * 8.2.7 级联故障

​	我们做不到总是完全隔离故障：**被高负载压垮的进程会增加集群其余部分的负载，从而使其他节点更有可能发生故障。级联故障能够从系统的一部分传播到另一部分，扩大了问题的范围**。

​	有时，级联故障甚至可能来源于完全善意的目的。例如，某个节点离线了一段时间，因而没有接收到最近的更新。当它恢复在线时，乐于助人的其他节点希望帮助它追赶上最近的变化，于是开始向它发送缺失的数据，而这又导致网络资源耗尽，或是导致该节点启动后短时间内再次发生故障。

> ​	为了防止系统的故障扩散并妥善处理故障场景，我们可以使用**断路器(或熔断机制)**。在电气工程中，断路器可通过中断电流来保护昂贵且难以更换的部件，使其免受电流过载或短路的影响。在软件开发中，熔断机制会监视故障，并使用**回退(fallback)**机制保护整个系统：避免使用出故障的服务，给它一些时间进行恢复，并妥善处理失败的调用。

​	<u>当与某一台服务器的连接失败或服务器没有响应时，客户端将开始循环重连。那时候，过载的服务器已经难以应付新的连接请求，因而客户端的循环重试也无济于事。为了避免这一情况，我们可以使用**退避(backoff)**策略，客户端不要立即重试，而是等待一段时间</u>。

​	**退避通过合理安排重试、增加后续请求之间的时间窗口来避免问题扩大**。

​	退避用于增加单个客户端的请求间隔。但是，<u>使用相同退避策略的多个客户端也会产生大量负载</u>。为了防止多个客户端在退避期之后同时重试，我们可以引入**抖动(Jitter)**。<u>抖动在退避上增加了一个小的随机时间间隔，从而降低了多个客户端同时醒来并重试的可能性</u>。

​	<u>硬件故障、比特位腐烂和软件错误都会导致数据损坏，而损坏的数据会通过标准的传递机制传播</u>。如果没有适当的验证机制，系统可能将损坏的数据传播到其他节点，甚至可能覆盖未损坏的数据记录。为了避免这一情况，我们应该采用**校验和(checksum)**以及验证机制，来验证节点之间交换的任何内容的完整性。

​	**通过计划和协调执行可以避免过载和热点问题。<u>相比于让各个对等节点独立执行操作步骤，我们可以用协调器来依据可用资源准备一份执行计划，并根据过去的执行数据来预测负载</u>**。

​	总之，我们应该始终考虑这样的情形：系统某一部分的故障可能导致其他地方也出现问题。我们应该为系统装备上**熔断**、**退避**、**验证**和**协调**机制。<u>处理被隔离的小问题总比从大规模故障中恢复更简单</u>。

​	我们用整整一节讨论了分布式系统中的问题和潜在的故障场景，但是我们应当将其视为警告，而不是被它们吓跑。

​	了解什么会出问题，并仔细设计和测试我们的系统，可以让它更健壮、更具弹性。了解这些问题可以帮助你在开发过程中识别、发现潜在的问题根源，也能帮助你在生产环境中调试

## * 8.3 分布式系统抽象

​	讨论编程语言时，我们使用<u>通用术语</u>并用<u>函数</u>、<u>运算符</u>、<u>类</u>、<u>变量</u>和<u>指针</u>来定义我们的程序。通用的词汇可以帮助我们避免每次都为了描述某些东西而发明新词。我们的定义越精确、越没有歧异，听众也就越容易理解。

​	在开始学习算法之前，我们首先要了解分布式系统中的词汇：这些定义你会经常在演讲书籍和论文中遇到。

---

+ **链路**

  ​	网络是不可靠的：消息会丢失、延迟或被打乱。记住这一点之后，我们来尝试构建几种通信协议。我们从最不可靠的协议开始，确定它们可能处于的状态，然后找出可以为协议增加的东西使它提供更好的保证。

  **公平损失链路**

  ​	我们可以从两个进程开始，它们之间以**链路**相连。进程可以相互发送消息。任何通信介质都是不完美的，消息可能丢失或延迟。

  ​	看看我们能得到什么样的保证。消息M被发送之后(从发送方的角度来看)，它可能处于以下状态之一：

  + 还未送达进程B(但会在某个时间点送达)
  + 在途中丢失且不可恢复
  + 成功送达远程进程

  ​	注意，发送方没有任何方法确定消息是否已经送达。在分布式系统的术语中，这种链路称为**公平损失(fair-loss)**。这种链路具有以下属性：

  **公平损失**

  ​	如果发送方和接收方都是正确的，且发送方无限多次重复发送，则消息最终会被送达。

  **有限重复**

  ​	发送的消息不会被送达无限次。

  **不会无中生有**

  ​	链路不会自己生成消息。换句话说，它不会传递一个从未发送过的消息。

  ​	公平损失链路是一种很有用的抽象，它是构建具有更强保证的通信协议的基石。我们可以假设该链路不会在通信双方之间*系统性地*丢弃消息，也不会创建新消息。但与此同时，我们也不能完全依靠它。这可能让你想起了用户数据报协议(UDP)，UDP允许我们从一个进程发送消息到另一个进程，但在协议层面上不提供可靠的传输语义。

  **消息确认**

  ​	为了改善这一情况、更清晰地获得消息状态，我们可以引入**确认(acknowledgment)机制**：接收方通知发送方消息已送达。为此，我们需要双向通信信道，并增加一些措施以区分不同的消息，例如序列号——单调递增的唯一消息标识符。

  > ​	每个消息只要有唯一标识符就足够了。序列号只是唯一标识符的一种特殊情况，即使用计数器来获取标识符，从而实现唯一性。<u>当使用哈希算法来唯一地标识消息时，我们应当考虑可能的冲突，并确保能消除歧义</u>。

  ​	现在，进程A可以发送消息M(n)，其中n是单调递增的消息计数器。B收到消息后立即向A发送确认ACK(n)。

  ​	确认消息，就像原始消息一样，也有可能在途中丟失。消息可能处于的状态数会稍有变化。在A收到确认之前，该消息仍处于我们前面提到的三种状态之一，但是，一旦A收到确认，就可以确信该消息已送达B。

  **消息重传**

  ​	增加确认机制仍不足以保证通信协议完全可靠：发送的消息仍可能会丢失，远程进程也可能在确认之前发生故障。为了解决该问题并提供送达保证，我们可以尝试**重传**(retransmit)。重传是指发送方重试可能失败的操作。我们之所以说可能失败，是因为发送方并不能真的知道有没有失败，因为我们要讨论的链路<u>不使用确认机制</u>。

  ​	进程A发送消息M之后，它将等到超时T被触发，然后尝试再次发送同一条消息。假设进程之间的链路完好无损，进程间的网络分区不会无限持续下去，并且并非所有数据包都丢失，我们可以认为，从发送方的角度看，消息要么尚未送达进程B，要么已经成功送达。由于A一直在尝试发送消息，可以认为传输过程中不会发生不可恢复的消息丢失。

  ​	在分布式系统的术语中，这种抽象称为**顽固链路(stubborn link)**。之所以称为顽固，是因为发件人会<u>无限期地反复发送消息</u>，但是，由于这种抽象非常不切实际，因此我们需要将重试与确认结合起来。

  **重传的问题**

  ​	每当我们发送消息时，在收到远程进程的确认之前，我们无从得知消息的状态：可能已被处理，可能马上就要处理，也可能已经丟失，甚至可能在收到消息之前远程进程就崩溃了——上述的任意状态都是可能的。我们可以重试操作、再次发送消息，但这可能导致消息重复。<u>只有当我们要执行的操作是幂等时，处理重复消息才是安全的</u>。

  ​	<u>幂等(dempotent)的操作可以执行多次而产生相同的结果，且不会产生其他副作用</u>。例如，服务器关机操作可以是幂等的，第一次调用将发起关机，而所有后续调用都不会产生任何其他影响。

  ​	如果毎个操作都是幂等的，那我们可以少考虑一些传递语义，更多地依赖重传来实现容错，并以完全反应式的方式构建系统：为某些信号触发相应的操作，而不会引起预期之外的副作用。但是，操作不一定是幂等的，简单地假设它们幂等可能会导致集群范围的副作用。例如，向客户的信用卡收费不是幂等操作，绝对不可以重复收费多次。

  ​	在存在部分故障和网络分区的情况下，幂等性尤其重要，因为我们无法总是确定远程操作的确切状态——是成功还是失败，还是会马上被执行——我们只能等待更长的时间。<u>**保证毎个操作都是幂等的是不切实际的，因此我们需要在不改变实际操作语义的情况下，提供与幂等性等价的保证**。为此，我们可以使用去重来避免多次处理消息</u>。

  **消息顺序**

  ​	不可靠的网络给我们带来了两个问题：一是消息可能会乱序到达；二是由于重传某些消息可能会多次送达。我们已经引入了序列号，利用这些消息标识符我们可以在接收方确保先进先出(FIFO)的顺序。由于每条消息都有一个序列号，因此接收方可以跟踪下列信息：

  + n<sub>consecutive</sub>表示最大连续序列号：所有小于或等于该序列号的消息都已经收到，这些消息可以按顺序放到正确的位置上。
  + n<sub>processed</sub>表示最大已处理序列号：所有小于或等于该序列号的消息都已经按照原来的顺序被处理。此序列号可以用于去重。

  ​	如果收到的消息序列号不连续，接收方会将其放入重新排序缓冲区。例如，它在接收到序列号为3的消息后收到消息5，那我们就知道4还是缺失的，因此我们将5放在一旁，直到4到来，然后就能构造出原本的消息顺序。<u>由于通信构建在公平损失链路之上，可以认为n<sub>consecutive</sub>和n<sub>max_seen</sub>之间的消息最终一定会送达</u>。

  ​	接收方可以安全地丢弃收到的序列号小于等于n<sub>consecutive</sub>的消息，因为这些消息确定已经送达了。去重的工作原理是检查带有序列号n的消息是否已被处理(已被传给网络栈的更上层)，丢弃已处理的消息。

  ​	去重的工作原理是检査带有序列号n的消息是否已被处理(已被传给网络栈的更上层)，丢弃已处理的消息。

  ​	在分布式系统的术语中，这种类型的链路称为**完美链路**，它提供以下保证[CACHIN11]：

  **可靠传递**

  ​	正确的进程A发送一次到正确的进程B的每个消息**最终**都会被传递。

  **没有重复**

  ​	消息不会被传送多次。

  **不会无中生有**

  ​	与其他种类的链路一样，它只能传递实际由发送者发送过的消息。

  ​	这可能会让你想起TCP协议(但是，TCP仅在单个会话内保证可靠传递)。当然，上述模型仅仅是一种用于说明原理的简化表示。TCP中处理消息确认的模型更为复杂，它按组进行确认以减少协议层面的开销。另外，TCP具有选择性确认、流控、拥塞控制错误检测等很多其他功能，这些不在我们的讨论范围之内。

  **严格一次传递**

  ​	<u>关于是否可以做到严格一次传递(exactly-once delivery)这个问题已经有很多讨论。这里，语义和精确的措辞非常重要。**由于链路故障可能导致传递消息的第一次尝试无法成功，因此大多数实际的系统都采用至少一次传递(at-least-once delivery)，它确保了发送方将重试直到收到确认为止，否则就认为对方没有收到该消息**。还有一种传递语义是最多一次(at-most-once)：发送方仅仅发送消息而不期待得到任何确认</u>。

  ​	TCP协议的原理是将消息分成数据包，一个一个传输，然后在接收端将它们拼接到起。TCP可能会尝试重传某些数据包，并且可能有不止一次的传输会成功。由于TCP用序列号标记毎个数据包，即使某些数据包被发送多次，它也可以对其进行去重，确保接收方只会看到并处理一次该消息。<u>在TCP中，此保证仅对单个会话有效：如果消息被确认并处理，但是发送方在收到确认消息前连接就中断了，则应用程序并不知道此传递成功，取决于其逻辑，它可能会尝试再次发送消息</u>。

  ​	这意味着严格一次处理是个有趣的问题，因为重复的传送(或数据包传输)没有副作用，仅仅是链路尽力而为的产物。举个例子，<u>如果数据库节点仅接收到记录但还没将它持久化。在这种情况下传递已经完成了，但除非该记录可以被査到(换句话说，除非消息被传递并且处理了)，否则这次传递毫无用处</u>。

  ​	为了确保严格一次传递，各节点需要一个共同知识[HALPERN90]：每个节点都知道某件事，每个节点都知道其他所有节点也都知道这件事。<u>用简化的术语来说，**节点必须在记录状态上达成共识**：两个节点都认为该记录已经或者还未被持久化。正如本章之后会说的，这在**理论上是不可能的**，但在实践中，我们仍通过放宽协调的要求来使用这一概念</u>。

  ​	各种关于是否是严格一次发送的误解，大多是因为从不同协议和抽象层次上考虑该问题，以及对“传递”的不同定义。**要想建立可靠的链路，不可能不重复传送某些消息**。但是，<u>我们可以通过仅处理毎个消息一次并忽略重复消息，使得从发送方的角度来看是严格一次发送</u>。

  ​	现在，在建立了实现可靠通信的方法之后，我们可以继续前进，探寻实现分布式系统中进程间一致性和共识的方法。

## 8.4 两将军问题

> [【翻译】两军问题（Two Generals’ Problem） | 知研片语 (liuzhaocn.com)](https://www.liuzhaocn.com/?p=1235)

​	一个被广泛称为两将军问题的思想实验，是对分布式系统一致性的最著名的描述之一。

​	这个思想实验表明，**<u>如果链路可能发生故障并且通信是异步的，则不可能在通信的双方之间达成共识</u>**。尽管TCP具有完美链路的性质，但是务必记住：完美链路尽管被称为完美链路，并不能保证完美的传递。它们也不能保证参与方一直活着，而只关心传输本身。

​	想象现在有两支军队，分别由两位将军领导，准备进攻一座要塞城市。两支军队分别位于城市的两侧，只有在同时进攻的情况下才能获胜。

​	两位将军通过信使进行通信。他们已经制定了攻击计划，现在唯一需要达成共识的就是是否执行计划。该问题的变体包括：其中一位将军的级别较高，但需要确保攻击是有协调的；或者两位将军需要就确切时间达成共识。这些细节不会改变问题的定义：将军们需要达成一项共识。

​	将军们只需要对“他们都会发起进攻”这一事实达成共识。否则，攻击将无法成功。将军A发出一条消息MSG(N)，表明如果对方也同意的话，就在指定的时间发起进攻。

​	将军A送出信使之后，他不知道信使是否已经到达：信使可能会被抓而无法传达消息。当将军B收到消息时，他必须发送确认ACK(MSG(N))。一条消息由一方发送并由另一方确认。

​	传递确认消息的信使也可能会被抓而无法传达消息。B无从得知信使是否已成功送达确认消息。

​	为了确认这一点，B必须等待ACK(ACK(MSG(N)))，一个二阶的确认，用于确认A收到了确认。

​	**<u>无论将军们互相发送多少确认，他们始终距离安全地发起攻击还差一个ACK。将军们注定要怀疑最后一个确认消息是否已送达目的地</u>**。

​	注意我们没有做任何时序上的假设:将军间的通信是完全异步的。并没有一个上限约束将军必须在多长时间内做出回应。

## 8.5 FLP不可能定理

​	Fisher、Lynch和Paterson在论文中描述了一个著名的问题：FLP不可能问题[FISCHERI85] (FLP是作者姓氏的首字母)，论文讨论了一种共识形式：各进程启动时有一个初始值，并尝试就新值达成共识。算法完成后，所有正常进程上的新值必须相同。

​	如果网络完全可靠，很容易对特定值达成共识。但实际上，系统容易出现各式各样的故障，例如消息丟失、重复、阒络分区，以及进程缓慢或崩溃。

​	共识协议描述了这样一个系统：给定初始状态的多个进程，它将所有进程带入决定状态。一个正确的共识协议必须具备以下三个属性：

+ 一致性

  ​	协议达成的决定必须是一致的：每个进程都做出了决定且所有进程决定的值是相同的。否则我们就尚未达成共识。

+ 有效性

  ​	达成共识的值必须由某一个参与者提出，这意味着系统本身不能“提出”值。这也意味着这个值不是无关紧要(trivial)的：进程不能总是决定某个预定义的默认值。

+ 终止性

  ​	只有当所有进程都达到决定状态时，协议才算完成。

​	<u>文献[FISCHER85]假定处理过程是完全异步的，进程之间没有共享的时间概念。这样的系统中的算法不能基于超时，并且一个进程无法确定另一个进程是崩溃了还是仅仅运行太慢。论文表明，在这些假设下，不存在任何协议能保证在有限时间内达成共识。**完全异步的共识算法甚至无法容忍一个远程进程无通知地突然崩溃**</u>。

​	**<u>如果我们不给进程完成算法步骤设定一个时间上限，那么就无法可靠地检测出进程故障，也不存在确定性的共识算法</u>**。

​	但是，FLP不可能定理并不意味着我们要收拾东西回家(由于达成共识是不可能的)。**<u>它仅仅意味着我们不能总是在有限的时间内在一个异步系统中达成共识</u>**。实践中，系统至少会表现出一定程度的同步性，而要想解决共识问题还需要一个更完善的模型。

## 8.6 系统同步性

​	从FLP不可能定理中可以看出<u>**时序假设**是分布式系统的关键特征之一</u>。在异步系统中，我们不知道进程运行的相对速度，也不能保证在有限时间内或以特定顺序传递消息。进程可能要花无限长的时间来响应，而且无法总是可靠地检测到进程故障。

​	对异步系统的主要批评在于上述假设不切实际：进程不可能具有任意不同的处理速度，链路传递消息的时间也不会无限长。依赖时间能够简化推理，并提供时间上限的保证。

​	在异步模型中不一定能解决共识问题[FISCHER85]。而且，不一定能设计出高效的异步算法。<u>对于某些任务，切实可行的解决方案很可能需要依赖时间</u>[ ARJOMANDIS83]。

​	我们可以放宽一些假设，认为系统是同步的。为此我们引入了时间的概念。在同步模型下对系统进行推理要容易得多。它假定各进程的处理速度相近、传输延迟是有限的，并且消息传递不会花任意长的时间。

​	同步系统也可以表示为同步的进程本地时钟：两个进程本地时间源之间的时间差存在上限[CACHIN11]。

​	**在同步模型中设计系统可以使用超时机制**。我们可以构建更复杂的抽象，例如**领导者选举、共识、故障检测**以及基于它们的其他抽象。<u>这使得最佳情况的场景更加健壮，但是如果时序假设不成立则可能导致故障</u>。

​	例如：Raft共识算法(参见14.4节)中，可能最终有多个进程认为它们是领导者，为了解决该问题，我们强制滞后的进程接受其他进程成为领导者；故障检测算法(参见第9章)。可能会错误地将活动进程标记为故障，反之亦然。设计系统时，我们必须考虑这些可能性。

​	异步和同步模型的性质可以组合使用，我们可以将系统视为部分同步的。部分同步的系统具有同步系统的某些属性，但是消息传递、时钟漂移和相对处理速度的边界范围可能并不精确，并且仅在大多数时候成立[DWORK88]。

​	同步是分布式系统的基本属性：它对性能、扩展性和一般可解性有影响，并且有许多对系统正常工作来说是必要的因素。本书中讨论的一些算法就工作在同步系统的假设下。

## 8.7 故障模型

​	我们一直在提到故障这个词，但到目前为止，它还是一个十分宽泛的概念，可能包含多种含义。就像我们可以做出不同的时序假设那样，我们也可以假设存在不同种类的故障。故障模型准确地描述了分布式系统中的进程可能以怎样的方式崩溃，并基于这些假设来开发算法。例如，我们可以假设进程可能崩溃并且永远无法恢复，或者可以预期它将在一段时间后恢复，或者它可能会失控并且产生错误的值。

​	分布式系统中，进程互相依赖以共同执行算法，因此故障可能导致整个系统的执行错误。

​	我们将讨论分布式系统中现有的多种故障模型，例如崩溃、遗漏和任意故障。这个列表并非面面俱到，但它涵盖了在实际中的大多数重要场景。

### 8.7.1 崩溃故障

​	通常，我们期望进程正确执行算法的所有步骤。最简单的崩溃方式是进程停止执行接下来的算法步骤，并且不再发送任何消息给其他进程。换句话说，该进程崩溃了。大多数情况下，我们使用**崩溃-停止(crash-stop)**进程抽象的假设，它规定一旦进程崩溃就会保持这种状态。

​	该模型不假定该进程无法恢复，也不阻拦或试图阻止恢复。这仅仅意味着该算法的正确性或活动性不依赖于恢复过程。实际上，并没有什么东西会去阻止进程恢复、追上系统状态以及参与下一次的算法执行。

​	失败的进程无法再继续参与当前这一轮的协作。为恢复的进程分配一个新的、不同的ID不会使模型等价于崩溃-恢复模型(之后会讨论)，因为大多数算法使用预定义的进程列表，并且依据最多可容忍的故障数明确定义了故障的语义[CACHIN11]。

​	**崩溃-恢复(crash-recovery)**是另一种的进程抽象。在这个抽象中，进程停止执行算法步骤，但会在稍后恢复并尝试执行剩下的步骤。要想让恢复成为可能，需要在系统中引入持久状态以及恢复协议[SKEEN83]。允许崩溃-恢复的算法需要考虑所有可能的恢复状态，因为恢复的进程会尝试从最后一个已知的步骤开始继续执行。

​	想利用恢复的算法必须同时考虑状态和进程ID。在这种情况下，崩溃恢复也可以看作是遗漏故障的一种特殊情况，因为<u>从另一个进程的角度看，不可达的进程与崩溃再恢复的进程没什么区别</u>。

### 8.7.2 遗漏故障

​	另一个故障模式是**遗漏故障(omission fault)**。该模型假设故障进程跳过了某些算法步骤，或者无法执行这些步骤，或者执行过程对其他参与者不可见，或者无法与其他参与者通信。遗漏故障中包含了由于网络链路故障、交换机故障或网络拥塞而导致的网络分区。网络分区可以表示为单个进程或进程组之间的消息遗漏。进程崩溃可以模拟为遗漏所有该进程收发的消息。

​	如果进程的运行速度慢于其他参与者，发送响应比预期迟得多，那么对于系统的其余部分来说，这个节点看起来丢三落四的。慢节点没有完全停止，而是发送结果太慢，常常与其他节点不同步。

​	<u>如果本应执行某些步骤的算法跳过了这些步骤或者执行结果不可见时，就发生了遗漏故障</u>。例如，消息在送往接收方的途中丢失，而发送方就像消息发送成功时那样，没有再次发送而是继续运行，即使消息已经不可恢复地丟失了。遗漏故障也可能是由间歇性停顿、网络过载、队列满等引起的。

### 8.7.3 任意故障

​	最难以解决的故障种类是**任意故障**或**拜占庭故障(Byzantine fault)**：<u>进程继续执行算法步骤，但是以与违背算法的方式(例如，共识算法中的进程决定一个从未由任何参与者提出过的值)</u>。

​	此类故障可能是由于软件bug或运行不同版本算法的进程，在这种情况下，故障很容易被发现和理解。如果我们无法控制所有进程，并且其中一个进程有意地误导其他进程，则发现和理解故障会变得非常困难。

​	你可能在航空航天工业中听说过拜占庭式的容错：飞机和航天器的系统不会直接使用子部件传来的值，而是会对结果进行交叉验证。另一个广泛的应用是加密货币[GILAD17]，那里没有中央权威，节点被多方控制，并且敌对的参与者有强烈的动机通过提供错误响应来欺骗系统。

### 8.7.4 故障处理

​	我们可以通过构成进程组、在算法中引入冗余来掩盖故障：即使其中一个进程发生故障，用户也不会注意到[CHRISTIAN91]。

​	故障可能会带来一些性能损失：正常的执行依赖于进程可响应，而且系统必须回退到较慢的执行路径来处理故障和纠正错误。故障往往可以通过一些方式来避免，例如：代码审査、广泛的测试、引入超时重试机制确保消息送达，以及确保各算法步骤在本地按顺序执行。

​	我们这里介绍的大多数算法都基于崩溃-故障模型，并通过**引入冗余来解决故障**。这些假设帮助我们创造性能更好、更易于理解和实现的算法

## 8.8 本章小结

​	本章中，我们讨论了一些分布式系统的术语，并介绍了一些基本概念。我们讨论了分布式系统的固有困难和复杂性，这是由于系统组件不可靠性导致的：链路可能无法传递消息、进程可能崩溃、网络可能发生分区。

​	这些术语应该足够让我们继续讨论。本书的剩余部分将讨论分布式系统中常见的解决方案:我们将先回想下哪些地方可能会出问题，然后看看有哪些可用的选项。

# * 9. 故障检测

​	为了使系统对故障做出适当的反应，应该及时检测故障。其他进程可能联系发生错误的进程，即使它无法响应，这会增加延迟并降低整个系统的可用性。

​	在异步分布式系统中检测故障(即不做任何时序假设)是极其困难的，因为我们无法判断进程是崩溃了，还是运行缓慢而需要无限长的时间来响应。我们在8.5节中讨论了与此相关的问题。

​	诸如死亡(dead)、失效(failed)和崩溃(crashed)等术语通常用于描述完全停止执行其步骤的进程。而<u>诸如无响应(unresponsive)、有故障(faulty)和缓慢(slow)等术语用于描述可疑进程，这些进程可能实际上已经死亡</u>。

​	故障可能发生在链路层上(进程之间的消息丢失或传递缓慢)，或者发生在进程层上(进程崩溃或运行缓慢)，而缓慢可能不一定能与故障区分开来。这意味着在如下两个方面总是面临一个杈衡：

+ 将活着的进程错误地怀疑为死的(产生假阳性)
+ 推迟将无响应的进程标记为死的并期望它最终做出响应(产生假阴性)。

​	**故障检测器(failure detector)**是一个本地子系统，其负责识别故障或无法到达的进程，将它们排除在算法之外，并在维持安全性的同时保证算法的活动性。

​	<u>**活动性**和**安全性**是描述算法解决特定问题的能力及其输出正确性的属性</u>。

​	更正式地说，**活动性(liveness)是一种保证特定预期事件必须发生的属性**。例如，如果其中一个进程发生故障，则故障检测器必须检测到该故障。**安全性(safety)保证意外事件不会发生**，例如，如果一个故障检测器已经将一个进程标记为死亡，那么这个进程实际上必须是死亡的[LAMPORT77， RAYNAL99， FREILING11]。

​	从实际的角度来看，排除发生故障的进程有助于避免不必要的工作，防止错误传播和级联故障，而排除疑似故障的活动进程会降低系统可用性。

​	故障检测算法应该表现出几个基本特性。首先，每一个没有问题的成员最终都应该注意到进程故障，并且算法应该能够向前推进并最终得出结果。这种特性称为**完备性（completeness）**。

​	我们可以通过算法的效率来判断其优劣：故障检测器识别进程故障的速度有多快。另一种方法是观察算法的准确性：是否精确地检测到了进程故障。换句话说，如果一个算法错误地认为一个活着的进程发生了故障或者不能检测出实际已发生的故障，那么它就是不准确的。

​	我们可以把效率和准确度之间的关系看作是一个可调参数：一个更高效的算法可能更不准确，而一个更准确的算法通常更不髙效。**建立既准确又高效的故障检测器被证明是不可能的**。同时，<u>故障检测器是允许产生假阳性的(即，错误地将活着的进程识别为故障，或者反过来)</u>[CHANDRA96]。

​	故障检测器是许多共识和原子广播算法的必要前提和组成部分，我们将在本书后面讨论这些算法。

​	许多分布式系统使用**心跳(heartbeat)**来实现故障检测器。由于其简单性和很强的完备性，这种方法非常普遍。我们在这里讨论的算法<u>假设不存在拜占庭式故障：进程不会试图故意谎报它们自己及相邻进程的状态</u>。

## 9.1 心跳和ping

​	我们可以通过触发如下两个周期性过程之一来查询远程进程的状态：

+ 我们可以触发一个**ping**，它将消息发送到远程进程，通过在指定的时间段内是否得到响应来检查它们是否仍处于活动状态。
+ 我们可以触发一个**心跳**，即进程通过向其对等方发送消息来主动通知其仍在运行。

​	在这里我们将使用ping作为例子，使用心跳也可以解决相同的问题并产生相似的结果。

​	每个进程维护一个其他进程的列表(存活、死亡和疑似死亡)，并且用每个进程最新的响应时间对这个列表进行更新。如果一个进程在较长的时间内无法响应一个ping消息，它会被标记为疑似死亡(suspected)。

​	许多故障检测算法都是基于心跳和超时的。例如，用于构建分布式系统的流行框架Akka实现了一个截止时间故障检测器(deadline failure detector)，这一检测器使用心跳机制，如果进程在某一固定时间间隔内未能成功注册，它将报告进程故障。

​	<u>这种方法有几个潜在的缺点：它的精度依赖于对ping频率和超时的仔细挑选，并且它不能从其他进程的角度捕获进程的可见性(参见9.1.2节)</u>。

### * 9.1.1 无超时的故障检测器

​	一些算法避免依赖超时来检测故障。例如Heartbeat，一种**无超时(timeout-free)**故障检测器[AGUILERA97]，该算法仅对心跳计数并允许应用程序基于心跳计数器向量中的数据来检测进程故障。由于该算法是无超时的，因此它能在异步系统假设下运行。

​	该算法假设任意两个正确的进程用公平路径(fair path)相互连接，该路径只包含公平链路(即如果无限频繁地通过该链路发送一条消息，该消息也会无限频繁地被接收)，并且每个进程都能意识到网络中所有其他进程的存在。

​	每个进程维护一个邻居列表和与其相关联的计数器。首先，进程向邻居发送心跳消息，毎个消息都包含心跳到目前为止所经过的路径。初始消息包含路径中的第一个发件人和一个唯一标识符，该标识符可用于避免同一消息被广播多次。

​	当进程接收到新的心跳消息时，它会递增路径中所有参与者的计数器，将心跳信号发送到尚未参与的进程，并将其自身追加到路径中。进程一旦看到所有已知进程已经接收到消息(换句话说，进程ID出现在路径中)，就会停止传播该消息。

​	<u>由于消息是通过不同的进程传播的，并且心跳路径包含从相邻进程接收的聚合信息，因此即使两个进程之间的直接链路出现故障，我们也可以(正确地)将无法到达的进程标记为活动进程</u>。

​	心跳计数器表示系统的全局和归一化视图。这个视图捕获了心跳是如何在节点间传播的，让我们可以对进程进行比较。然而，<u>这种方法的一个缺点是，对心跳计数器进行解释可能相当棘手：**需要选择一个能够产生可靠结果的阈值**。除非我们能做到这一点，否则算法会错误地将活动进程标记为疑似死亡</u>。

### * 9.1.2 外包心跳

​	可扩展弱一致性感染式进程组成员协议(Scalable Weakly Consistent Infection-style Process Group Membership Protocol，SWIM)[GUPTA01使用的是另一种方法。它使用外包心跳(outsourced heartbeat)来提高可靠性，**<u>利用的是从其相邻进程的角度查看到的进程活动性(liveness)信息。这种方法不需要进程知道网络中的所有其他进程，只需要知道其连接的对等进程的子集</u>**。

​	进程P1向进程P2发送ping消息。P2不响应该消息，因此P1继续选择多个随机成员(P3和P4)发送ping消息。这些随机成员会尝试向P2发送心跳消息，如果P2响应，则将确认转发回P1。

​	这允许我们将直接和间接可达性都考虑在内。例如，如果有进程P1、P2和P3，我们可以同时从P1和P2的角度检查P3的状态。

​	通过将决策责任分布到成员组中，外包心跳可以做到可靠的故障检测。这种方法不需要向很多对等进程广播消息。由于外包心跳请求可以并行触发，这种方法可以快速收集更多关于疑似死亡进程的信息，进而让我们做出更准确的决策。

## * 9.2 phi增量故障检测器

> 某种意义上有点类似Google TCP的BBR拥塞控制算法。
>
> [谈TCP BBR拥塞控制算法_yang_oh的博客-CSDN博客_bbr拥塞控制算法](https://blog.csdn.net/u013032097/article/details/96212770)

​	phi增量(φ-accrual)故障检测器[HAYASHIBARA04]不是将节点故障视为二元判断问题(即进程只能处于两种状态：在线或宕机)，而是<u>用连续范围来捕获被监视进程崩溃的概率</u>。它的工作方式是**维护一个滑动窗口，从对等进程收集最近心跳的到达时间**。该信息用于估算下一个心跳的到达时间，将该近似值与实际到达时间进行比较，并计算可疑程度φ：代表在给定当前网络条件下，故障检测器对故障的置信度。

​	该算法的原理是：<u>收集和采样到达时间，创建出一个可用于对节点健康状况做出可靠判断的视图，然后使用这些釆样结果计算φ的值：如果该值达到阈值，则节点被标记为宕机。**通过调整标记节点为疑似死亡的阈值，这种故障检测器能够动态地适应变化的网络条件**</u>。

​	从架构的角度来看，phi增量故障检测器可以看作三个子系统的组合。

+ **监控**

  通过ping、心跳或请求-响应采样来收集进程存活信息。

+ **解释**

  决定是否将该进程标记为疑似死亡。

+ **行动**

  每当标记进程为疑似死亡时执行的回调。

​	监控进程将数据样本(假定是正态分布)收集并储存在心跳到达时间的固定大小窗口中。新到达的心跳被添加到窗口中，同时最早的心跳数据点被丢弃。

​	<u>通过确定样本的均值和方差，可以从采样窗口估算出分布参数。该信息用于计算在前个消息到达之后t个时间单位内消息到达的概率。基于这个信息我们能计算出φ，它描述了我们对一个进程活动性做出正确决定的可能性。换句话说，有多大的可能性犯错——接收到一个与计算出的假设相矛盾的心跳</u>。

​	这种方法是由日本高级科学技术研究所的研究人员开发的，现在已用于许多分布式系统中，例如，Cassandra和Akka(连同前面提到的截止时间故障检测器)。

## * 9.3 Gossip和故障检测

> 有点像OSPF动态路由协议。
>
> [动态路由协议_百度百科 (baidu.com)](https://baike.baidu.com/item/动态路由协议/2915884?fr=aladdin)

​	另一种避免依赖单节点视图做出决策的方法是Gossip式的故障检测服务[VANRENESSE98]它使用Gossip(参见12.6节)来收集和分发相邻进程的状态。

​	<u>每个成员维护一个其他成员的列表：它们的心跳计数器(heartbeat counter)和时间戳</u>。

​	<u>时间戳列出了心跳计数器上次递增的时间。每个成员定期递增其心跳计数器，并将其列表分发给随机相邻节点。在接收到消息时，相邻节点将列表与它自己的列表进行合并，更新其他相邻节点的心跳计数器</u>。

​	节点还定期检查状态列表和心跳计数器。如果任何节点在足够长的时间里没有更新其计数器，就认为它发生了故障。超时时间应当谨慎选择，以将误报的概率降至最低。<u>成员之间通信的频率(换句话说，最坏情况下所使用的带宽)是有上限的，并且最多可以随着系统中的进程数线性增长</u>。

​	通过这样的方式，我们就可以检测出崩溃的以及任何其他集群成员都无法访问的节点。这个决策是可靠的，因为集群的视图是来自多个节点的聚合。如果两台主机之间的链路出现故障，心跳仍然可以通过其他进程传播。**使用Gossip来传播系统状态增加了系统中的消息数量，但使得信息传播更可靠**。

## * 9.4 反向故障检测

​	由于并不总是能传播故障的信息，并且通过通知每个成员来进行传播可能成本较高，因此出现了一种称为FUSE(Failure Notification Service，故障通知服务)[DUNAGANO4]的方法，它专注于可靠且廉价的故障传播，即使在网络分区的情况下也能工作。

​	<u>为了检测进程故障，该方法将所有活动进程进行分组。**如果其中一组变得不可用，则所有参与者都能检测到该故障**。换句话说，每次检测到单个进程故障时，它被转换并传播为**组故障**</u>。它可以检测任何形式的网络中断、网络分区和节点故障。

​	<u>组中的进程定期向其他成员发送ping消息，以查询它们是否仍处于活动状态。**如果其中一个成员由于崩溃、网络分区或链路故障而无法响应此消息，则发出这个ping的成员本身将停止响应ping消息**</u>。

​	展示了四个通信进程：

+ 初始状态：所有进程都处于活动状态并可以通信。
+ P2崩溃并停止响应ping消息。
+ P4检测到P2的故障并停止响应自己收到的ping消息。
+ 最终，P1和P3注意到P4和P2都没有响应，将进程故障传播到整个组。

​	**所有故障都通过系统从故障源传播到所有其他参与者**。<u>参与者逐渐停止响应ping消息，将单个节点故障转换为组故障</u>。

​	在这里，我们**<u>利用不通信作为一种传播的手段</u>**。这种方法的一个优点是保证每个成员都能了解组的故障并对其做出充分的反应。它的一个缺点是：<u>将单个进程与其他进程分开的链路故障也可能会被转换为组故障，但这其实也可以被看作是一个优点，应由具体的用例所决定</u>。应用程序可以使用其自身对故障传播的定义来应对这种情况。

## 9.5 本章小结

​	故障检测器在任何分布式系统中都是一个重要的组成部分。正如FLP不可能定理所说的，**<u>异步系统中没有协议能够保证一致性</u>**。

​	<u>故障检测器有助于扩展模型，允许我们通过在**准确性**和**完备性**之间进行权衡来解决一致性问题</u>。文献[CHANDRA96]描述了这一领域的一个重要发现，证明了故障检测器是有用的，它表明，<u>**即使使用一个犯了无限多个错误的故障检测器，解决共识问题仍然是可能的**</u>。

​	我们介绍了几种故障检测算法，每种算法都使用不同的方法：一些专注于通过直接通信来检测故障，而一些则使用广播或Gossip来传播信息，还有一些使用沉默(换句话说，不再通信)作为传播手段。现在，我们知道可以使用心跳、ping、截止时间、连续范围等方法，毎一种方法都有自己的优点:简单性、准确性或精确性。

# * 10. 领导者选举

​	同步的代价可能会非常大：如果算法的每一个步骤都需要联系其他参与者，那么结果定是产生相当显著的通信开销。在大型且地理分布的网络中尤其如此。<u>为了减少同步开销和达成决定所需消息的往返次数，一些算法依赖于**领导者(有时称为协调者)进程**的存在，该进程负责执行或协调分布式算法的各个步骤</u>。

​	一般来说，分布式系统中的进程是对等的，任何进程都可以接管领导者的角色。进程可以长期担任领导者，但这不是一个永久的角色。通常情况下，进程可以一直担任领导者直到崩溃为止。崩溃后，任何其他进程都可以开始新一轮的选举，如果其当选，就可以担任领导者并继续执行上个领导者遗留的工作。

​	选举算法的**活动性(liveness)**保证了大多数时候会有一个领导者，选举最终会完成(即<u>系统不应该无限期地处于选举状态</u>)。

​	理想情况下，我们也希望获得安全性，保证一次最多只能有一个领导者，并完全消除<u>脑裂(两个目的相同的领导者被选举出来，但彼此不知情)</u>的可能性。然而在实践中，许多领导者选举算法违反了这一协定。

​	可以使用领导者进程来实现广播中消息的全序。领导者收集并保存全局状态，接收消息，并将消息分发给各个进程。它还可以用于协调发生在系统故障后、初始化期间或重要状态变更时的系统重组。

​	系统初始化时将会触发选举，第一次选择领导者。当上一个领导者崩溃或通信失败时也会触发选举。选举必须是确定性的：<u>选举过程中必须只产生一个领导者。这一决定需要对所有参与者都有效</u>。

​	<u>尽管领导者选举和分布式锁(即对共享资源的独占所有权)从理论角度看可能很相似，但它们略有不同。如果一个进程因为执行**临界区**而持有锁，那么对于其他进程来说，知道现在到底是谁持有锁并不重要，只要满足活动性(即锁最终将被释放并允许其他人获得它)就可以了。**相比之下，选出的进程具有一些特殊性质，必须让所有其他参与者都知道，因此新当选的领导者必须将其角色通知所有对等进程**</u>。

​	<u>如果分布式锁算法对某个进程或进程组有任何偏好，不被偏好的进程最终将产生对共享资源的饥饿，这与活动性相矛盾。相比之下，领导者可以保持领导的角色直到停止或崩溃，长期存活的领导者是更好的</u>。

​	**在系统中具有稳定的领导者有助于避免远程参与者之间的状态同步，减少交换消息的数量，并能通过单进程(而不是对等进程间的协调)来驱动执行**。

​	在具有领导权的系统中，一个潜在的问题是，**领导者可能会成为瓶颈**。<u>为了克服这种情况，许多系统将数据划分在不相交的独立副本集中(参见13.6节)。每个副本集都有自己的领导者，而不是在整个系统范围上使用一个领导者</u>。使用这种方法的系统之一是Spanner(参见13.5节)。

​	因为毎一个领导者进程最终都会发生故障，所以故障必须被检测、报告和处理：系统必须选择另一个领导者来替换故障领导者。

​	一些算法，例如ZAB(参见14.2.2节)、 Multi-Paxos(参见14.34节)或Raft(参见14.4节)，<u>使用**临时领导者**来减少参与者达成一致所需的消息数量</u>。然而，这些算法都使用算法特有的手段来进行领导者选举、故障检测以及解决竞争领导者的进程之间的冲突。

## 10.1 霸道选举算法

​	有一个被称为霸道选举算法(bully algorithm)的领导者选举算法，<u>它使用进程排名来认定新的领导者。每个进程都有一个唯一的排名。在选举过程中，排名最高的进程成为领导者[MOLINA82]</u>。

​	这种算法以其简单性而闻名。之所以被命名为霸道(bully)，是因为排名最高的节点"霸道"地强迫其他节点接受它。它有时也被称为君主领导者选举：在前一个君主不复存在后，排名最高的兄弟姐妹成为君主。

​	如果一个进程注意到系统中没有领导者(该系统从未被初始化)或以前的领导者已停止响应，则按如下三个步骤进行选举：

1. 进程将选举消息发送到具有较高标识符的进程。
2. 进程等待较高排名的进程进行响应。如果没有排名更高的进程响应，则继续执行步骤3。否则，进程通知它所了解到的排名最高的进程，让它继续执行步骤3。
3. 进程假定没有排名更高的活动进程了，因此将新的领导者通知给所有排名更低的进程。

​	霸道领导者选举算法：

+ a）进程3注意到前一个领导者6已崩溃，于是向具有更高标识符的进程发送选举消息来开始新的选举。
+ b）4和5回应Alive(存活)消息，因为它们具有比3更高的排名。
+ c）3通知在此轮响应中排名最高的进程5。
+ d）5被选为新的领导者，它广播Elected(选举完成)消息，将选举结果通知排名较低的进程。

​	这种算法的一个明显问题是，在存在网络分区的情况下，它违反了安全性保证(一次最多只能选举一个领导者)。很容易出现这样的情况：<u>节点将被分成两个或多个独立运行的子集，每个子集选举出了这个子集的领导者。这种情况被称为**脑裂(split brain)**</u>。

​	该算法的另一个问题是对排名较高节点有强烈的偏向性，<u>如果这些节点不稳定，可能导致算法一直处于重新选举状态</u>。一个不稳定的高排名节点提议出任领导者，此后不久发生故障，稍后又再次赢得选举，然后再次发生故障，整个过程重复进行。可以通过分发主机质量的度量并在选举时将其纳入考虑因素来解决这一问题。

## 10.2 依次故障转移

​	霸道算法有许多改进其各种性质的版本。例如，我们可以使用多个依次(next-in-line)替代进程作为故障转移候选来缩短重选过程[GHOLIPOUR09]。

​	<u>每个选举出的领导者都提供一个故障转移节点的列表。当其中一个进程检测到领导者故障时，它通过向列表中排名最高的备选进程发送消息来发起新一轮选举。如果提议的备选进程中有一个是活动的，它就会成为一个新的领导者，而不必经过整个选举回合</u>。

​	如果检测到领导者故障的进程本身是列表中排名最高的进程，则它可以立即通知其他进程新领导者的信息。

优化过的流程：

+ a）6是具有指定备选进程{5，4}的领导者，它崩溃了。3注意到此故障并联系5，它是列表中排名最高的备选进程。
+ b）5响应3，它处于活动状态，以防止它联系备选进程列表中的其他节点
+ c）5通知其他节点它是新的领导者。

​	因此，如果下一个进程仍然活着，我们在选举期间需要的步骤就更少。

## 10.3 候选节点/普通节点优化

​	另一种算法试图通过将节点分成两个子集，即**候选节点(candidate)**和**普通节点(ordinary)**，来降低所需的消息数量，只有候选节点的其中之一最终可以成为领导者[MURSHED12]。

​	普通进程通过联系候选节点来发起选举，收集它们的响应，选择排名最高的活动候选节点作为新的领导者，然后通知其余节点选举结果。

​	**为了解决同时发生多个选举的问题，该算法建议使用一个特定于进程的延迟变量δ(各进程的δ差异很大)，使得其中一个节点可以在其他节点之前发起选举**。δ通常大于消息的往返时间。高优先级的节点具有较低的δ，反之则具有较高的δ。

​	选举过程的步骤（假设1、2、6都是候选节点；3、4、5都是普通节点）：

+ a）普通进程集合里的进程4注意到领导者进程6的故障。它通过联系候选节点集合里的所有剩余进程来发起新的选举回合。
+ b）候选进程发送响应通知4它们仍然活动。
+ c）4将新的领导者2通知给所有进程。

## * 10.4 邀请算法

​	邀请算法(invitation algorithm)允许进程"邀请"其他进程加入它们的组，而不是试图超越它们的排名。这种算法从定义上就**允许多个领导者存在**，因为每个组都有自己的领导者。

​	<u>每个进程一开始都是一个新组的领导者，组内唯一的成员是这个进程本身。组领导者联系不属于该组的对等进程，邀请其加入。如果对等进程本身是领导者，则合并两个组。否则，被联系的进程会回复组领导者ID，从而让两个组的领导者以较少的步骤建立联系并合并两个组</u>。

​	邀请算法的执行步骤：

+ a）开始时，有四个进程成为各有一名成员的组的领导者。1邀请2加入它所在组，3邀请4加入它所在组。
+ b）2加入进程1的组，4加入进程3的组。1作为第一组的领导者去联系另一组的领导者3。随后，该组剩余的成员(在本例中为4)被通知关于新的组领导的信息。
+ c）两个组被合并，并且1成为扩展后组的领导者。

​	因为组被合并了，是建议组合并的进程还是另一个进程成为新的领导者都无关紧要。为了将合并组所需的消息数量保持在最低限度，较大组的领导者可以成为新组的领导者。这样，只需要把领导者变更的消息通知给较小组的进程。

​	与其他讨论过的算法类似，该算法允许各个进程处于多个组中，并允许多个领导者存在。邀请算法允许创建进程组并合并它们，而不必从头开始触发新的选举，这减少了完成选举所需的消息数量。

## 10.5 环算法

​	在环算法(ring algorithm)[CHANG79]中，系统中所有的节点形成一个环，并且知道环拓扑(即它们在环中的前驱和后继)。当进程检测到领导者故障时，它发起新的选举。选举消息沿着环向下转发：每个进程联系它的后继节点(环中离它最近的下一个节点)。如果该节点不可用，则进程跳过不可达的节点，并尝试联系环中之后的节点，直到最终有一个节点响应为止。

​	节点联系其兄弟节点，沿着环收集活动节点的集合，并在将集合传递到下一个节点之前将其自身添加到该集合中，类似于9.1.1节中描述的故障检测算法那样，节点在将其传递到下一个节点之前将其标识符附加到路径中。

​	该算法通过完全遍历环来进行。当消息返回到开始选举的节点时，从活动节点集中选择排名最高的节点作为领导者。一个遍历的例子（假设1-2-3-4-5-6～1成环，6故障后，5和1成相邻节点）：

+ a）先前的领导者6发生了故障，并且每个进程都具有该环的视图。
+ b）3通过开始遍历来发起选举。每一步中，都维护一个在路径上遍历过的节点集合。5无法达到6，所以它跳过6直接联系1。
+ c）由于5是排名最高的节点，3会发起另一轮消息传递来广播关于新领导者的信息。

​	**这种算法的变体包括收集单个排名最高的标识符，而不是一组活动节点，以节省空间**：因为max函数是可交换的，所以知道当前遍历过的节点的排名最大值就足够了。当算法返回到已经开始选举的节点时，最后已知的最高标识符会再次在环上循环传递。

​	**由于环可以划分为两个或更多的部分，因此可能出现脑裂这种不安全的情况**。

​	正如你所看到的，一个具有领导者的系统要正确行使职能，我们就需要知道目前领导者的状况(它是否还活着)，因为，要想将进程组织起来继续执行算法，领导者必须是活动且可达的。要检测领导者崩溃，我们可以使用故障检测算法(参见第9章)。

## 10.6 本章小结

​	领导者选举是分布式系统中的一个重要课题，使用特定领导者可以减少协调开销并提高算法的性能。每轮选举的成本可能很高，但由于它们不经常发生，因此不会对整个系统的性能产生负面影响。<u>单个领导者可能会成为瓶颈，但大多数时候可以通过对数据进行分区来解决(对不同分区或不同的动作使用不同的领导者)</u>。

​	<u>不幸的是，我们在本章中讨论的所有算法都容易出现**脑裂**的问题</u>：**我们可能会在独立的子网中选出两个领导者，它们彼此都不知道对方的存在**。<u>为了避免脑裂，我们必须获得整个集群范围内的多数票</u>。

​	<u>许多共识算法，包括Multi-paxos和Raft，都依赖于一个领导者来进行协调。但领导者选举不就是共识本身吗?要选举一个领导者，我们需要就其身份达成共识。如果我们能就领导者的身份达成共识，我们就能用同样的方法在其他任何事情上达成共识[ABRAHAM13]</u>。

​	一个领导者的身份可能会在其他进程不知晓的情况下发生变化，因此进程本地关于领导者的知识是否仍然有效是个问题。为了解决这个问题，<u>我们需要将领导者选举与故障检测相结合</u>。例如，稳定领导者选举(stable leader election)算法使用具有唯一稳定领导者的回合和基于超时的故障检测，以保证领导者只要不崩溃且可访问，就能维持其地位[AGUILERA01]。

​	**依赖于领导者选举的算法通常允许存在多个领导者，并试图尽可能快地解决领导者之间的冲突**。例如，对于 Multi-Paxos(参见14.3.4节)就是这样的，其中只有两个冲突的领导者(提议者)中的一个可以继续执行，这些冲突通过收集第二个Quorum的投票来解决，保证来自两个不同提议者的值不会被同时接受。

​	在Raft(参见14.4节)中，领导者可以发现其任期过时(这意味着系统中存在不同的领导者)，并将其任期更新为最新的任期。

​	在这两种情况下，拥有一个领导者是确保活动性的一种方式(如果当前的领导者发生故障，我们需要一个新的领导者)，而且其他进程不应该花费无限长的时间来了解领导者是否真的发生了故障。安全性的缺失和允许多个领导者是一种性能优化：算法可以继续进行复制阶段，而安全性则通过检测和解决冲突来保证。

​	我们将会在第14章里更深入地讨论共识和共识范畴内的领导者选举。

# 11. 复制和一致性

​	在继续讨论共识和原子提交算法之前，让我们学习最后一块缺失的部分：一致性模型(consistency model)。一致性模型非常重要，因为它们解释了多数据副本系统的可见性语义和行为。

​	容错(fault tolerance)描述了这样一种特性：当系统中的部分组件发生故障时，系统仍然能继续正确地运行。使系统具有容错性并不是一件容易的事情，而使现有的系统具备容错能力则更加困难。<u>容错的主要目标是从系统中消除单点故障，并确保关键任务组件有冗余</u>。通常，冗余对用户来说是完全透明的。

​	系统可以存储多个数据副本，当其中一台机器发生故障时，通过另一台机器可以进行**故障转移(failover)**，这样系统就可以继续正确运行。<u>在具有单一真相来源(source of truth)的系统中(例如，主/副本数据库)，可以通过将副本晋升为新的主数据库来显式地完成故障转移</u>。有些系统则不需要显式的重新配置，它们通过在读写查询期间收集多个参与者的响应来确保一致性。

​	**数据复制(replication)**通过在系统中维护多个数据副本来引入冗余。然而，由于原子地更新数据的多个副本是一个等同于共识的问题[MILOSEVIC11]，因此<u>**数据库中的每个操作都执行共识操作可能成本相当高**</u>。我们可以探索一些性价比更高且更灵活的方法，允许参与者之间存在某种程度的差异，但使数据从用户的角度看起来是一致的。

​	在多数据中心部署中，复制尤为重要。在这种情况下，**跨地域复制(geo-replication)**有多种用途：它通过提供冗余来提高可用性，增强抵御一个或多个数据中心的故障的能力。它还<u>可以将数据副本放置在离客户端更近的物理位置以减少延迟</u>。

​	当数据记录被修改时，其副本必须被相应地更新。在谈论复制时，我们最关心这三种事件： **写入、副本更新和读取**。这些操作触发了由客户端发起的一系列事件。在某些情况下，从客户端角度看，更新副本可能发生在写操作完成之后，但这仍然不能改变这样个事实：<u>客户端必须能够以特定的顺序观察到发生过的操作</u>。

## 11.1 实现可用性

​	我们已经讨论了分布式系统的误区，并确定了许多可能出错的事情。在现实世界中，节点不一定处于活动状态或能够相互通信。然而，**间歇性故障不应影响可用性**：从用户的角度来看，系统作为一个整体会继续运行，就像什么都没有发生一样。

​	系统可用性是一个非常重要的属性：在软件工程中，我们总是努力实现高可用性，并尽量减少停机时间。工程团队夸耀他们软件的正常运行时间指标。我们之所以那么关心可用性是出于如下几个原因：软件已经成为我们社会不可分割的一部分；没有它，许多重要的事情都难以进行，例如银行业务、通信、旅行等。

​	对于公司来说，缺乏可用性可能意味着失去客户或金钱：如果电商系统出现故障，你就无法在那里购物；如果你的银行网站没有响应，你就无法转账。

​	为了使系统具有高可用性，我们需要以这样一种方式设计我们的系统——允许它优雅地处理一个或多个参与者出现故障或不可用的情况。为此，我们需要引入**冗余**和**复制**。然而，<u>一旦我们添加冗余，就会面临**多数据副本同步**的问题，并且必须实现**恢复机制**</u>。

## 11.2 臭名昭著的CAP理论

​	**可用性**是一个衡量系统成功响应请求能力的属性。可用性的理论定义提到了最终响应，但是，在现实世界的系统中，我们当然希望避免等待无限长的时间。

​	理想情况下，我们希望毎个操作都是一致的。**一致性在这里定义为原子性或可线性化linearizable)的一致性**(参见11.5.2节)。<u>可线性化历史能够表示为一个可以保持原始操作顺序的瞬时操作序列</u>。可线性化简化了对可能的系统状态的推理，使分布式系统看起来就像是在单机系统上运行一样。

​	我们希望在容忍网络分区的同时实现一致性和可用性。网络可能被分裂为几个部分，在这些部分之间，进程不能相互通信：被分隔的节点之间发送的一些消息将无法到达目的地。

​	<u>**可用性**要求任何无故障的节点交付结果，而**一致性**要求结果是可线性化的</u>。 Eric Brewer提出的CAP猜想讨论了一致性(consistency)、可用性(availablity)和分区容忍性(partition tolerance)之间的权衡[BREWER00]。

​	<u>在一个异步系统中，可用性要求是不可能被满足的，而在网络分区存在的情况下，我们无法实现一个同时保证可用性和一致性的系统[GILBERT02]</u>。构建系统时，我们可以在提供尽力而为(best effort)可用性的同时保证强一致性，或者在提供尽力而为一致性的同时保证可用性[GILBERT12]。在这里，尽力而为意味着：如果一切正常，系统将不会故意违反任何保证，但是在网络分区的情况下，允许系统削弱和违反保证。

​	换句话说，CAP描述了一系列潜在的可选项，而在这些选项的两端是以下两种系统：

+ **一致性和分区容忍系统(CP系统)**

  CP系统更倾向拒绝请求，而不是提供可能不一致的数据。

+ **可用性和分区容忍系统(AP系统)**

  AP系统放松了一致性要求，允许在请求期间提供可能不一致的值。

​	CP系统的一个例子是共识算法的实现，它要求多数派节点的参与才能进行：系统总是一致的，但在网络分区的情况下可能不可用。

​	而AP系统的一个例子是，只要有一个副本活着，数据库就一定能进行读写，这最终可能导致数据丢失或返回不一致的结果。

​	PACELC猜想[ABADI12]是CAP的一个扩展，它指出在网络分区(P)存在的情况下可用性和一致性(AC)之间存在一个选择。否则(Else，E)，即使在没有网络分区的情况下系统运行正常，我们仍然要在延迟(Latency，L)和一致性(C)之间做出选择。

### 11.2.1 小心使用CAP

​	需要注意的是，<u>CAP讨论的是网络分区，而不是节点崩溃或任何其他类型的故障(如崩溃恢复)。与集群其他节点分隔开的节点可以做出不一致的响应，但崩溃的节点根本不会响应</u>。一方面，这意味着没有必要考虑任何宕掉的节点需要面对一致性问题。而另一方面，现实世界中的情况并非如此：还有许多不同的故障场景(其中一些可以用网络分区来模拟)。

​	CAP意味着，即使所有节点都启动了，**<u>只要它们之间有连接性问题，我们仍可能面临一致性问题</u>**。这是因为<u>我们期望毎个没有故障的节点都能正确响应，而不考虑有多少节点可能宕掉</u>。

​	CAP猜想有时用一个三角形来表示，就好像我们可以转动一个旋钮，或多或少地得到所有这三个参数中的两个。然而，尽管我们可以转动旋钮用一致性换取可用性，但**分区容忍性是一个实际上我们无法调节或用任何东西来交换的属性**[HALE10]。

> CAP中的一致性定义与ACID(参见第5章)定义的一致性完全不同。ACID致性描述了事务一致性：事务将数据库从一个有效状态带到另一个有效状态，保持所有数据库的约束(如唯一性约束和引用完整性)。而在CAP中，它意味着操作是**原子的**(操作全部成功或失败)和**一致的**(操作从不让数据处于不一致的状态)。

​	CAP中的可用性也不同于前面提到的高可用性[KLEPPMANN15]。CAP的定义对执行延迟没有限制。另外，与CAP相反，数据库中的可用性并不要求每个非故障节点响应每个请求。

​	CAP猜想用于解释分布式系统、推理故障场景和评估可能的情况，放弃一致性并不意味着系统可以提供不可预测的结果，它们之间有所区别。

​	如果使用正确，声称具备可用性的数据库仍然能够提供来自副本的一致结果，前提是要存在足够多的活着的副本。当然，还有更复杂的故障场景，**CAP猜想只是一条经验法则，并不一定能说明全部事实**。

### 11.2.2 收成与产量

​	CAP猜想仅以它们最强的形式讨论一致性和可用性：可线性化和系统最终响应每一个请求的能力。

​	这迫使我们在这两个属性之间做出艰难的权衡。然而，有些应用程序可以从稍微放松的假设中获益，我们可以用它们较弱的形式来思考这些属性。

​	系统不一定非得在一致或可用中二选一，也可以提供更宽松的保证。我们可以定义两个可调度量：收成(harvest)和产量(yield)，在两者之间进行选择仍然可以形成正确的行为[FOX99]：

+ 收成

  收成定义査询的完成程度：如果查询必须返回100行，但由于某些节点不可用而只能获取99行，这仍然比查询完全失败而不返回任何内容要好。

+ 产量

  产量指成功完成的请求数与尝试请求总数之比。产量与正常运行时间(uptime)不同，例如一个繁忙的节点没有宕机，但仍然可能无法响应某些请求。

​	这就把权衡的重点从绝对条件变成了相对条件。我们可以用收成换取产量，并允许某些请求返回不完整的数据。<u>提高产量的一个方法是只从可用的分区返回查询结果</u>(参见13.6节)。例如，如果存储某些用户记录的节点子集宕机，我们仍然可以继续为其他用户处理请求。或者，我们可以要求，对关键应用程序的数据必须完整返回，但对其他请求允许出现一些偏离。

​	定义和权衡收成与产量，并在二者之间做出慎重的决定，有助于我们建立容错性更好的系统。

## 11.3 共享内存

​	对于客户端来说，分布式系统在储存数据时仿佛拥有共享的存储，类似于单节点的系统。节点间通信和消息传递则被抽象出来并发生在幕后。这就造成了一种共享内存的假象。

​	可通过读写操作来访问的单个存储单元通常被称为寄存器(register)。我们可以将分布式数据库中的共享内存视为一个寄存器阵列。

​	我们用调用((invocation)和完成(completion)事件来标识每个操作。如果调用该操作的进程在操作完成之前崩溃，则将该操作定义为失败操作。<u>如果一个操作的调用和完成事件都发生在另一个操作被调用之前，我们说这个操作在另一个操作之前，并且这两个操作是顺序的(sequential)。否则说它们是并发的</u>。

​	进程P1和P2执行不同的操作：

+ a)进程P2执行的操作在进程P1执行的操作已经完成之后才开始，两个操作是顺序
+ b)两个操作之间有重叠，因此这两个操作是并发的。
+ c)进程P2执行的操作在P1执行的操作之后开始，并在P1执行的操作完成之前结束，这两个操作也是并发的。

​	多个读取者或写入者可以同时访问寄存器。对寄存器的读写操作不是即时的，即需要些时间。由不同进程执行的并发读写操作不是串行的(serial)：根据操作重叠时寄存器的行为，它们的顺序可能不同，并可能产生不同的结果。根据寄存器在并发操作下的行为，我们将寄存器分为以下三类：

+ 安全寄存器

  在并发写操作期间，对安全寄存器的读取可能返回寄存器范围内的任意值(这听起来不太实用，但可能描述出了异步系统的语义——不限定执行的顺序)。**具有二元值的安全寄存器在读写过程中可能会出现闪烁**(flickering，即结果交替返回两个值)。

+ 常规寄存器

  常规寄存器有稍强一点的保证：<u>读操作只能返回最近完成的写操作所写的值，或者与当前读操作重叠的写操作所写的值</u>。在这种情况下，**系统有一些顺序的概念，但是写结果并不同时对所有读取者可见**(例如，在复制数据库中可能会发生这样的情况：主进程接受写操作，并将其复制给提供读服务的工作者进程)。

+ 原子寄存器

  **原子寄存器保证可线性化：每个写操作都对应一个时刻，在此之前，每个读操作返回一个旧值，在此之后，每个读操作返回一个新值。原子性是简化系统状态推断的基本性质**。

## * 11.4 顺序

​	当我们看到一系列事件时，我们对它们的执行顺序会有直观感受。然而，在分布式系统中，理解顺序并不总是那么容易，因为很难准确地知道什么时候发生了什么事情，并且也很难在整个集群中立即获得这些信息。每个参与者可能都有自己的状态视图，因此我们必须査看毎个操作，并根据其调用和完成事件定义顺序，并描述其操作边界。

​	让我们这样定义一个系统，在其中的进程可以对共享寄存器执行read(register)和write(register， value)操作，每个进程按顺序执行自己的一组操作(即，必须完成每个操作之后才能启动下一个操作)。顺序进程执行的组合形成了一个全局历史，这其中，操作可能并发执行。

​	**考虑一致性模型的最简单方法是讨论读和写操作及其重叠的方式：读操作没有副作用，而写操作会改变寄存器状态**。这能帮助我们推断写入后数据何时变得可读。例如，考虑两个进程并发执行如下事件的历史记录：

```pseudocode
进程1:				进程2:
write(x,1)		read(x)
							read(x)
```

​	单看这些事件无法确定这两种情况下read(x)运算的结果是什么。有几种可能的历史：

+ 写操作在两次读操作之前完成。
+ 写操作和两次读操作交错进行，而且可能在两次读操作之间执行。
+ 两次读操作都在写操作之前完成。

​	即使我们只有一份数据，也并不能很容易地回答会发生什么。在一个复制系统中，我们会有更多可能的状态组合，当多个进程读写数据时情况将变得更加复杂。

​	如果所有这些操作都是由单个进程执行的，我们可以强制一个严格的事件顺序，但是对于多个进程则很难这样做。潜在的困难可以分为两组：

+ 操作可能会重叠。
+ 不重叠调用的影响可能不会立即显现。

​	**为了推理操作顺序并明确描述可能的结果，必须定义一致性模型**。我们用共享内存和并发系统来讨论分布式系统中的并发性，因为大多数一致性定义和规则仍然是适用的。尽管并发系统和分布式系统之间存在着许多重叠的术语，但由于通信方式、性能和可靠性等方面的差异，我们无法直接应用大多数并发算法。

## 11.5 一致性模型

​	由于共享内存寄存器上的操作允许重叠，我们需要定义清楚语义：如果多个客户端同时(或在短时间内)读取或修改数据的不同副本将会发生什么。这个问题没有唯一正确的答案，因为这些语义根据应用程序的不同而不同，但是在一致性模型的上下文中它们都得到了很好的研究。

​	一致性模型(consistency model)提供不同的语义和保证。你可以将一致性模型看作是参与者之间的契约：每个副本要做什么才能满足所需的语义，以及用户在发出读写操作时可以期望得到什么结果。

​	一致性模型描述了在存在多份数据和并发访问的情况下可能岀现的返回结果。在本节，我们将讨论单一操作(single-operation)一致性模型。

​	毎个模型都描述了系统行为与我们期望的(或感觉自然的)行为之间存在的差距。它帮助我们区分交错操作的"所有可能的历史"和"在模型X下允许的历史"，这显著地简化了关于状态变化可见性的推理。

​	我们可以从状态的角度考虑一致性，描述哪些状态不变式是可以接受的，并建立不同副本上的数据拷贝之间所准许的关系。或者，我们也可以考虑操作一致性，它提供一个数据存储的外部视图，描述了操作并约束它们发生的顺序[TANENBAUM06，AGUILERA16]。

​	<u>如果没有全局时钟，很难给分布式操作一个精确且确定的顺序</u>。这就像是数据的狭义相对论：每个参与者对状态和时间都有自己的视角。

​	**<u>从理论上讲，每当我们想要改变系统状态时，可以先获取一个系统范围的锁，但这是非常不实际的。相反，我们使用一组规则、定义和约束来限制可能的历史和结果的数量</u>**。

​	一致性模型为我们在11.2节中讨论的内容增加了另一个维度。现在我们不仅要兼顾一致性和可用性，还要从**同步代价**的方面考虑一致性[ATTIYA94]。同步代价可能包括时延执行额外操作所花费的**CPU周期**、**用于持久化恢复信息的磁盘I/O**、**等待时间**、**网络I/O**，以及一切可以通过避免同步而节省的代价。

​	首先，我们将重点关注操作结果的可见性和传播。回到并发读写的例子，通过将写入按照依赖关系一个接一个地放置，或定义一个新值被传播出去的时间点，我们就能限制可能的历史数量。

​	我们从执行数据读写操作的进程(客户端)的角度来讨论一致性模型。由于我们在数据复制的上下文中讨论一致性，因此我们假设数据库可以有多个副本。

### 11.5.1 严格一致性

​	严格一致性(strict consistency)相当于完全透明的复制：**<u>任何进程的任何写入都可以立即被任何进程的后续的读操作读取</u>**。<u>它涉及全局时钟的概念，如果在时刻t1有write(x，1)，则任何read(x)的操作将在时刻t2>t1时返回新写入的值t1</u>。

​	不幸的是，**<u>这只是一个理论模型，且不可能实现</u>**，因为物理定律和分布式系统的工作方式限制了事情发生的速度[SINHA97]。

### * 11.5.2 可线性化

​	可线性化(linearizability)是最强的单对象、单操作一致性模型。**该模型下，在写操作开始和结束之间的某个时间点，写操作的效果严格一次性地对所有读取者可见，没有客户端会观察到状态转移、部分(即未完成的、仍在进行中的)或不完全(即在完成之前中断的)的写入操作**[LEE15]。

​	并发操作表现为可见性属性所支持的可能的顺序历史记录之一。可线性化有一定的不确定性，因为可能存在不止一种方式来对事件进行排序[HERLIHY90]。

​	如果两个操作重叠，它们可以按任何顺序生效。<u>在写操作完成之后发生的所有读操作都可以观察到该操作的结果</u>。**一旦一个读操作返回一个特定的值，在它之后的所有读操作返回的值至少都与它返回的那个值一样新**[BAILIS14a]。

​	**在全局历史中，并发事件发生的顺序有一定的灵活性，但它们不能被任意地重新排序**。操作的结果生效不能早于操作的开始时间，因为谁也不能预测未来。同时，结果必须在完成之前生效，否则，我们无法定义一个线性化点。

​	可线性化既尊重顺序性进程局部的操作顺序，也尊重相对于其他进程的并行操作的顺序，因此它定义了事件的全序关系(total order)。

​	这个顺序应该是一致的，这意味着共享值的每次读取都应该返回在此之前写入此共享变量的最新值，或者返回与此读取重叠的写操作所写入的值。**对共享变量的可线性化写访问也意味着互斥：在两个并发写之间，只有一个可以先进行**。

​	即使操作是并发并存在重叠，它们的效果也会以一种看似连续的方式变得可见。当然，没有一个操作是瞬间发生的，但操作看起来仍然是原子的。

**线性化点**

​	<u>可线性化最重要的一个特征是可见性：一旦操作完成，每个参与者都必须能看到它，并且系统不能“穿越到过去”还原它或使它对某些参与者不可见。换句话说，**可线性化禁止读取过时的数据**，它要求读取是单调的</u>。

​	这种一致性模型最好用原子的(即：不间断、不可分割)操作来解释。<u>操作不一定是瞬时的(也因为并不存在瞬时的东西)，但是它们的效果必须在某个时间点变得可见，从而造成操作是瞬时的错觉。这个时刻称为**线性化点(linearization point)**</u>。

​	越过写操作的线性化点(换句话说，当该值对于其他进程变得可见时)，每个进程必然看到该操作所写的值或更新的值(如果在该值之后还有其他的写操作)。<u>一个可见值应保持稳定，直到下一个值变得可见，寄存器不应在这两个临近的状态之间反复</u>。

> 当下大多数编程语言都提供了原子原语，允许原子写操作和原子比较-交换(CAS)操作。原子写操作不考虑当前寄存器值，这与CAS不同，CAS仅在前一个值没有变化时才从一个值变成另一个值[HERLIHY94]。读取值后修改它，并随后用CAS写入，这比简单地检查和设置值要复杂得多，因为可能存在**ABA问题**[DECHEⅥ10]：如果CAS期望A出现在寄存器中，那么值A将最终被设置，即使通过其他两个并发写入操作设置了值B并切换回值A。换句话说，值A本身不变并不能保证自上次读取以来该值没有被改过。

​	<u>线性化点起到切分点的作用，在此之后，操作效果变得可见。我们可以通过使用**锁**来保护临界区、**原子读/写**或**读-修改-写(read-modify-write)原语**来实现它</u>。

**可线性化的代价**

​	**<u>当今许多系统避免实现可线性化</u>**。

​	**在默认情况下，即使是CPU在访问主存时也不提供可线性化一致性**。这是因<u>为**同步指令开销大、速度慢，并且涉及跨节点CPU流量和缓存失效**</u>。

​	然而，**可以使用底层原语来实现可线性化**[MCKENNEY05a，MCKENNEY05b]。

​	在并发编程中，你可以使用CAS操作来引入可线性化。**<u>许多算法的工作方式都是先准备结果，然后使用CAS交换指针使结果可见</u>**。例如，并发队列可以这样实现：创建一个链表节点，然后原子性地将其追加到列表尾部[KHANCHANDANI8]。

​	在分布式系统中，可线性化需要协调冲突和顺序。它可以使用**共识算法**来实现：客户端使用消息与复制存储进行交互，**共识模块**负责确保应用的操作在整个集群中是一致且相同的。**毎个写操作将在它的调用和完成事件之间的某个时间点瞬间出现且仅出现一次**[HOWARD14]。

​	有趣的是，可线性化在其传统理解中被视为*局部性质*，并且意味着其是独立实现和验证的单元组合。**<u>合并可线性化的历史将产生一个同样可线性化的历史</u>**[HERLIHY90]。换句话说，其中所有对象都是可线性化的系统，也是可线性化的。这是一个非常有用的属性，但是我们应该记住，**它的范围仅限于单个对象**，而且即使对两个独立对象的操作是可线性化的，<u>涉及两个对象的操作还必须依赖于额外的同步手段</u>。

> **可重用可线性化基础设施**
>
> ​	**可重用可线性化基础设施(Reusable Infrastructure For Linearizability，RIFL)是一种用于实现可线性化远程过程调用(RPC)的机制**[LEE15]。在RIFL中，消息用客户端ID和客户端本地单调递增的序列号唯一标识。
>
> ​	为了分配客户端ID，RIFL使用由系统层面的服务颁发的租约——规避重复序列号的唯一标识符。如果发生故障的客户端试图使用过期的租约执行操作，它的操作将不会被提交：客户端必须接收新的租约，然后重试。
>
> ​	<u>如果服务器在写操作回复确认消息前崩溃，客户端可能会尝试重试此操作，而不知道此操作已被应用</u>。甚至会出现这样的情况：客户端C1写入值V1，但没有收到确认。同时，客户端C2写入值Ⅴ2。如果C1重试其操作并成功写入V1，则C2的写入操作将丢失。为了避免这种情况，系统需要防止重复执行重试操作。**当客户端重试操作时，<u>RIFL返回一个完成对象(completion object)来指示与它相关联的操作已经被执行并返回结果，而不是再次应用操作</u>**。
>
> ​	<u>**完成对象**与**实际数据记录**一起储存在持久存储中</u>。但是，它们的生存期是不同的：在发出请求的客户端保证它不会重试相关联的操作之前，或者服务器检测到客户端崩溃之前，完成对象都应该存在。而在这两种情况下，所有与它相关联的完成对象都可以被安全地删除。**创建完成对象的操作应该与它所关联数据记录的变化在一起原子地执行**。
>
> ​	**客户端必须定期续订租约，以表明它们的活动性**。<u>如果客户端未能续订其租约，它将被标记为已崩溃的，并且所有与其租约相关联的数据都将被垃圾回收</u>。租约有个有限的生存期，以确保属于故障进程的操作不会被永远保留在日志中。**如果出现故障的客户端试图使用到期的租约继续运行，其结果将不会被提交，客户端将不得不从头开始**。
>
> ​	<u>**RIFL的优点在于，通过保证RPC不会被多次执行**，一个操作可以通过确保其**结果原子可见**而**实现可线性化**，并且它的大部分实现细节独立于底层存储系统</u>。

### * 11.5.3 顺序一致性

​	实现可线性化代价可能过高，但是可以在放松模型的同时仍然提供相当强的一致性保证。<u>*顺序一致性(sequential consistency)*允许对操作进行排序，就好像它们是以某种串行顺序执行的一样，并要求属于某一进程的操作在排序后保持先后顺序不变(不同进程的操作之间，先后顺序可能改变)</u>。

​	进程可以观察到其他参与者执行操作的顺序与它们自己的历史相一致，但是从全局角度看，这个视图可能会过时任意长时间[KINGSBURY18a]。<u>进程之间的执行顺序是未定义的，因为这里没有共享的时间概念</u>。

​	顺序一致性最初是在并发这个领域中引入的，被描述为一种正确执行多处理器程序的方式。最初的描述要求对同一单元的内存请求在队列中排序(FIFO，到达顺序)，对独立内存单元的重叠写操作没有施加全局排序，并且允许读操作从内存单元中获取值(如果队列非空则从队列中获取最新值)[LAMPORT79]。这个例子有助于理解顺序一致性的语义。<u>操作可以以不同的方式排序(取决于到达顺序，甚至可以在两个写入同时到达的情况下任意排序)，但**所有进程都以相同的顺序观察到操作**</u>。

​	每个进程都可以按照自己的程序指定的顺序发出读写请求，这是很符合直觉的，因为任何非并发的单线程程序都一个接一个地执行它的步骤。<u>从同一进程传播的所有写操作都按此进程提交它们的顺序出现。**来自不同来源的操作可以任意排序，但从读取者的角度来看，这个顺序是一致的**</u>。

> ​	顺序一致性常与可线性化混淆，因为两者具有相似的语义。顺序一致性与可线性化一样，要求操作的全局有序性，而<u>可线性化要求每个过程的**局部有序性**与**全局有序性**一致</u>。换句话说，可线性化遵循操作的真实顺序。而在顺序一致性条件下，顺序仅适用于来源相同的写入[VIOTTI16]。另一个重要的区别在于组合：<u>我们可以组合可线性化的历史，并且仍然期望结果是可线性化的，而**顺序一致的调度是不可组合的**[ATTIYA94]</u>。

​	**<u>读到过时值(stale read)可以用副本差异来解释，例如：即使写入以相同的顺序传播到不同的副本，它们也可能在不同的时间到达</u>**。

​	**<u>顺序一致性与可线性化的主要区别在于没有全局强制的时间界限</u>**。在可线性化中，一个操作必须在其挂钟时间范围内变得有效。当写入操作W1完成时，它的结果必须已经被应用，并且每个读取者应该能够看到至少与W1所写入的值一样新的值。类似地，在一个读操作R1返回之后，在它之后执行的任何读操作都应该返回R1已经看到的值或更晚的值(当然，这也必须遵循同样的规则)。

​	顺序一致性放宽了这一要求：操作的结果可以在操作完成后才变得可见，只要从单个参与者的角度来看顺序是一致的。**同一来源的写操作程序不能互相"跳过"：它们的程序顺序(相对于自己的执行进程)必须保持不变。另一个限制是<u>操作出现的顺序必须对所有读取者一致</u>**。

​	**<u>与可线性化相似，现代CPU在默认情况下不保证顺序一致性</u>**，而且，由于处理器可以重新排序指令，我们应该使用**<u>内存屏障</u>**(也称为栅栏(fences))来确保写入操作对并发线程按顺序可见[DREPPER07，GEORGOPOULOS16]。

### * 11.5.4 因果一致性

​	尽管通常没有必要使用全局操作顺序，但在某些操作之间建立顺序可能是必要的。在因果一致性(causal consistency)模型下，所有过程都必须以相同的顺序看到因果相关的操作。没有因果关系的并发写入可以被不同的进程以不同的顺序观察到。

​	首先，让我们来看看为什么需要因果关系，以及没有因果关系的写操作是如何传播的。进程P1和P2进行非因果顺序的写操作。这些操作的结果可以在不同的时间无序地传播到读取者。进程P3先看到值1，然后才看到值2，而P4将首先看到值2，然后才看到值1。

​	一个有因果关系的写入的例子。除了写入值之外，我们现在还必须指定一个<u>逻辑时钟值</u>，以便在操作之间建立因果顺序。P1以写入操作 write(x，0，1) -> t1开始，该写入操作从初始值开始。P2执行另一个写操作 write(x，t1，2)，并指定其逻辑排序在t1之后，要求操作仅按逻辑时钟建立的顺序传播。

​	这样便在这些操作之间建立了因果顺序。<u>即使后一个写入比前一个传播得更快，在它的所有依赖项到达之前，该写入是不可见的，并且事件顺序会从它们的逻辑时间戳重建</u>。换句话说，**在逻辑上建立了发生在前(happened-before)的关系，而不使用物理时钟并且所有进程都认同这个顺序**。

​	**<u>在因果一致的系统中，我们获得了应用程序的会话保证，确保数据库视图与它自己的行动相一致，即使它在不同的、潜在的不一致的服务器上执行读写请求</u>**[TERRY94]。这保证是：单调读、单调写、读自己写(read-your-wite)、读后写(write-follow-read)。你可以在11.6节中找到关于这些会话模型的更多信息。

​	**<u>因果一致性可以使用逻辑时钟(logical clock)[LAMPORT78]来实现</u>**，在每个消息中发送上下文元数据，并总结哪些操作在逻辑上先于当前操作。<u>当从服务器接收到更新时，更新包含了上下文的最新版本。任何操作只有在其前面的所有操作都已应用的情况下才能被处理</u>。上下文不匹配的消息将缓存在服务器上，因为传递这些消息还为时过早。

​	实现因果一致性的两个重要且经常被引用的项目是Clusters of Order-Preserving Servers(COPS)[LLOYD11]和 Eiger[LLOYD13]。这两个项目都通过一个库实现因果关系(被实现为用户要连接的前端服务器)，并跟踪依赖关系以确保一致性。COPS通过键版本跟踪依赖关系，而Eiger则建立操作顺序(Eiger中的操作可能依赖于在其他节点上执行的操作，例如，在多分区事务的情况下)。这两个项目都不像最终一致的存储那样暴露无序操作。相反，<u>它们检测和处理冲突：在COPS中，这通过检查键顺序和使用应用程序特定的函数来完成；而 Eiger实现了“最后写胜出” (last-write-wins)规则</u>。

**向量时钟**

​	建立因果顺序使得系统可以在消息无序传递的情况下重新构建事件序列，填补消息之间的空隙，避免在某些消息仍然缺失的情况下发布操作结果。例如，如果消息{M1(∅，t1)，M2(M1，t2)，M3(M2，t3)}(均指定了依赖关系)是因果相关的，但以乱序传播，则进程会将它们缓冲起来，直到收集到所有的操作依赖项并恢复消息的因果顺序[KINGSBURY18b]。许多数据库，例如Dynamo[DECANDIA07]和Riak[SHEEHY10a]，使用向量时钟[LAMPORT78， MATTERN88]来建立因果顺序。

​	**向量时钟(vector clock)**是一种用于在事件之间建立偏序、检测和解决事件链之间分歧的结构。利用向量时钟，我们可以模拟公共时间和全局状态，将异步事件表示为同步事件。<u>进程维护逻辑时钟的向量，其中**毎个时钟对应一个进程**</u>。每个时钟从初始值开始每次新事件到达(例如发生写入)时递增。当从其他进程接收到时钟向量时，进程将其本地向量中的各个值更新为接收到向量中对应进程时钟的最大值(即，传输节点曾经看到的最大时钟值)。

​	<u>为了使用向量时钟来解决冲突，每当我们对数据库进行写入时，首先检查写入键的值是否已经在本地存在。如果前值已经存在，我们将一个新版本追加到版本向量中，从而建立两个写入之间的因果关系。否则，我们启动一个新的事件链，并用单个版本初始化该值</u>。

​	我们在两种场景下讨论了一致性问题：共享内存寄存器的访问以及真实(依据挂钟时间)的操作顺序。讨论过程中，我们首先提到了**潜在的副本差异**。因为<u>只有对相同内存位置的写操作才必须排序，所以如果值是独立的，就不会出现写冲突的情况</u>[LAMPORT79]。

​	<u>由于我们要找的是一个能够提高可用性和性能的一致性模型，**我们必须允许副本出现分歧**——不仅因为读到过期数据，还因为接受潜在冲突的写操作</u>。因此系统被允许创建两个独立的事件链。从一个副本的角度来看，我们看到历史为1，5，7，8，而另一个副本认为是1，5，3。Riak允许用户查看不同的历史记录并解决冲突[DAILY13]。

> ​	为了实现因果一致性，我们必须存储因果关系历史，支持垃圾收集，并要求用户在冲突的情况下解决分歧。
>
> ​	而**向量时钟可以告诉你冲突已经发生，但并不提出确切的解决方法**，因为解决语义通常是应用程序特定的。
>
> ​	因此，一些最终致的数据库，例如Apache Cassandra，不会对操作进行因果排序，而是使用“**最后写胜出**”规则来解决冲突[ELLIS13]。

## * 11.6 会话模型

​	从值传播的角度考虑一致性对于数据库开发者很有用，因为它有助于我们去理解和施加所需的数据不变式，但是有些事情却是从客户端角度看更容易理解和解释。这里，我们可以从单个客户端而不是多个客户端的角度来看待分布式系统。

​	**会话模型(session mode)**[VIOTTI16] (也称为以客户端为中心的一致性模型[TANENBAUM06])帮助我们从客户端的角度推理分布式系统的状态：*每个客户端在发出读写操作时如何观察系统的状态*。

​	<u>如果说到目前为止我们讨论的其他一致性模型聚焦在解释同时存在多个客户端时的操作顺序，那么以客户端为中心的一致性，则是聚焦在单个客户端如何与系统交互</u>。我们仍然假设每个客户端的操作是顺序的：它必须在完成一个操作之后才能开始执行下一个操作。如果客户端在其操作完成之前崩溃或失去与服务器的连接，我们不对未完成操作的状态做任何假设。

​	在分布式系统中，客户端通常可以连接到任何可用的副本，并且如果最近对一个副本的写入结果没有传播到另一个副本，则客户端可能无法观察到该写入所做的状态更改。

​	<u>一个合理的期望是，客户端发出的毎一个写操作都对它自己可见。这个假设在**读自己写(read-own-write)一致性模型**下成立，该模型声明：当写操作完成后，在**相同或其他副本**上的每个读操作都必须能够观察到写入的值。例如，在 **write(x，V)之后立即执行的read(x)将返回值V**</u>。

​	**单调读(monotonic read)模型**限制值的可见性，它声明：如果read(×)已经观察到值V，则接下来的读必须观察到至少与V一样新的值或某个更新的值。

​	**单调写(monotonic write)模型**假定同一客户端写入的值以写入操作执行的顺序出现。如果根据客户端会话顺序，在 write(x，V1)操作之后进行 write(x，V2)操作，则它们的效果必须以相同的顺序(即先V1后V2)对所有其他进程可见。如果没有这种假设，旧数据则可能“复活”，从而导致数据丢失。

​	**读后写(Write-follow-read**，有时称为会话因果关系)确保写入被排在之前的读取操作观察到的写入之后。<u>例如，如果write(x，V2)操作排在返回V1的read(x)操作之后，则write(x，V2)将排在write(x，V1)之后</u>。

> ​	会话模型对由不同进程(客户端)或来自不同逻辑会话的操作没有任何假设[TANENBAUM14]。这些模型从单个进程的角度描述操作排序。然而，系统中的毎一个进程都必须有相同的保证。换句话说，如果P1能够读取它自己的写操作，那么P2也应该能够读取它自己的写操作。

​	将单调读、单调写和读自己写结合起来，可以提供流水线随机访问存储器(Pipelined RAM，PRAM)一致性[LIPTON88， BRZEZINSKI03]，也称为**FIFO一致性**。<u>PRAM保证来自一个进程的写操作将按照该进程执行它们的顺序传播。与顺序一致性不同的是，来自不同进程的写入可以被以不同的顺序观察到</u>。

​	以客户端为中心的一致性模型所描述的特性很有用，大多数情况下，**分布式系统的开发者利用这些特性来验证他们的系统并简化其使用**。

## * 11.7 最终一致性

​	在多处理器编程和分布式系统中，同步都是昂贵的。正如在11.5节中所讨论的，我们可以放松一致性保证，允许节点之间存在一些差异。例如，顺序一致性允许读取以不同的速度传播。

​	**在最终一致性(eventual consistency)下，更新将异步地在系统中传播**。形式上说，它声明如果没有对数据项执行额外的更新，最终(eventually)所有的访问都会返回最新写入的值[VOGELS09]。在冲突的情况下，最新值的概念可能会改变，因为会取决于使用何种冲突解决策略(如最后写胜出或使用向量时钟(参见11.5.4节))来协调分歧副本中的值。

​	<u>*最终*是描述值传播的一个有趣的术语，因为它没有指定它必须发生的硬性时间限制。如果传递服务仅仅提供了一个“最终”的保证，这听起来就很不可靠。**然而在实践中，这个模型工作得很好，当下许多数据库都是最终一致的**</u>。

## * 11.8 可调一致性

​	最终一致性的系统有时用CAP来描述：你可以用可用性来换取一致性，反之亦然(参见1.2节)。从服务器端的角度来看，最终一致的系统通常实现可调一致性，其中使用三个变量调节数据的复制、读取和写入：

+ **复制因子N**

  储存数据副本的节点数。

+ **写入一致性W**

  使写入成功需要确认的节点数。

+ **读取一致性R**

  使读取成功需要响应的节点数。

​	<u>**通过选择一致性级别使之满足R+W>N，系统可以保证返回最近写入的值，因为读取集合和写入集合之间总是存在重叠**</u>。例如，如果N=3，W=2，R=2，则系统仅能容忍一个节点的故障。三个节点中的两个必须确认写操作。理想情况下，系统还会异步地将写复制到第三个节点。如果第三个节点宕机，反熵机制(参见第12章)最终会传播这个写入。

​	读取时，至少需要有三个副本中的两个来处理请求，才能回复一致的结果。任何一组节点都将至少包含一个具有给定键的最新记录的节点。

> ​	当执行写操作时，协调者应该将其提交给N个节点，但是**在继续操作之前只等待W个节点**(或者在协调者也是副本的情况下等待W-1个节点)。其余的写操作可以异步完成或失败。类似地，当执行读取时，协调者必须收集至少R个响应。某些数据库使用*推测执行(speculative execution)*提交额外的读取请求以减少协调者响应延迟。这意味着如果最初提交的读请求中的一个失败或到达缓慢，推测请求可以被计入R。

​	<u>写入较多的系统有时可能选择W=1，R=N，这允许写操作仅由一个节点确认，但也会要求所有副本(甚至可能出现故障的副本)都可用于读操作。对于W=N，R=1的组合也是如此：写入必须应用到所有副本才算成功，因此便可以从任何节点读取最新值</u>。

​	**增加读或写一致性级别会增加延迟，并提高请求期间对节点可用性的要求**。而降低一致性级别则可以提高系统的可用性，同时牺牲一致性。

> Quorum
>
> ​	**由[N/2]+1个节点组成的一致性级别称为 Quorum，即多数派节点**。网络分区或节点故障的情况下，在2f+1个节点的系统中，如果不可用的节点数不大于f，则活动节点还可以继续接受写入或读取。换句话说，这样的系统最多能容忍f个节点故障。
>
> ​	当使用Quorum执行读写操作时，系统不能容忍多数派节点的故障。例如，如果总共有三个副本，其中两个已宕机，则读写操作将无法达到读写一致性所需的节点数，因为三个节点中只有一个节点能够响应请求。
>
> ​	在不完全写入的情况下，使用Quorum进行读写并不能保证单调性。如果在向三个副本中的一个副本写入值之后，某个写操作失败，则Quorum读取可能返回不完全操作的结果或旧值，具体取决于所联系的副本。由于后续的对相同值的读取不需要联系同一个副本，因此它们返回的值可能交替出现两种结果。**为了实现读单调性(在牺牲可用性的情况下)，我们需要使用阻塞读修复(参见12.1节)**。

## * 11.9 见证者副本

​	在读取一致性上使用Quorum能够提高可用性：即使某些节点宕机，数据库系统仍然可以接受读取和写入。要求多数派参与保证了在任意多数派集合中至少有一个节点是重叠的，因此任何 Quorum读取都将观察到最近完成的 Quorum写入操作。但是，<u>使用复制和多数派会增加存储成本：我们必须在每个副本上储存数据的一份拷贝</u>。如果我们的复制因子是5，则我们必须存储5个拷贝。

​	<u>我们可以通过使用见证者副本(witness replica)来降低存储成本。我们不需要在毎个副本上存储数据记录的拷贝，而是可以将副本分为拷贝副本(copy replica)和见证者副本</u>。拷贝副本仍然持有以前的数据记录。在正常操作下，<u>见证者副本仅储存一些记录，表示写操作发生过的事实</u>。然而，当拷贝副本的数量太少时，可能会发生一种情况，例如我们有三个拷贝副本和两个见证者副本，此时两个拷贝副本宕机，我们最终的Quorum是个拷贝副本和两个见证者副本。

​	而在写入超时或拷贝副本发生故障的情况下，为了临时存储记录，可以升级见证者副本以临时代替故障或超时的拷贝副本。一旦原来的拷贝副本恢复，升级的副本就可以回退到它以前的状态，或者也可以让恢复后的副本成为见证者副本。

​	让我们考虑一个三节点的复制系统，其中两个节点保存数据拷贝，第三个节点是见证者副本：[1c，2c，3w]。我们试图进行写操作，但2c暂时不可用，无法完成这个写操作在这种情况下，我们将这条记录临时储存在见证者副本3w上。无论2c何时恢复，修复机制都可以使其恢复到最新状态，并从见证者副本上移除多余的拷贝。

​	<u>另一种情况，我们尝试执行读取，要读的记录出现在1c和3W上，但不在2c上。由于任意两个副本足以构成 Quorum，只要有任意两个节点的子集可用，无论是两个拷贝副本[1c，2c]，还是一个拷贝副本和一个见证者副本(即[1c，3w]或[2c，3w])，我们都可以保证提供一致的结果。如果从[1c，2c]读取，我们从1c处获取最新的记录，并可以将其复制到2c，因为2c上缺少这个值。如果只有[2c，3w]可用，则可从3W获取最新记录。要恢复原来的节点配置并让2c追上最新数据，可以将该记录复制到2c并从见证者副本3w中删除</u>。

​	更一般地，只要我们遵循如下两个规则，那么n个拷贝副本和m个见证者副本就能达到与n+m个副本相同的可用性保证：

+ **使用多数派(即N/2+1个参与者)执行读和写操作**。
+ **该Quorum中至少有一个副本是拷贝副本**。

​	能这么做的原因是数据一定要么在拷贝副本上，要么在见证者副本上。在发生故障时，修复机制会更新拷贝副本，在此期间数据储存在见证者副本上。

​	使用见证者副本可以降低存储成本，同时保持一致性约束。这种方法有几种实现，例如Spanner[CORBETT12]和Apache Cassandra。

## * 11.10 强最终一致性和CRDT

> [CRDT——解决最终一致问题的利器_hellozhxy的博客-CSDN博客](https://blog.csdn.net/hellozhxy/article/details/103527186)
>
> [聊聊CRDT - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1422458)

​	我们讨论了几种强一致性模型，例如可线性化和串行化，以及弱一致性的一种形式：最终一致性。在两者之间还存在一种可能的中间地带——强最终一致性，提供了两种模式各自的一些好处。该模型下，更新可能会延迟或无序地传播到其他节点，但当所有更新最终传播到目标节点时，它们之间的冲突可以被解决，并且它们合并后将产生相同的有效状态[GOMES17]。

​	在某些条件下，我们可以放宽一致性要求，允许操作保留附加的状态，该附加状态使得我们可以在操作执行后协调(或者说合并)出现分歧的状态。这种方法最突出的一个例子是在Redis[Blyikoglu13]中实现的*无冲突复制数据类型*(Conflict-Free ReplicatedData Type，CRDT，参见文献[SHAPIRO11a])。

​	**CRDT是一种特殊的数据结构，它排除了冲突的存在，允许以任意顺序应用对它的操作而不改变结果。这个特性在分布式系统中非常有用**。例如，在使用无冲突复制计数器的多节点系统中，我们可以独立地增加毎个节点上的计数器值，即使它们由于网络分区而不能相互通信。一旦恢复通信，来自所有节点的结果就可以进行协调与合并，分区期间应用的任何操作都不会丢失。

​	这样的特性使得CRDT在最终一致性系统中很有用，因为<u>它允许系统中的副本状态临时出现分歧</u>。副本可以在本地执行操作，而无须事先与其他节点同步，操作最终会(可能无序地)传播到所有其他副本。CRDT让我们能够从局部的个体状态或操作序列重建完整的系统状态。

​	CRDT最简单的例子是基于操作的可交换复制数据类型(Commutative Replicated DataType， CmRDT)。 CmRDT的操作需要满足：

+ 无副作用

  应用这些操作不会改变系统状态。

+ 可交换

  参数顺序无关紧要：x·y=y·x。换句话说，无论是x与y合并还是y与X合并不改变最终结果。

+ 按照因果关系排序

  操作成功传递所依赖的前提条件是：要确保系统已经达到操作可以应用的状态。

​	例如，我们可以实现仅增长计数器(grow-only counter)。每个服务器可以保存一个状态向量，该状态向量由包含所有其他参与者的最近已知计数器更新，初始值为零。每个服务器只允许修改向量中自己的值。当更新被传播时，函数merge(state1，state2)合并来自两个服务器的状态。

​	例如，我们有三台服务器，都有已被初始化而处于初始状态的向量：

```pseudocode
节点1:			节点2:		节点3:
[0,0,0]		[0,0,0]		[0,0,0]
```

​	如果我们更新第一个和第三个节点上的计数器，它们的状态会发生如下变化：

```pseudocode
节点1:			节点2:		节点3:
[1,0,0]		[0,0,0]		[0,0,1]
```

​	当更新传播时，我们使用一个合并函数，选取每个位置上的最大值来组合结果：

节点1(已传播节点3的状态向量):

```pseudocode
merge([1，0，0]，[0，0，1])=[1，0，1]
```

节点2(已传播节点1的状态向量)：

```pseudocode
merge([0，0，0]，[1，0，0])=[1，0，0]
```

节点2(已传播节点3的状态向量)：

```pseudocode
merge([1，0，0]，[0，0，1)=[1，0，1]
```

节点3(已传播节点1的状态向量)：

```pseudocode
merge([0，0，1]，[1，0，0])=[1，0，1]
```

​	为了确定当前向量状态，可以计算所有位置上的值的和：sum([1，0，1])=2。合并函数是可交换的。由于服务器只能更新自己的值，而且这些值是相互独立的，因此不需要额外的协调。

​	通过节点用于递增计数的向量P和节点用于递减计数的向量N，可以实现一个支持增减的正负计数器(PN-Counter)。在较大规模的系统中，为了避免传播巨大的向量，可以使用*超级对等节点(super-peers)*。超级对等节点复制计数器的状态，从而帮助避免持续的点对点交流[SHAPIRO11b]。

​	要保存和复制值，我们可以使用寄存器。最简单的寄存器是**最后写入胜出(Last-Write-Wins，LWW)寄存器**，它为每个值存储一个唯一的、全局排序的时间戳以解决冲突。当发生写冲突时，我们只保留具有较大时间戳的那个值。这里的合并操作(选择具有最大间戳的值)也是可交换的，因为它依赖于时间戳。如果不允许丢弃值，我们可以提供应用程序特定的合并逻辑，并使用多值寄存器，该寄存器存储所有写入的值，让应用程序来选择正确的值。

​	CRDT的另一个例子是无序仅增长集合(Grow-only Set，G-Set)。每个节点维护本地状态，并可以向其追加元素。添加元素将生成一个有效集合。合并两个集合也是一个可交换的操作。与计数器类似，我们可以使用两个集合来支持增加和删除。<u>在这种情况下我们必须遵循一个约束：只有在增加集合中的值才能被插入到删除集合中。要想重建集合的当前状态，可以从增加集合中移除被包含在删除集合中的所有元素[ SHAPIRO1b]</u>。

​	结合更复杂结构的另一个无冲突类型的例子是无冲突复制JSON数据类型，它允许对具有列表和映射表类型的深度嵌套JSON文档进行诸如插入、删除和赋值等修改。此算法在客户端执行合并操作，并且不要求操作以任何特定顺序传播[KLEPPMANN14]。

​	CRDT为我们提供了相当多的可能性，我们看到越来越多的数据存储利用它来提供强最终一致性(SEC)。这是一个强有力的概念，我们可以将其添加到构建容错分布式系统的工具库中。

## 11.11 本章小结

​	容错系统使用复制来提髙可用性：即使某些进程发生故障或没有响应，系统作为一个整体也可以继续正常运行。但是，保持多个副本同步需要额外的协调。

​	我们讨论了几个单操作一致性模型，从保证最多的到保证最少的依次是：

+ 可线性化

  操作看起来是瞬时应用的，并且保持操作实际的顺序。

+ 顺序一致性

  操作效果是以某种全序传播的，此顺序与它们被单个进程执行的顺序一致。

+ 因果一致性

  因果相关的操作的效果以相同的顺序对所有进程可见。

+ PRAM/FIFO 一致性

  操作变为可见的顺序与它们在各个进程执行的顺序相同。来自不同进程的写操作可能会被以不同的顺序观察到。

​	之后，我们讨论了多个会话模型：

+ 读自己写

  读操作反映先前的写操作。写操作经过系统传播一定能被之后来自同一客户端的读操作看到。

+ 单调读

  任何读操作不能读到比已经读过的值更早的值。

+ 单调写

  来自同一客户端的写入按此客户端执行的顺序传播到其他客户端。

+ 读后写

  同一个客户端执行的读操作如果观察到一些写操作，则后续的写操作将会排在这些写操作之后。

​	理解这些概念可以帮助你理解底层系统能提供何种保证，并将其用于应用程序开发。一致性模型描述了操作数据必须遵循的规则，但仅限于特定的系统。将具有较弱保证的系统构建在具有较强保证的系统之上，或者忽略底层系统的一致性含义，可能会导致不可恢复的不一致性和数据丢失。

​	我们还讨论了最终一致性和可调一致性的概念。基于Quorum的系统使用多数派来保证数据的一致性。见证者副本可用于降低存储成本。

# 12. 反熵和传播

​	到目前为止，我们讨论的通信模式都是点对点(对等的)或一对多的(协调者和副本)。为了在整个系统中可靠地传播数据记录，我们需要传播节点本身是可用的并能够访问到其他节点，但是即便如此，吞吐量也受单台机器限制。

​	快速而可靠的传播可能不太适用于大量数据记录，但是对集群范围的元数据却很重要，例如成员信息(节点加入或离开)、节点状态、故障情况、表结构变更等。传达这些信息的消息往往出现频率不高且数据量很小，但是必须被尽可能快且可靠地传播。

​	将此类的更新传播到集群中的所有节点通常有以下三大类方法[DEMERS87]：

+ a)通知从一个进程广播到其他所有进程。
+ b)定期的点对点交换信息，对等节点成对地连接并交换消息。
+ c)合作广播，消息接收者会成为广播者，帮助更快更可靠地传播消息。

​	当集群中的节点数较少时，将消息广播到其他所有进程是最简单直接的方法。但在大型集群中，由于节点数量庞大，广播会变得很昂贵。并且由于过度依赖单个进程，广播也很不可靠。各个进程可能并不总是知道网络中其他所有进程的存在。此外，广播的进程及其毎个接收者都必须在工作时间上有重叠，而这在某些情况下可能难以实现。

​	为了放宽这些限制，我们可以假设某些更新可能会传播失败。协调者尽力将消息传递给所有可用的参与者，而<u>一旦出现故障，**反熵(anti-entropy)机制**会让节点重新同步</u>。这样，传递消息的责任由系统中所有节点共同承担，并分成两个步骤：<u>主要传递</u>和<u>定期同步</u>。

​	熵(entropy)是一种衡量系统无序程度的属性。**在分布式系统中，熵表示节点之间的状态分歧程度**。我们希望将熵保持在最低的水平，有许多技术可以帮助减少熵。

​	<u>反熵机制通常用于在主要传递机制失败的情况下将节点恢复最新状态</u>。即使协调者在某个时刻发生故障，系统也可以继续正常运行，因为其他节点将继续传播信息。换句话说，<u>在最终一致的系统中，反熵被用来降低收敛的时间界限</u>。

​	为了保持各节点的同步，反熵会触发一个后台或前台进程，比较和调和丢失或冲突的记录。后台反熵进程会利用Merkle树之类的辅助数据结构，从更新日志中识别出分歧的数据。前台反熵进程会捎带(piggyback)地在读取或写入请求上附加额外逻辑，比如提示移交、读修复等。

​	如果多副本系统中的副本间出现分歧，为了恢复一致性和同步，我们必须通过成对比较副本状态来查找和修复丢失的记录。对于大型数据集这会非常耗时：我们必须从两个节点分别读出整个数据集，并将尚未传播的、包含最新状态的更改通知相关副本。为了降低成本，我们可以考虑副本过期的方式以及数据访问的模式。

## 12.1 读修复

​	读取时最容易检测出副本之间的差异，因为此时我们可以联系副本，从每个副本上查询状态，然后检查它们的响应是否一致。注意，这种情况下，我们不会查询每个副本上存储的完整数据集，而是将目标限制在那些客户端请求的数据。

​	<u>协调者执行分布式读取时会乐观地假设各个副本是同步的并且存有相同的信息。如果副本的响应不同，协调者会把缺失的变更发送给相应的副本</u>。

​	这种机制称为**读修复(read repair)，它常常被用于检测和消除数据不一致**。<u>读修复中，协调者节点向各个副本发出请求，等待副本响应并对其进行比较。如果某些副本因错过了最近的变更而导致它们的响应不同，那么协调者会检测出不一致，并将变更发送回副本[DECANDIA07]</u>。

​	有些Dynamo式的数据库选择不要求联系所有的副本，而改用可调的一致性级别。要返回一致的结果，我们不必联系并修复所有副本，而只需联系满足一致性级别的节点数。如果我们进行Quorum读取和写入，我们仍可以获得一致的结果，但是某些副本可能仍未包含所有写入。

​	读修复可以实现为阻塞或异步操作。阻塞读修复中，原始的客户端请求必须等到协调者“修复”副本才能返回。而异步读修复仅仅只是调度一个修复任务，读请求的结果可以立即返回给用户。

​	**对于Quorum读来说，阻塞读修复能确保读取的单调性**(参见11.6节)：一旦客户端读取到某个值之后，后续的读请求要么返回一样的值，要么返回更新的值，因为副本状态已经修复了。如果不使用 Quorum读，则会失去这一单调性保证，因为在后续读取时数据可能尚未传播到目标节点。与此同时，阻塞读修复会牺牲可用性，因为修复需要由目标副本确认，并且只有当它们响应后才能返回。

​	为了准确检测出各副本响应的记录之间的差异，一些数据库(例如 Apache Cassandra)使用了具有合并监听器(merge listener)的专用迭代器，它能重新构建出合并后的结果与各个输入之间的差异。之后，协调者将这些差异通知副本，告知它们缺失的数据。

​	读修复假定副本大多数情况下是同步的，我们不希望毎个请求都回退到阻塞修复。由于阻塞修复的读取单调性，只要中间没有发生写操作，我们可以期望后续请求返回相同且一致的结果。

## 12.2 摘要读

​	协调者也可以不向每个节点都发出完全读取请求，而是只发一个完全读取请求，而向其他副本仅仅请求数据的摘要(digest)。<u>摘要请求读取副本本地数据——不是完整的数据快照，而是计算出数据的哈希值</u>。<u>之后，协调者可以算出完整读取的哈希值，并将其与所有其他节点返回的摘要进行比较。如果所有摘要都匹配，则可以确信各个副本是同步的</u>。

​	如果摘要不匹配，协调者无法确定哪些副本是领先的、哪些副本是滞后的。为了使滞后的副本与其他节点恢复同步，协调者需要向那些摘要与其他节点不同的副本发出完全读取请求，比较它们的响应，找出差异之处，并将变更发送给滞后的副本。

> ​	摘要通常使用非加密哈希函数(例如MD5)来计算，因为只有快速地算出摘要才能让“快乐路径”性能更佳。哈希函数可能发生冲突，但是对于大多数实际系统而言，冲突可能性可以忽略不计。数据库通常有不止一种反熵机制，因此我们可以预期，即使在(不太可能发生的)哈希冲突的情况下，数据也将会被其他的子系统修复。

## 12.3 提示移交

​	另一个反熵方法称为提示移交(hinted handoff)[DECANDIA07]，这是一种**写侧修复机制**。如果目标节点未能确认写入，则写入协调者或某一副本会存储一条特殊的记录，称为一个提示(hint)，当目标节点恢复后，该记录会立即被重放过去。

​	在Apache Cassandra中，除非用的是ANY一致性级别[ELLIS11]，提示的写入不用满足复制因子(参见11.8节)，因为提示日志中的数据不会被用于读取，仅仅是帮助滞后的参与者追赶上来。

​	一些数据库(例如Riak)同时使用Sloppy Quorum和提示移交。在Sloppy Quorum中，当副本故障的情况下，写操作可以使用节点列表中的其他健康节点，这些节点可以不是该操作原来的目标副本。

​	例如，假设我们有一个包含节点{A，B，C，D，E}的五节点集群，其中{A，B，C}是写操作对应的副本，而节点B处于不可用状态。A作为查询的协调者，选择节点D以满足Sloppy Quorum并保持所需的可用性和持久性保证。<u>现在，数据被复制到{A，D，C}上。但是，D上的这条记录在元数据中带有一个提示，因为该写入最初是针对B的。一旦B恢复，D将会尝试向它转发提示。当提示在B上重放之后，就可以安全地删除这条记录，而不会减少副本的总数[DECANDIA07]</u>。

​	在类似的情况下，如果节点{B，C}由于网络分区而被短暂地与集群其他部分分开，并且对{A，D，E}进行了 Sloppy Quorum写入，那么，紧随其后的对{B，C}的读取将无法观察到最新的数据[DOWNEY12]。换句话说， **Sloppy Quorum提高了可用性但牺牲了一致性**。

## * 12.4 Merkle树

> [梅克尔树_百度百科 (baidu.com)](https://baike.baidu.com/item/梅克尔树/22456281?fr=aladdin)
>
> [区块链- Merkle树_朝歌-CSDN博客_merkle树](https://blog.csdn.net/qq_40452317/article/details/90482721)

​	由于读修复只能修复当前被查询到的数据的不一致性，我们需要另一套机制来寻找和修复未被查询到的数据的不一致性。

​	<u>正如我们之前所讨论的，要想精确找出副本之间哪些行存在不一致，需要成对地交换和比较数据记录。这是很昂贵且不切实际的。许多数据库使用*Merkle树*[MERKLE87]来降低数据比对的成本</u>。

​	**<u>Merkle树是一个对本地数据的紧凑的哈希表示，它是一棵由哈希值构成的树</u>**。哈希树的底层是通过扫描整个表的数据记录、计算记录范围的哈希值构成的。较高层级则来自对较低层级的哈希值再次进行哈希，从而建立一个层次结构表示形式，使得我们可以通过比较哈希值来快速检测出不一致性，或通过递归地遍历哈希树节点来缩小不一致的范围。这可以通过逐层交换和比较子树来完成，也可以交换和比较整棵树。

​	Merkle树的构成：最低层级包括数据记录范围的哈希值，而每个更高层级的哈希值是通过对下一层级的哈希值进行哈希得出的，递归地重复此过程直至树的根节点。

​	**<u>要确定两个副本间是否存在不一致，我们只需要比较其Merkle树中的根节点哈希值。通过自顶向下成对地比较哈希值，我们可以找到节点间存在差异的数据范围，并修复其中的数据记录</u>**。

​	**<u>由于Merkle树是自底向上递归计算的，因此一个数据的变化会触发整个子树的重新计算。在树的大小(决定了交换消息的大小)和它的精度(数据范围有多小，或者说有多精确)之间也存在一个权衡</u>**。

## * 12.5 位图版本向量

​	关于反熵的最新研究中引入了位图版本向量(bitmap version vector)[GONCALVES15]，它<u>基于最近更新情况来解决数据冲突</u>：**每个节点都会保存各个对等节点的操作日志，这些日志包含本地发生或从副本复制过来的事件**。**反熵时，我们会比较日志，并将丢失的数据复制到目标节点**。

​	<u>每次写入(由某一个节点协调)均表示为点(i，n)：由节点n协调的节点本地序列号为i的一个事件。序列号i从1开始，在每次节点执行写操作时递增</u>。

​	为了跟踪副本状态，我们使用<u>节点本地逻辑时钟</u>。每个时钟代表一组点的集合，表示该节点直接看到(由该节点本身协调的)或间接看到(由其他节点协调并复制过来的)的写入。

​	<u>在节点的逻辑时钟中，节点本身协调的事件之间没有间隙。如果某些写操作没有从其他节点复制过来，时钟则会包含间隙。为了让两个节点重新同步，可以让它们交换逻辑时钟，识别出缺失的点所表示的间隙，然后复制与之相关联的数据记录。为此，我们需要重新构建出毎个点引用的数据记录。这项信息存放在点因果容器(Dotted Causal Container，DCC)中，它将各点映射到给定键上的因果信息。这样一来，冲突解决过程就能够获取写操作之间的因果关系</u>。

​	该方法的**优势在于，它捕获了值写入之间的因果关系，并允许节点精确标识其他节点上缺少的数据点**。一个可能的**缺点是，如果节点长时间宕机，对等节点就一直无法截断日志，因为当滞后节点恢复时，还需要将数据复制给它**。

## 12.6 Gossip传播

 	为了纳入其他节点，并让更新的传播具有广播的范围和反熵的可靠性，我们可以用Gossip协议。

​	<u>Gossip协议是一种概率性的通信过程，它是基于谣言在人类社会中的传播方式或疾病在人群中的传播方式</u>。谣言和流行病十分形象地描述了该协议的工作方式：只要还有想听的人，谣言就会继续蔓延；只要人群中还有易感的成员，疾病就会继续传播。

​	Gossip协议的主要目标是使用协作式的传播将信息从一个进程传播到集群的其余部分就像病毒在整个人群中传播的过程，它从一个人传染给另一个人，每一步都可能扩大感染的范围， Gossip协议中信息也在系统中不断中继，使越来越多的进程参与进来。

​	<u>持有需要传播的记录的进程称为传染性的(infective)，而任何尚未收到更新的进程称为易感染的(susceptible)</u>。<u>传染性的进程经过一段时间的主动传播之后，不再传播新状态，这时我们称它为已删除的(removed)</u> [DEMERS87]。所有进程都以易感染状态开始。每当某个数据记录的更新到达时，接收到该更新的进程将变成传染性状态，并开始将更新分发给其他随机的相邻进程，从而感染它们。<u>一旦传染性进程确定更新已经传播，它们就会变成已删除状态</u>。

​	**<u>为了避免显式的协调和维护全局的接收者列表，以及避免要求单个协调者向系统中的每个参与者都广播消息，这类算法使用兴趣损失(loss of interest)函数建模完整性</u>**。协议的效率取决于：在将发送冗余消息的开销保持在最低的情况下，能多快地感染尽可能多的节点。

​	Gossip可用于同构(所有节点都是对等的)去中心系统中的异步消息传递，这类系统中，节点可能没有长期成员资格，也没有以任何拓扑进行组织。**由于Gossip协议通常不需要显式的协调，因此它们在具有灵活的成员关系(节点频繁加入和离开)或网状网络(mesh network)的系统中很有用**。

​	Gossip协议非常健壮，它能帮助我们在分布式系统存在固有故障的情况下实现高可靠性。<u>由于**消息是以随机方式中继**的，因此即使节点之间的某些通信组件发生故障，消息仍可以通过其他路径传递。可以说这样的系统天然能够适应故障</u>。

### 12.6.1 Gossip技术细节

​	<u>进程定期随机选择f个对等节点并与它们交换当前的“热”信息，其中f是可配置的参数，称为扇出(fanout)</u>。每当进程从其他对等节点获悉新信息时，它会尝试将其传递到更多节点。由于对等节点的选择是概率性的，因此总会有一些重叠，消息会被重复传递，并且可能会继续流传一段时间。消息冗余性是一个衡量重复传递开销的度量指标。**冗余性是一种重要的属性，它对于Gossip至关重要**。

​	**系统达到收敛所需的时间称为*延迟***。达到收敛(停止Gossip过程)和将消息传递给所有对等节点这两个概念存在细微的差异，因为消息可能会在很短时间内就通知到所有对等节点，但Gossip仍在继续。<u>扇岀和延迟取决于系统规模：在更大规模的系统中，我们要么增加扇出以保持延迟的稳定，要么允许更高的延迟</u>。

​	一段时间后，随着节点注意到它们一次又一次地接收到相同的信息，消息将开始失去重要性，节点最终将会停止中继。兴趣损失可以概率性地计算(每个进程的每一步都会计算传播停止的概率)，也可以使用一个阈值(对接收到重复消息的次数进行计数，当次数过高时停止传播)。两种方法都必须考虑集群的规模和扇出。对重复消息进行计数以衡量收敛性可以改善延迟并减少冗余[DEMERS87]。

​	<u>在一致性方面，Gossip协议提供*收敛*一致性[BIRMANO7]：节点对更早发生的事件具有一致视图的可能性更高</u>。

### 12.6.2 覆盖网络

​	尽管Gossip协议很重要也很有用，但它们通常只适用于有限范围的问题。非传染性的方法可以以非概率性的确定性、较少的冗余和往往更优的方式来分发消息[BIRMANO7]。<u>Gossip算法广受赞誉的主要是它的扩展性，以及它可以在logN个消息回合内分发一条消息(其中N是集群的大小) [KREMARRECO7]，但务必记住**Gossip回合中也会产生大量的冗余消息**。**为了达到可靠性，基于 Gossip的协议会产生一些重复的消息传递**</u>。

​	随机选择节点可以大大提高系统的健壮性：如果存在网络分区，只要尚且存在间接连接两个进程的链路，消息最终将被传递。<u>该方法的明显缺点是它不是消息最优的：为了保证健壮性，我们必须维护对等节点之间的冗余连接和发送冗余消息</u>。

​	两种方法的折中是在Gossip系统中<u>构建一个临时的固定拓扑</u>。为此，我们可以创建对等节点间的覆盖网络(overlay network)：节点可以对它的对等节点进行采样，并根据接近程度(通常由延迟来衡量)选择最佳的联系点。

​	系统中的节点可以构成生成树(spanning tree)：不含重边的无向无环图，并覆盖整个网络。有了这个图，消息就可以按照固定数量的步骤分发。

​	<u>这种方法的潜在缺点之一是，它可能会导致形成对等节点的“岛”——这些节点之间互相连接，并且彼此之间具有较强的偏好</u>。

​	为了保持较少的消息数量，同时在连接断开时能够快速恢复，我们可以在系统处于稳定状态时混合使用两种方法(固定拓扑和基于树的广播)，而当故障切换和系统恢复时回退到Gossip。

### 12.6.3 混合Gossip

​	*推送/惰性推送多播树(Plumtree)* [LEITAO07]在传染和基于树的广播之间做了权衡。**Plumtree的工作原理是构建出节点的生成树覆盖网络，从而以最小的开销主动分发消息**。正常情况下，节点将完整的消息发送给一个很小的对等节点子集，该子集由对等节点采样服务提供。

​	<u>毎个节点将完整的消息发送给很小的节点子集，而对于其余节点，它只是惰性地(lazily)转发消息ID。如果节点接收到它从未见过的消息标识符，它可以查询对等节点以获取这条消息。这个惰性推送(lazy-push)步骤能确保高可靠性，并提供一种快速修复广播树的方法。故障发生时，协议通过惰性推送步骤回退到Gossip，广播消息并修复覆盖网络</u>。

​	由于分布式系统的固有性质，任何节点或节点之间的链接都随时可能发生故障，从而使这一段无法访问，并导致无法遍历树。惰性Gossip网络帮助节点将看到的消息通知给对等节点，从而构造以及修复树。

​	<u>使用惰性推送机制进行树的构建和修复的一个优点是：在负载恒定的网络中，由于最先响应的节点会被加入广播树中，因此**该算法倾向于生成一棵最小化消息延迟的树**</u>。

### 12.6.4 局部视图

​	向所有已知的对等节点广播消息、维护集群的完整视图可能会很昂贵甚至不切实际，在流失率(衡量加入和离开系统的节点数量)较高时更是如此。为了避免这一代价，Gossip协议常常使用对等节点采样服务。该服务维护集群的局部视图，该视图定期用Gossip刷新。局部视图之间互相重叠，因为Gossip协议需要一定程度的冗余，但是过多的冗余也意味着更多的工作负担。

​	例如，**混合局部视图(Hybrid Partial View，HyParView)协议**[LEITAO07]维护集群的**较小的活跃视图(active view)**和**较大的被动视图(passive view)**。<u>活跃视图中的节点构成一个可用于传播消息的覆盖图。被动视图则用于维护节点列表，用于替换活跃视图中的故障节点</u>。

​	每隔一段时间，节点进行一次洗牌(shuffle)操作，在此期间它们互相交换活跃和被动视图。交换期间，节点将它从对等节点收到的被动或活跃视图中的成员都添加到自己的被动视图中，并滚动删除最旧的值以限制列表大小。

​	活跃视图的更新则是根据视图中节点的状态变更以及对等节点的请求。如果进程P1怀疑P2发生了故障(P2是P1活跃视图中的一个对等节点)，则P1会从它的活跃视图中删除P2，并尝试与被动视图中的替代进程P3建立连接。如果连接失败，则从P1的被动视图中删除P3。

​	根据P1活跃视图中的进程数，如果P3的活跃视图已满，它可以选择拒绝连接。如果P1的视图为空，则P3必须将其当前的活跃视图中的某一个对等节点替换成P1。<u>这能加快节点的启动或恢复过程，使其快速成为集群中的有效成员，但也可能会导致一些环状连接</u>。

​	该方法中，节点仅向活跃视图中的对等节点分发消息，因此减少了系统中的消息数量；同时，该方法使用被动视图实现恢复机制，从而保证了高可靠性。衡量算法性能和质量的一个途径是：当拓扑重新组织时，对等节点采样服务能以多快的速度收敛到一个稳定覆盖[JELASITY04]。HyParView在这项上的得分很高，这要归功于它的视图的维护方式，以及给启动过程赋予更高的优先级。

​	**<u>HyParView和Plumtree使用混合Gossip方法：使用一小部分对等节点来广播消息，但是在发生故障或网络分区时回退到更宽的对等网络形式</u>**。两种系统都不依赖所有节点的全局视图，这是很有用的特性——不仅因为系统中的节点数量很多(多数情况下并非这种情况)，也是因为<u>维护每个节点上的全局成员列表代价很高</u>。<u>局部视图允许节点仅与相邻节点的一个较小子集保持活动连接</u>。

## 12.7 本章小结

​	最终一致系统允许副本状态存在不一致。可调一致性使得我们可以牺牲一致性来换取可用性，或者反之。副本不一致可以通过下面反熵机制中的一种来解决：

+ 提示移交

  如果目标节点宕机，则将写入临时存储在临近的节点上，并在目标恢复后立即重放这些变更。

+ 读修复

  在读取期间比较各个响应，并检测缺失的记录，随后将其发给滞后副本，从而使被请求的数据范围恢复一致状态。

+ Merkle树

  通过计算和交换哈希树来检测需要修复的数据范围。

+ 位图版本向量

  维护有关最新写入信息的紧凑记录，利用它检测丢失的副本写入。

​	**上述反熵方法针对三个维度之一做了优化：缩小范围、最后更新时间或数据完整性**。

+ 为了减小反熵的范围，我们可以只同步正在被査询的数据(读修复)或个别缺失的写入(提示移交)。
+ 如果假设大多数故障都是暂时的并且参与者可以很快从中恢复，那么我们可以保存最近发生分歧的事件日志，从而准确地找出需要同步的数据(位图版本向量)。
+ 如果我们需要成对地比较多个节点上的完整数据集，并高效地找出它们之间的差异，则可以对数据进行哈希并比较哈希值(Merkle树)。

​	为了在大规模系统中可靠地分发信息，可以使用Gossip协议。混合Gossip协议减少了消息交换的数量，同时尽可能保持了容忍网络分区的能力。

​	**许多现代系统都使用Gossip来检测故障以及维护成员信息**[DECANDIA07]。 HyParView被用在Partisan中，这是一个高性能、高扩展性的分布式计算框架。Plumtree被用在Riak Core中，用于传递集群范围的信息。

# 13. 分布式事务

P224