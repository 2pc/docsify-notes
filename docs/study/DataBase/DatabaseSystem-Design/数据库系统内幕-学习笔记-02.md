# 第二部分 分布式系统

​	没有分布式系统，我们将无法拨打电话、转账或远距离交换信息。我们每天都在使用分布式系统。有时候，即使没有明确说明，任何客户端/服务器架构的应用程序其实都是分布式系统。

​	对于许多现代软件系统，垂直扩展(将软件运行在更大更快的机器上，配备更多的CPU、RAM或更快的磁盘)是不现实的。更大的机器也更贵，更难以置换，而且可能需要特殊的维护。一个替代选项是<u>水平扩展：将软件运行在多个用网络相连的机器上，而在逻辑上视为单个实体</u>。

​	分布式系统有各种规模，少则几台机器，多则上百台机器。系统参与者的特性也各不相同，可以是手持式或传感器设备，也可能是高性能计算机。

​	数据库系统运行在单个节点上的时代已经过去很久了，大多数现代数据库系统拥有多个以集群方式相互连接的节点，以增加存储容量、提升性能和增强可用性。

​	尽管某些分布式计算的理论突破不是新生事物，但大部分的实际应用还是在近期才出现的。今天，这一主题越来越受到人们的关注，我们也看到更多的相关研究与新的发展正在进行。

---

+ 第二部分基本定义

  ​	分布式系统有若干个参与者(participant，有时也称为进程、节点或副本)，每个参与者都有其自己的本地状态。参与者通过在通信链路上交换消息来进行彼此间的通信。

  ​	进程可以用时钟来获取时间，它可以是逻辑的，也可以是物理的。<u>逻辑时钟是用单调递增的计数器实现的。物理时钟也被称为挂钟(wall clock)，它与物理世界的时间概念紧密相连，可以通过进程本地的方式(例如通过操作系统)获取到</u>。

  ​	说到分布式系统就不得不提到一点：系统中的各个部分彼此分开放置，这本身就带来了很大的困难。远程进程间的通信链路可能既慢又不可靠，这导致确定远程进程的状态变得更复杂。

  ​	**分布式系统领域中的大多数研究都与"没有什么是完全可靠的"这一事实有关：通信信道可能会延迟、乱序甚至无法传递消息；进程可能会暂停、变慢、崩溃、失控或突然停止响应**。

  ​	并行编程和分布式编程领域中有许多共同主题，因为CPU也可以看作一个具备通信链路、进程和通信协议的微型的分布式系统。你将在11.5节中看到分布式编程和并发编程的许多相似之处。但是多数的原语无法在二者之间直接复用，这是因为远程通信的成本要大得多，并且通信链路和进程是不可靠的。

  ​	为了克服分布式环境的困难，我们需要用到一类特殊的算法——分布式算法，它定义了本地和远程的状态以及执行的概念，即便在不稳定的网络或发生组件故障的情况下也能工作。为了解释这些算法，我们会用到术语状态和步骤(或阶段， phase)，并描述它们之间的转换(transition)。毎个进程都在本地执行算法步骤，而本地执行以及进程之间的交互构成了分布式算法。

  ​	分布式算法描述了本地的行为和多个独立节点的交互过程。节点通过相互发送消息进行通信。算法定义了参与者的角色、要交换的消息、状态、转换、执行步骤、传递介质的特性、时序假设、故障模型，以及其他描述进程和进程间交互的特性。

  ​	分布式算法有很多不同的用途:

  + **协调**

    监督若干工作者的动作和行为的进程。

  + **合作**

    多个参与者互相依靠，共同完成任务。

  + **分发**

    进程相互配合，将信息快速而可靠地分发给感兴趣的进程。

  + **共识**

    在多个进程间达成共识。

  ​	本书中，我们将站在使用的角度讨论算法，相比纯学术的材料，我们更倾向于实用的方法。首先，我们会介绍所有必要的抽象，包括进程和它们间的联系，并逐步构建出更复杂的通信模式。我们将会从UDP开始：使用UDP时，发送者无法确认它的消息是否已送达目的地。最终，在系统中达成共识，即多个进程都认同某个特定值。

# 8. 简介与概述

## 8.1 并发执行

​	一旦两个执行线程都能访问变量，除非我们在线程间同步这些步骤，否则这些并发步骤的执行结果是无法预知的。

​	即便仅在单个节点上，我们就已经遇到了分布式系统中的第一个问题：并发。每个并发程序都具有分布式系统的某些特性。线程访问共享状态，在本地执行一些运算，再将结果传回共享变量。

​	为了精确定义执行历史并减少可能的结果数量，我们需要一致性模型。一致性模型描述并发执行的过程，并且确定了运算执行以及对其他参与者可见的顺序。使用不同的一致性模型，我们可以约束或放松系统可能的状态数量。

​	分布式系统和并发计算在术语和学术研究上有许多重叠之处，但也存在一些差异。并发系统中存在共享内存，进程可以用它来交换信息。在分布式系统中，各个进程拥有自己的本地状态，参与者之间通过传递消息进行通信。

> **并发与并行**
>
> ​	我们经常互换使用并发和并行计算这两个术语，但是这两个概念在语义上有细微的差异。当两个步骤序列并发执行时，二者都在进行中，但任意时刻都只有其中一个在执行。当两个步骤序列并行执行时，它们的步骤可以(在某一时刻)同时执行。并发的操作时间上存在重叠，而并行的操作由多个处理器执行[WEIKUM01]
>
> ​	Erlang编程语言的创建者 Joe Armstrong举过一个例子：并发执行就像一台咖啡机前排了两队，而并行执行就像两台咖啡机前排了两队。即便如此，绝大部分资料都用术语"并发"来描述拥有多个并行执行线程的系统，而"并行"这个词则很少见。

---

+ 分布式系统中的共享状态

  ​	我们可以尝试在分布式系统中引入共享内存的概念，例如，单一信息源(比如数据库)。

  ​	即使我们解决了并发访问的问题，我们依然无法保证所有进程都是同步的。

  ​	为了访问数据库，进程需要通过通信介质发送和接受消息，以查询或修改状态。但是，如果一个进程很久都没有从数据库得到响应会如何？为了回答这个问题，我们首先要定义什么是很久。为此，必须<u>从同步性的角度来描述系统：通信是否是全异步的？是否存在某些时序假设？如果存在的话，这些时序假设将允许我们引入操作超时和重试机制</u>。

  ​	我们无从知晓数据库没有响应是因为过载、不可用、响应太慢还是网络问题。这描述了崩溃的本质——进程可能以各种方式崩溃：可能因某种原因无法继续执行后面的算法步骤；可能遇到了临时性的故障；也可能是消息丟失。我们需要定义一个**故障模型**并描述故障可能发生的方式，然后再决定如何处理它们。

  ​	<u>如果系统在故障发生时仍然能继续正常运行，我们将这样的特性称为**容错性**</u>。故障是不可避免的，所以我们需要构建出具有可靠组件的系统。消除单点故障，比如前文提到的单节点数据库，可能是我们朝此方向迈出的第一步。我们可以引入一些冗余，增设备份数据库。然而这就引出了另一个问题：如何使共享状态的多个副本保持同步?

  ​	到目前为止，在我们这个简单系统中引入共享状态所带来的问题比答案还多。现在我们知道，共享状态不像引入数据库那样简单，还必须采取更细化的方法，即基于消息传递来描述各个独立进程之间的交互。

## 8.2 分布式计算的误区

​	理想情况下，当两台计算机在网络上通信时，一切都能正常工作：进程开启一个连接、发送数据、收到响应，每个人都很开心。但是假设所有操作总会成功并且没有任何错误是很危险的，因为当某些东西出问题时，我们的假设也就不成立了，那时系统的行为将变得难以预测。

​	大多数时候，假设网络可靠是合理的。网络至少在一定程度上可靠才能有用。我们都曾经历过这样的情况，当我们尝试连接到远程服务器时，却收到了一个"网络不可达"的错误。即使能建立连接，一个成功的初始连接也无法保证这条链路是稳定的，连接随时可能中断。消息可能送达了对端，但对端的响应却可能丢失了，也有可能在对端的响应发送之前连接就中断了。

​	网络交换机会有故障，电缆可能断开，网络配置也随时可能发生变化。我们构建系统时需要适当地处理所有这些情况。

​	连接可以是稳定的，但我们不能期望远程调用能像本地调用一样快。我们应尽可能少地对延迟做出假设，并且永远不要假设延迟为零。一条消息要想到达远程服务器，需要穿过若干个软件层和一个物理媒介(比如光纤或电缆)，所有这些操作都不是瞬间完成的。

​	Michael Lewis在他所著的书Flash Boys(Simon and Schuster公司出版)中讲述了这样个故事，公司花费数百万美元把延迟降低几毫秒，从而能比竞争对手更快地访问交易所。这是一个把延迟作为竞争优势的绝佳例子，然而值得一提的是，根据其他一些研究，比如文献[BARTLETTI6]，过时报价套利(通过比竞争对手更快地得知价格并执行交易来获取利润)并不能使快速交易者从市场中获利。

​	从上述教训当中学习，我们增加了重试和重连机制，并去掉了关于瞬间执行的假设，但是事实证明这还不够：**当我们增加消息的数量、发送速率和大小，并向现有网络中添加新的进程时，我们不应该假设带宽是无限的**。

> 1994年， Peter Deutsch发布过一个如今很有名的断言列表，标题为"分布式计算的误区"，描述了分布式计算中易被忽视的一些方面。除了网络可靠性、延迟和带宽假设，它还提到了其他问题，比如，网络的安全性、可能存在的攻击者、有意或无意的拓扑变化都可能打破我们的一些假设，这些假设包括：某一资源存在性和所在位置，网络传输所消耗的时间和资源，以及最后——存在一个拥有整个网络的知识和控制权的权威个体。

​	Deutsch的列表可以说非常详尽，但它侧重于通过链路传递消息时可能出错的地方。这些担忧是合理的，而且描述了最通用、最底层的复杂性，但不幸的是，在设计和实现分布式系统时，我们还做出了很多其他假设，这些假设也可能在运行中导致问题。

### 8.2.1 处理

​	在远程进程响应刚刚收到的消息之前，它还需要在本地执行一些工作，因此我们不能假定处理是瞬时完成的。只考虑网络延迟还不够，因为远程进程执行的操作也不是立即完成的。

​	此外，我们还无法保证消息送达后会立刻被处理。消息可能会进入远程服务器的等待队列中，等到所有更早到达的消息处理完后才被处理。

​	节点可能相距很近，也可能很远，各节点可能有不同的CPU、内存和磁盘配置，可能运行不同的软件版本和配置。我们不能期望它们以相同的速度处理请求。<u>如果完成一项任务需要等待几个并行工作的远程服务器响应，则**整个执行的完成时间取决于最慢的服务器**</u>。

​	与普遍存在的看法相反，队列容量并非是无限的，堆积更多的请求不会对系统有任何好处。当生产者产生消息的速度大于消费者能够处理的速度时，我们可以使用**背压(backpressure)**策略减慢生产者的速度。背压是分布式系统中人们了解和应用最少的概念之一，通常是事后才建立，而不是将其视为系统设计必需的一个组成部分。

​	尽管增加队列容量听起来像是个好主意——可以帮助我们管道化、并行化以及有效地调度请求，但是，如果消息仅仅是停在队列中等待处理，什么也不会发生。**增大队列大小可能对延迟产生负面影响，因为这并不会改善处理速度**。

​	通常，进程本地队列是用于实现以下目标：

+ **解耦**

  使接收和处理在时间上分开，并各自独立发生。

+ **流水线化**

  不同阶段的请求由系统中独立的部分处理。负责接收消息的子系统不用阻塞到上条消息处理完成。

+ **吸收瞬时突发流量**

  系统负载可能经常变化，但是请求到达的间隔时间对负责处理请求的组件是隐藏的。总体的系统延迟会由于排队而增加，但这通常仍比响应失败并重试请求更好。

​	队列大小取决于工作负载和应用程序。对于相对稳定的工作负载，我们可以通过测量任务处理时间以及各任务的平均排队时间来确定队列大小，从而确保在提升吞吐量的同时，延迟仍保持在可接受的范围内。在这种情况下，队列大小相对较小。对于不可预测的工作负载，可能会出现任务提交的突发流量，这时队列大小也应当考虑突发流量和高负载。

​	即使远程服务器可以快速地处理请求，也并不意味着我们总是能获得正面的响应。它也可能回应一个失败：无法进行写操作、要查找的值不存在或是触发了bug。总之，即使是最顺利的情况也需要我们的关注。

### 8.2.2 时钟和时间

​	假设不同的远程计算机上的时钟都同步也很危险。再加上延迟为零以及处理是瞬时的这些假设，将会导致不同的特质，尤其是在时序和实时数据处理中。例如，当从时间感知不同的参与者收集和聚合数据时，你必须了解它们之间的时间漂移并相应地对时间进行归一化，而不是依赖源时间戳。除非使用特殊的髙精度时间源，否则不能依赖时间戳进行同步或排序。当然，这并不意味着我们完全不能或不该依赖时间：说到底，<u>任何同步系统都依靠本地时钟实现超时</u>。

​	**我们必须始终注意进程之间可能存在的时间误差，以及传递和处理消息所需的时间**。例如，Spanner(参见13.5节)使用特殊的时间API，该API返回时间戳和不确定性界限以施加严格的事务顺序。一<u>些故障检测算法依赖于共享的时间概念，要求时钟漂移始终在允许的范围内才能确保正确性</u>[GUPTA01]。

​	除了分布式系统中的时钟同步非常困难之外，当前时间也在不断变化：你可以从操作系统请求当前的POSIX时间戳，并在执行几个步骤后请求另一个当前时间戳，两次结果是不同的。尽管这是一个明显的现象，但是了解时间的来源以及时间戳捕获的确切时刻至关重要。

​	了解时钟源是否是单调的(即永远不会后退)，以及与调度时间相关的操作可能偏移多少，可能也会有所帮助。

### 8.2.3 状态一致性

​	之前说到的假设大多属于"几乎总是错的"一类，但是，还有一些假设最好归入"并非总是对的"一类。这类假设帮助我们走思维捷径，通过以特定方式思考来简化模型，忽略某些棘手的边缘情形。

​	分布式算法并不总是保证状态严格一致。<u>一些方法具有较宽松的约束，允许各副本之间的状态存在分歧，并依赖冲突解决(检测和解决系统内分歧状态的能力)和读取时数据修复(读取期间，当各副本响应不同结果时，使副本恢复同步的能力)</u>。有关这些概念的更多信息参见第12章。假定状态在节点间完全一致可能会导致难以察觉的bug。

​	最终一致的分布式数据库可能具有这样的逻辑：读取时通过查询Quorum的节点来处理副本不一致，但是假定数据库表结构和集群视图是强一致的。除非我们确保这些信息的一致性，否则依赖该假设可能会造成严重的后果。

​	例如， Apache Cassandra曾有一个bug，其原因是表结构变更在不同时刻传播到各个服务器。如果在表结构传播过程中尝试从数据库读取数据，则可能会读到损坏的数据，因为一台服务器以某种表结构进行编码，而另一台服务器使用不同的表结构对其进行解码。

​	另一个例子是由环的视图分歧引起的bug：如果一个节点假定另一个节点保存了某个键的数据记录，但另一个节点具有不同的集群视图，此时读写数据可能会导致数据记录被错误放置，或是获得一个空的响应，虽然数据实际上好端端地存放在另一个节点上。

​	即使完全的解决方案成本很高，我们也最好事先考虑各种可能的问题。通过了解和处理这些情况，你能以更自然的方式解决问题，比如内置防护措施或修改设计。

### 8.2.4 本地和远程执行

​	将复杂性隐藏在API内部可能很危险。例如，对于本地数据集上的一个迭代器，即使你对存储引擎不熟悉，也可以合理地推测内部行为。理解远程数据集上的迭代过程则是个完全不同的问题：你需要理解一致性、传递语义、数据协调、分页、合并、并发访问含义以及许多其他事情。

​	简单地将两者隐藏在同一个接口后，即便有用，也可能会产生误导。调试、配置和可观察性可能需要额外的API参数。我们应该始终牢记，**本地执行和远程执行是不同的**[ WALDO96]。

​	**隐藏远程调用最明显的问题是延迟：远程调用的成本比本地调用高很多倍，因为它涉及<u>双向网络传输、序列化/反序列化</u>以及许多其他步骤**。交错使用本地调用和阻塞的远程调用可能会导致性能下降和预期之外的副作用[VINOSKIO08]。

### 8.2.5 处理故障的需要

​	刚开始构建系统的时候，我们可以假设所有节点都可以正常工作，但如果总是这么想就很危险了。在长时间运行的系统中，节点可能会关机维护(通常会有个优雅关闭的过程)或因为种种原因(例如软件问题、内存耗尽(out-of-memory killer [KERRISK10])、运行时bug、硬件问题等)而崩溃。<u>进程会发生故障，而你能做的最好的事情就是做好准备并知道如何处理它们</u>。

​	如果远程服务器没有响应，我们并不总是知道确切的原因。这可能是由系统崩溃、网络故障、远程进程或中间链路太慢导致的。<u>一些分布式算法使用**心跳协议**和**故障检测**机制来确定哪些参与者还活着且可达</u>。

### 8.2.6 网络分区和部分故障

​	<u>当两个或更多服务器无法相互通信时，我们称这种情况为**网络分区**</u>。Seth Gilbert和Nancy Lynch在 Perspectives on the CAP Theorem[GILBERT12]中区分了以下两种情况：两个参与者无法相互通信；几组参与者彼此隔开，无法交换消息并继续运行算法。

​	网络的总体不可靠性(数据包丢失、重传、延迟难以预测)令人烦恼但尚可容忍，而网络分区则会造成更多的麻烦，因为各个独立的分组可以继续执行并产生冲突的结果。**网络链路的故障也可能是不对称的：消息仍然能从一个进程传递到另一个进程，反之则不行**。

​	为了构建在一个或多个进程出现故障的情况下仍健壮的系统，我们必须考虑**部分故障**的情况[TANENBAUM06]，如何让系统在部分不可用或运行不正常的情况下仍能继续工作。

​	故障很难检测，并且在系统的不同部分看来，不总是以相同的方式可见。设计高可用性系统时，我们应该始终考虑边缘情形：如果我们确实复制了数据却没有收到确认该怎么办？要重试吗？在发送了确认的节点上，数据仍可用于读取吗？

​	**墨菲定律告诉我们故障一定会发生**。编程界又补充道，故障将以最坏的方式发生。因此，作为分布式系统工程师，我们的工作是尽可能减少可能出现错误的场景，并为故障做好准备——包括这些故障可能导致的破坏。

​	避免一切故障是不可能的，但我们仍可以构建一个弹性的系统，使之在故障出现时仍然能正常运行。**<u>设计应对故障的最佳方式是进行故障测试</u>**。我们无法考虑清楚毎种可能的故障场景，并预测多个进程的行为。最好的解决方法就是通过测试工具来制造网络分区、模拟比特位腐烂[GRAY05]、增加延迟、使时钟发生偏移以及放大相对处理速度。现实世界中分布式系统的设置可能是对抗性的、不友好的，甚至是“有创造性的”(然而以非常敌对的方式)，因此测试工作应当尝试覆盖尽可能多的场景。

> 过去几年中出现了一些开源项目，它们能帮助我们构造出各种故障场景。
>
> + Toxiproxy用于<u>模拟网络问题</u>：<u>限制带宽、引入延迟、超时</u>等。
> +  Chaos Monkey的方法更为激进，它通过<u>随机关闭服务使工程师直面生产环境故障的风险</u>。
> + CharybdeFS<u>模拟文件系统及硬件错误与故障</u>。你可以用这些工具来测试软件以确保在这些故障出现时软件仍能正确工作。 
>
> + CrashMonkey是一个与文件系统无关的记录-重放-测试框架，用于测试持久性文件的数据及元数据一致性。

​	设计分布式系统时，我们必须认真考虑容错性、弹性，以及可能的故障场景和边缘情形。类似于"足够多的眼睛，就可让所有问题浮现"，我们可以说足够大的集群最终定会命中所有可能的问题。与此同时，只要有足够多的测试，我们最终能够发现每个存在的问题。

### * 8.2.7 级联故障

​	我们做不到总是完全隔离故障：**被高负载压垮的进程会增加集群其余部分的负载，从而使其他节点更有可能发生故障。级联故障能够从系统的一部分传播到另一部分，扩大了问题的范围**。

​	有时，级联故障甚至可能来源于完全善意的目的。例如，某个节点离线了一段时间，因而没有接收到最近的更新。当它恢复在线时，乐于助人的其他节点希望帮助它追赶上最近的变化，于是开始向它发送缺失的数据，而这又导致网络资源耗尽，或是导致该节点启动后短时间内再次发生故障。

> ​	为了防止系统的故障扩散并妥善处理故障场景，我们可以使用**断路器(或熔断机制)**。在电气工程中，断路器可通过中断电流来保护昂贵且难以更换的部件，使其免受电流过载或短路的影响。在软件开发中，熔断机制会监视故障，并使用**回退(fallback)**机制保护整个系统：避免使用出故障的服务，给它一些时间进行恢复，并妥善处理失败的调用。

​	<u>当与某一台服务器的连接失败或服务器没有响应时，客户端将开始循环重连。那时候，过载的服务器已经难以应付新的连接请求，因而客户端的循环重试也无济于事。为了避免这一情况，我们可以使用**退避(backoff)**策略，客户端不要立即重试，而是等待一段时间</u>。

​	**退避通过合理安排重试、增加后续请求之间的时间窗口来避免问题扩大**。

​	退避用于增加单个客户端的请求间隔。但是，<u>使用相同退避策略的多个客户端也会产生大量负载</u>。为了防止多个客户端在退避期之后同时重试，我们可以引入**抖动(Jitter)**。<u>抖动在退避上增加了一个小的随机时间间隔，从而降低了多个客户端同时醒来并重试的可能性</u>。

​	<u>硬件故障、比特位腐烂和软件错误都会导致数据损坏，而损坏的数据会通过标准的传递机制传播</u>。如果没有适当的验证机制，系统可能将损坏的数据传播到其他节点，甚至可能覆盖未损坏的数据记录。为了避免这一情况，我们应该采用**校验和(checksum)**以及验证机制，来验证节点之间交换的任何内容的完整性。

​	**通过计划和协调执行可以避免过载和热点问题。<u>相比于让各个对等节点独立执行操作步骤，我们可以用协调器来依据可用资源准备一份执行计划，并根据过去的执行数据来预测负载</u>**。

​	总之，我们应该始终考虑这样的情形：系统某一部分的故障可能导致其他地方也出现问题。我们应该为系统装备上**熔断**、**退避**、**验证**和**协调**机制。<u>处理被隔离的小问题总比从大规模故障中恢复更简单</u>。

​	我们用整整一节讨论了分布式系统中的问题和潜在的故障场景，但是我们应当将其视为警告，而不是被它们吓跑。

​	了解什么会出问题，并仔细设计和测试我们的系统，可以让它更健壮、更具弹性。了解这些问题可以帮助你在开发过程中识别、发现潜在的问题根源，也能帮助你在生产环境中调试

## * 8.3 分布式系统抽象

​	讨论编程语言时，我们使用<u>通用术语</u>并用<u>函数</u>、<u>运算符</u>、<u>类</u>、<u>变量</u>和<u>指针</u>来定义我们的程序。通用的词汇可以帮助我们避免每次都为了描述某些东西而发明新词。我们的定义越精确、越没有歧异，听众也就越容易理解。

​	在开始学习算法之前，我们首先要了解分布式系统中的词汇：这些定义你会经常在演讲书籍和论文中遇到。

---

+ **链路**

  ​	网络是不可靠的：消息会丢失、延迟或被打乱。记住这一点之后，我们来尝试构建几种通信协议。我们从最不可靠的协议开始，确定它们可能处于的状态，然后找出可以为协议增加的东西使它提供更好的保证。

  **公平损失链路**

  ​	我们可以从两个进程开始，它们之间以**链路**相连。进程可以相互发送消息。任何通信介质都是不完美的，消息可能丢失或延迟。

  ​	看看我们能得到什么样的保证。消息M被发送之后(从发送方的角度来看)，它可能处于以下状态之一：

  + 还未送达进程B(但会在某个时间点送达)
  + 在途中丢失且不可恢复
  + 成功送达远程进程

  ​	注意，发送方没有任何方法确定消息是否已经送达。在分布式系统的术语中，这种链路称为**公平损失(fair-loss)**。这种链路具有以下属性：

  **公平损失**

  ​	如果发送方和接收方都是正确的，且发送方无限多次重复发送，则消息最终会被送达。

  **有限重复**

  ​	发送的消息不会被送达无限次。

  **不会无中生有**

  ​	链路不会自己生成消息。换句话说，它不会传递一个从未发送过的消息。

  ​	公平损失链路是一种很有用的抽象，它是构建具有更强保证的通信协议的基石。我们可以假设该链路不会在通信双方之间*系统性地*丢弃消息，也不会创建新消息。但与此同时，我们也不能完全依靠它。这可能让你想起了用户数据报协议(UDP)，UDP允许我们从一个进程发送消息到另一个进程，但在协议层面上不提供可靠的传输语义。

  **消息确认**

  ​	为了改善这一情况、更清晰地获得消息状态，我们可以引入**确认(acknowledgment)机制**：接收方通知发送方消息已送达。为此，我们需要双向通信信道，并增加一些措施以区分不同的消息，例如序列号——单调递增的唯一消息标识符。

  > ​	每个消息只要有唯一标识符就足够了。序列号只是唯一标识符的一种特殊情况，即使用计数器来获取标识符，从而实现唯一性。<u>当使用哈希算法来唯一地标识消息时，我们应当考虑可能的冲突，并确保能消除歧义</u>。

  ​	现在，进程A可以发送消息M(n)，其中n是单调递增的消息计数器。B收到消息后立即向A发送确认ACK(n)。

  ​	确认消息，就像原始消息一样，也有可能在途中丟失。消息可能处于的状态数会稍有变化。在A收到确认之前，该消息仍处于我们前面提到的三种状态之一，但是，一旦A收到确认，就可以确信该消息已送达B。

  **消息重传**

  ​	增加确认机制仍不足以保证通信协议完全可靠：发送的消息仍可能会丢失，远程进程也可能在确认之前发生故障。为了解决该问题并提供送达保证，我们可以尝试**重传**(retransmit)。重传是指发送方重试可能失败的操作。我们之所以说可能失败，是因为发送方并不能真的知道有没有失败，因为我们要讨论的链路<u>不使用确认机制</u>。

  ​	进程A发送消息M之后，它将等到超时T被触发，然后尝试再次发送同一条消息。假设进程之间的链路完好无损，进程间的网络分区不会无限持续下去，并且并非所有数据包都丢失，我们可以认为，从发送方的角度看，消息要么尚未送达进程B，要么已经成功送达。由于A一直在尝试发送消息，可以认为传输过程中不会发生不可恢复的消息丢失。

  ​	在分布式系统的术语中，这种抽象称为**顽固链路(stubborn link)**。之所以称为顽固，是因为发件人会<u>无限期地反复发送消息</u>，但是，由于这种抽象非常不切实际，因此我们需要将重试与确认结合起来。

  **重传的问题**

  ​	每当我们发送消息时，在收到远程进程的确认之前，我们无从得知消息的状态：可能已被处理，可能马上就要处理，也可能已经丟失，甚至可能在收到消息之前远程进程就崩溃了——上述的任意状态都是可能的。我们可以重试操作、再次发送消息，但这可能导致消息重复。<u>只有当我们要执行的操作是幂等时，处理重复消息才是安全的</u>。

  ​	<u>幂等(dempotent)的操作可以执行多次而产生相同的结果，且不会产生其他副作用</u>。例如，服务器关机操作可以是幂等的，第一次调用将发起关机，而所有后续调用都不会产生任何其他影响。

  ​	如果毎个操作都是幂等的，那我们可以少考虑一些传递语义，更多地依赖重传来实现容错，并以完全反应式的方式构建系统：为某些信号触发相应的操作，而不会引起预期之外的副作用。但是，操作不一定是幂等的，简单地假设它们幂等可能会导致集群范围的副作用。例如，向客户的信用卡收费不是幂等操作，绝对不可以重复收费多次。

  ​	在存在部分故障和网络分区的情况下，幂等性尤其重要，因为我们无法总是确定远程操作的确切状态——是成功还是失败，还是会马上被执行——我们只能等待更长的时间。<u>**保证毎个操作都是幂等的是不切实际的，因此我们需要在不改变实际操作语义的情况下，提供与幂等性等价的保证**。为此，我们可以使用去重来避免多次处理消息</u>。

  **消息顺序**

  ​	不可靠的网络给我们带来了两个问题：一是消息可能会乱序到达；二是由于重传某些消息可能会多次送达。我们已经引入了序列号，利用这些消息标识符我们可以在接收方确保先进先出(FIFO)的顺序。由于每条消息都有一个序列号，因此接收方可以跟踪下列信息：

  + n<sub>consecutive</sub>表示最大连续序列号：所有小于或等于该序列号的消息都已经收到，这些消息可以按顺序放到正确的位置上。
  + n<sub>processed</sub>表示最大已处理序列号：所有小于或等于该序列号的消息都已经按照原来的顺序被处理。此序列号可以用于去重。

  ​	如果收到的消息序列号不连续，接收方会将其放入重新排序缓冲区。例如，它在接收到序列号为3的消息后收到消息5，那我们就知道4还是缺失的，因此我们将5放在一旁，直到4到来，然后就能构造出原本的消息顺序。<u>由于通信构建在公平损失链路之上，可以认为n<sub>consecutive</sub>和n<sub>max_seen</sub>之间的消息最终一定会送达</u>。

  ​	接收方可以安全地丢弃收到的序列号小于等于n<sub>consecutive</sub>的消息，因为这些消息确定已经送达了。去重的工作原理是检查带有序列号n的消息是否已被处理(已被传给网络栈的更上层)，丢弃已处理的消息。

  ​	去重的工作原理是检査带有序列号n的消息是否已被处理(已被传给网络栈的更上层)，丢弃已处理的消息。

  ​	在分布式系统的术语中，这种类型的链路称为**完美链路**，它提供以下保证[CACHIN11]：

  **可靠传递**

  ​	正确的进程A发送一次到正确的进程B的每个消息**最终**都会被传递。

  **没有重复**

  ​	消息不会被传送多次。

  **不会无中生有**

  ​	与其他种类的链路一样，它只能传递实际由发送者发送过的消息。

  ​	这可能会让你想起TCP协议(但是，TCP仅在单个会话内保证可靠传递)。当然，上述模型仅仅是一种用于说明原理的简化表示。TCP中处理消息确认的模型更为复杂，它按组进行确认以减少协议层面的开销。另外，TCP具有选择性确认、流控、拥塞控制错误检测等很多其他功能，这些不在我们的讨论范围之内。

  **严格一次传递**

  ​	<u>关于是否可以做到严格一次传递(exactly-once delivery)这个问题已经有很多讨论。这里，语义和精确的措辞非常重要。**由于链路故障可能导致传递消息的第一次尝试无法成功，因此大多数实际的系统都采用至少一次传递(at-least-once delivery)，它确保了发送方将重试直到收到确认为止，否则就认为对方没有收到该消息**。还有一种传递语义是最多一次(at-most-once)：发送方仅仅发送消息而不期待得到任何确认</u>。

  ​	TCP协议的原理是将消息分成数据包，一个一个传输，然后在接收端将它们拼接到起。TCP可能会尝试重传某些数据包，并且可能有不止一次的传输会成功。由于TCP用序列号标记毎个数据包，即使某些数据包被发送多次，它也可以对其进行去重，确保接收方只会看到并处理一次该消息。<u>在TCP中，此保证仅对单个会话有效：如果消息被确认并处理，但是发送方在收到确认消息前连接就中断了，则应用程序并不知道此传递成功，取决于其逻辑，它可能会尝试再次发送消息</u>。

  ​	这意味着严格一次处理是个有趣的问题，因为重复的传送(或数据包传输)没有副作用，仅仅是链路尽力而为的产物。举个例子，<u>如果数据库节点仅接收到记录但还没将它持久化。在这种情况下传递已经完成了，但除非该记录可以被査到(换句话说，除非消息被传递并且处理了)，否则这次传递毫无用处</u>。

  ​	为了确保严格一次传递，各节点需要一个共同知识[HALPERN90]：每个节点都知道某件事，每个节点都知道其他所有节点也都知道这件事。<u>用简化的术语来说，**节点必须在记录状态上达成共识**：两个节点都认为该记录已经或者还未被持久化。正如本章之后会说的，这在**理论上是不可能的**，但在实践中，我们仍通过放宽协调的要求来使用这一概念</u>。

  ​	各种关于是否是严格一次发送的误解，大多是因为从不同协议和抽象层次上考虑该问题，以及对“传递”的不同定义。**要想建立可靠的链路，不可能不重复传送某些消息**。但是，<u>我们可以通过仅处理毎个消息一次并忽略重复消息，使得从发送方的角度来看是严格一次发送</u>。

  ​	现在，在建立了实现可靠通信的方法之后，我们可以继续前进，探寻实现分布式系统中进程间一致性和共识的方法。

## 8.4 两将军问题

> [【翻译】两军问题（Two Generals’ Problem） | 知研片语 (liuzhaocn.com)](https://www.liuzhaocn.com/?p=1235)

​	一个被广泛称为两将军问题的思想实验，是对分布式系统一致性的最著名的描述之一。

​	这个思想实验表明，**<u>如果链路可能发生故障并且通信是异步的，则不可能在通信的双方之间达成共识</u>**。尽管TCP具有完美链路的性质，但是务必记住：完美链路尽管被称为完美链路，并不能保证完美的传递。它们也不能保证参与方一直活着，而只关心传输本身。

​	想象现在有两支军队，分别由两位将军领导，准备进攻一座要塞城市。两支军队分别位于城市的两侧，只有在同时进攻的情况下才能获胜。

​	两位将军通过信使进行通信。他们已经制定了攻击计划，现在唯一需要达成共识的就是是否执行计划。该问题的变体包括：其中一位将军的级别较高，但需要确保攻击是有协调的；或者两位将军需要就确切时间达成共识。这些细节不会改变问题的定义：将军们需要达成一项共识。

​	将军们只需要对“他们都会发起进攻”这一事实达成共识。否则，攻击将无法成功。将军A发出一条消息MSG(N)，表明如果对方也同意的话，就在指定的时间发起进攻。

​	将军A送出信使之后，他不知道信使是否已经到达：信使可能会被抓而无法传达消息。当将军B收到消息时，他必须发送确认ACK(MSG(N))。一条消息由一方发送并由另一方确认。

​	传递确认消息的信使也可能会被抓而无法传达消息。B无从得知信使是否已成功送达确认消息。

​	为了确认这一点，B必须等待ACK(ACK(MSG(N)))，一个二阶的确认，用于确认A收到了确认。

​	**<u>无论将军们互相发送多少确认，他们始终距离安全地发起攻击还差一个ACK。将军们注定要怀疑最后一个确认消息是否已送达目的地</u>**。

​	注意我们没有做任何时序上的假设:将军间的通信是完全异步的。并没有一个上限约束将军必须在多长时间内做出回应。

## 8.5 FLP不可能定理

​	Fisher、Lynch和Paterson在论文中描述了一个著名的问题：FLP不可能问题[FISCHERI85] (FLP是作者姓氏的首字母)，论文讨论了一种共识形式：各进程启动时有一个初始值，并尝试就新值达成共识。算法完成后，所有正常进程上的新值必须相同。

​	如果网络完全可靠，很容易对特定值达成共识。但实际上，系统容易出现各式各样的故障，例如消息丟失、重复、阒络分区，以及进程缓慢或崩溃。

​	共识协议描述了这样一个系统：给定初始状态的多个进程，它将所有进程带入决定状态。一个正确的共识协议必须具备以下三个属性：

+ 一致性

  ​	协议达成的决定必须是一致的：每个进程都做出了决定且所有进程决定的值是相同的。否则我们就尚未达成共识。

+ 有效性

  ​	达成共识的值必须由某一个参与者提出，这意味着系统本身不能“提出”值。这也意味着这个值不是无关紧要(trivial)的：进程不能总是决定某个预定义的默认值。

+ 终止性

  ​	只有当所有进程都达到决定状态时，协议才算完成。

​	<u>文献[FISCHER85]假定处理过程是完全异步的，进程之间没有共享的时间概念。这样的系统中的算法不能基于超时，并且一个进程无法确定另一个进程是崩溃了还是仅仅运行太慢。论文表明，在这些假设下，不存在任何协议能保证在有限时间内达成共识。**完全异步的共识算法甚至无法容忍一个远程进程无通知地突然崩溃**</u>。

​	**<u>如果我们不给进程完成算法步骤设定一个时间上限，那么就无法可靠地检测出进程故障，也不存在确定性的共识算法</u>**。

​	但是，FLP不可能定理并不意味着我们要收拾东西回家(由于达成共识是不可能的)。**<u>它仅仅意味着我们不能总是在有限的时间内在一个异步系统中达成共识</u>**。实践中，系统至少会表现出一定程度的同步性，而要想解决共识问题还需要一个更完善的模型。

## 8.6 系统同步性

​	从FLP不可能定理中可以看出<u>**时序假设**是分布式系统的关键特征之一</u>。在异步系统中，我们不知道进程运行的相对速度，也不能保证在有限时间内或以特定顺序传递消息。进程可能要花无限长的时间来响应，而且无法总是可靠地检测到进程故障。

​	对异步系统的主要批评在于上述假设不切实际：进程不可能具有任意不同的处理速度，链路传递消息的时间也不会无限长。依赖时间能够简化推理，并提供时间上限的保证。

​	在异步模型中不一定能解决共识问题[FISCHER85]。而且，不一定能设计出高效的异步算法。<u>对于某些任务，切实可行的解决方案很可能需要依赖时间</u>[ ARJOMANDIS83]。

​	我们可以放宽一些假设，认为系统是同步的。为此我们引入了时间的概念。在同步模型下对系统进行推理要容易得多。它假定各进程的处理速度相近、传输延迟是有限的，并且消息传递不会花任意长的时间。

​	同步系统也可以表示为同步的进程本地时钟：两个进程本地时间源之间的时间差存在上限[CACHIN11]。

​	**在同步模型中设计系统可以使用超时机制**。我们可以构建更复杂的抽象，例如**领导者选举、共识、故障检测**以及基于它们的其他抽象。<u>这使得最佳情况的场景更加健壮，但是如果时序假设不成立则可能导致故障</u>。

​	例如：Raft共识算法(参见14.4节)中，可能最终有多个进程认为它们是领导者，为了解决该问题，我们强制滞后的进程接受其他进程成为领导者；故障检测算法(参见第9章)。可能会错误地将活动进程标记为故障，反之亦然。设计系统时，我们必须考虑这些可能性。

​	异步和同步模型的性质可以组合使用，我们可以将系统视为部分同步的。部分同步的系统具有同步系统的某些属性，但是消息传递、时钟漂移和相对处理速度的边界范围可能并不精确，并且仅在大多数时候成立[DWORK88]。

​	同步是分布式系统的基本属性：它对性能、扩展性和一般可解性有影响，并且有许多对系统正常工作来说是必要的因素。本书中讨论的一些算法就工作在同步系统的假设下。

## 8.7 故障模型

​	我们一直在提到故障这个词，但到目前为止，它还是一个十分宽泛的概念，可能包含多种含义。就像我们可以做出不同的时序假设那样，我们也可以假设存在不同种类的故障。故障模型准确地描述了分布式系统中的进程可能以怎样的方式崩溃，并基于这些假设来开发算法。例如，我们可以假设进程可能崩溃并且永远无法恢复，或者可以预期它将在一段时间后恢复，或者它可能会失控并且产生错误的值。

​	分布式系统中，进程互相依赖以共同执行算法，因此故障可能导致整个系统的执行错误。

​	我们将讨论分布式系统中现有的多种故障模型，例如崩溃、遗漏和任意故障。这个列表并非面面俱到，但它涵盖了在实际中的大多数重要场景。

### 8.7.1 崩溃故障

​	通常，我们期望进程正确执行算法的所有步骤。最简单的崩溃方式是进程停止执行接下来的算法步骤，并且不再发送任何消息给其他进程。换句话说，该进程崩溃了。大多数情况下，我们使用**崩溃-停止(crash-stop)**进程抽象的假设，它规定一旦进程崩溃就会保持这种状态。

​	该模型不假定该进程无法恢复，也不阻拦或试图阻止恢复。这仅仅意味着该算法的正确性或活动性不依赖于恢复过程。实际上，并没有什么东西会去阻止进程恢复、追上系统状态以及参与下一次的算法执行。

​	失败的进程无法再继续参与当前这一轮的协作。为恢复的进程分配一个新的、不同的ID不会使模型等价于崩溃-恢复模型(之后会讨论)，因为大多数算法使用预定义的进程列表，并且依据最多可容忍的故障数明确定义了故障的语义[CACHIN11]。

​	**崩溃-恢复(crash-recovery)**是另一种的进程抽象。在这个抽象中，进程停止执行算法步骤，但会在稍后恢复并尝试执行剩下的步骤。要想让恢复成为可能，需要在系统中引入持久状态以及恢复协议[SKEEN83]。允许崩溃-恢复的算法需要考虑所有可能的恢复状态，因为恢复的进程会尝试从最后一个已知的步骤开始继续执行。

​	想利用恢复的算法必须同时考虑状态和进程ID。在这种情况下，崩溃恢复也可以看作是遗漏故障的一种特殊情况，因为<u>从另一个进程的角度看，不可达的进程与崩溃再恢复的进程没什么区别</u>。

### 8.7.2 遗漏故障

​	另一个故障模式是**遗漏故障(omission fault)**。该模型假设故障进程跳过了某些算法步骤，或者无法执行这些步骤，或者执行过程对其他参与者不可见，或者无法与其他参与者通信。遗漏故障中包含了由于网络链路故障、交换机故障或网络拥塞而导致的网络分区。网络分区可以表示为单个进程或进程组之间的消息遗漏。进程崩溃可以模拟为遗漏所有该进程收发的消息。

​	如果进程的运行速度慢于其他参与者，发送响应比预期迟得多，那么对于系统的其余部分来说，这个节点看起来丢三落四的。慢节点没有完全停止，而是发送结果太慢，常常与其他节点不同步。

​	<u>如果本应执行某些步骤的算法跳过了这些步骤或者执行结果不可见时，就发生了遗漏故障</u>。例如，消息在送往接收方的途中丢失，而发送方就像消息发送成功时那样，没有再次发送而是继续运行，即使消息已经不可恢复地丟失了。遗漏故障也可能是由间歇性停顿、网络过载、队列满等引起的。

### 8.7.3 任意故障

​	最难以解决的故障种类是**任意故障**或**拜占庭故障(Byzantine fault)**：<u>进程继续执行算法步骤，但是以与违背算法的方式(例如，共识算法中的进程决定一个从未由任何参与者提出过的值)</u>。

​	此类故障可能是由于软件bug或运行不同版本算法的进程，在这种情况下，故障很容易被发现和理解。如果我们无法控制所有进程，并且其中一个进程有意地误导其他进程，则发现和理解故障会变得非常困难。

​	你可能在航空航天工业中听说过拜占庭式的容错：飞机和航天器的系统不会直接使用子部件传来的值，而是会对结果进行交叉验证。另一个广泛的应用是加密货币[GILAD17]，那里没有中央权威，节点被多方控制，并且敌对的参与者有强烈的动机通过提供错误响应来欺骗系统。

### 8.7.4 故障处理

​	我们可以通过构成进程组、在算法中引入冗余来掩盖故障：即使其中一个进程发生故障，用户也不会注意到[CHRISTIAN91]。

​	故障可能会带来一些性能损失：正常的执行依赖于进程可响应，而且系统必须回退到较慢的执行路径来处理故障和纠正错误。故障往往可以通过一些方式来避免，例如：代码审査、广泛的测试、引入超时重试机制确保消息送达，以及确保各算法步骤在本地按顺序执行。

​	我们这里介绍的大多数算法都基于崩溃-故障模型，并通过**引入冗余来解决故障**。这些假设帮助我们创造性能更好、更易于理解和实现的算法

## 8.8 本章小结

​	本章中，我们讨论了一些分布式系统的术语，并介绍了一些基本概念。我们讨论了分布式系统的固有困难和复杂性，这是由于系统组件不可靠性导致的：链路可能无法传递消息、进程可能崩溃、网络可能发生分区。

​	这些术语应该足够让我们继续讨论。本书的剩余部分将讨论分布式系统中常见的解决方案:我们将先回想下哪些地方可能会出问题，然后看看有哪些可用的选项。

# * 9. 故障检测

​	为了使系统对故障做出适当的反应，应该及时检测故障。其他进程可能联系发生错误的进程，即使它无法响应，这会增加延迟并降低整个系统的可用性。

​	在异步分布式系统中检测故障(即不做任何时序假设)是极其困难的，因为我们无法判断进程是崩溃了，还是运行缓慢而需要无限长的时间来响应。我们在8.5节中讨论了与此相关的问题。

​	诸如死亡(dead)、失效(failed)和崩溃(crashed)等术语通常用于描述完全停止执行其步骤的进程。而<u>诸如无响应(unresponsive)、有故障(faulty)和缓慢(slow)等术语用于描述可疑进程，这些进程可能实际上已经死亡</u>。

​	故障可能发生在链路层上(进程之间的消息丢失或传递缓慢)，或者发生在进程层上(进程崩溃或运行缓慢)，而缓慢可能不一定能与故障区分开来。这意味着在如下两个方面总是面临一个杈衡：

+ 将活着的进程错误地怀疑为死的(产生假阳性)
+ 推迟将无响应的进程标记为死的并期望它最终做出响应(产生假阴性)。

​	**故障检测器(failure detector)**是一个本地子系统，其负责识别故障或无法到达的进程，将它们排除在算法之外，并在维持安全性的同时保证算法的活动性。

​	<u>**活动性**和**安全性**是描述算法解决特定问题的能力及其输出正确性的属性</u>。

​	更正式地说，**活动性(liveness)是一种保证特定预期事件必须发生的属性**。例如，如果其中一个进程发生故障，则故障检测器必须检测到该故障。**安全性(safety)保证意外事件不会发生**，例如，如果一个故障检测器已经将一个进程标记为死亡，那么这个进程实际上必须是死亡的[LAMPORT77， RAYNAL99， FREILING11]。

​	从实际的角度来看，排除发生故障的进程有助于避免不必要的工作，防止错误传播和级联故障，而排除疑似故障的活动进程会降低系统可用性。

​	故障检测算法应该表现出几个基本特性。首先，每一个没有问题的成员最终都应该注意到进程故障，并且算法应该能够向前推进并最终得出结果。这种特性称为**完备性（completeness）**。

​	我们可以通过算法的效率来判断其优劣：故障检测器识别进程故障的速度有多快。另一种方法是观察算法的准确性：是否精确地检测到了进程故障。换句话说，如果一个算法错误地认为一个活着的进程发生了故障或者不能检测出实际已发生的故障，那么它就是不准确的。

​	我们可以把效率和准确度之间的关系看作是一个可调参数：一个更高效的算法可能更不准确，而一个更准确的算法通常更不髙效。**建立既准确又高效的故障检测器被证明是不可能的**。同时，<u>故障检测器是允许产生假阳性的(即，错误地将活着的进程识别为故障，或者反过来)</u>[CHANDRA96]。

​	故障检测器是许多共识和原子广播算法的必要前提和组成部分，我们将在本书后面讨论这些算法。

​	许多分布式系统使用**心跳(heartbeat)**来实现故障检测器。由于其简单性和很强的完备性，这种方法非常普遍。我们在这里讨论的算法<u>假设不存在拜占庭式故障：进程不会试图故意谎报它们自己及相邻进程的状态</u>。

## 9.1 心跳和ping

​	我们可以通过触发如下两个周期性过程之一来查询远程进程的状态：

+ 我们可以触发一个**ping**，它将消息发送到远程进程，通过在指定的时间段内是否得到响应来检查它们是否仍处于活动状态。
+ 我们可以触发一个**心跳**，即进程通过向其对等方发送消息来主动通知其仍在运行。

​	在这里我们将使用ping作为例子，使用心跳也可以解决相同的问题并产生相似的结果。

​	每个进程维护一个其他进程的列表(存活、死亡和疑似死亡)，并且用每个进程最新的响应时间对这个列表进行更新。如果一个进程在较长的时间内无法响应一个ping消息，它会被标记为疑似死亡(suspected)。

​	许多故障检测算法都是基于心跳和超时的。例如，用于构建分布式系统的流行框架Akka实现了一个截止时间故障检测器(deadline failure detector)，这一检测器使用心跳机制，如果进程在某一固定时间间隔内未能成功注册，它将报告进程故障。

​	<u>这种方法有几个潜在的缺点：它的精度依赖于对ping频率和超时的仔细挑选，并且它不能从其他进程的角度捕获进程的可见性(参见9.1.2节)</u>。

### * 9.1.1 无超时的故障检测器

​	一些算法避免依赖超时来检测故障。例如Heartbeat，一种**无超时(timeout-free)**故障检测器[AGUILERA97]，该算法仅对心跳计数并允许应用程序基于心跳计数器向量中的数据来检测进程故障。由于该算法是无超时的，因此它能在异步系统假设下运行。

​	该算法假设任意两个正确的进程用公平路径(fair path)相互连接，该路径只包含公平链路(即如果无限频繁地通过该链路发送一条消息，该消息也会无限频繁地被接收)，并且每个进程都能意识到网络中所有其他进程的存在。

​	每个进程维护一个邻居列表和与其相关联的计数器。首先，进程向邻居发送心跳消息，毎个消息都包含心跳到目前为止所经过的路径。初始消息包含路径中的第一个发件人和一个唯一标识符，该标识符可用于避免同一消息被广播多次。

​	当进程接收到新的心跳消息时，它会递增路径中所有参与者的计数器，将心跳信号发送到尚未参与的进程，并将其自身追加到路径中。进程一旦看到所有已知进程已经接收到消息(换句话说，进程ID出现在路径中)，就会停止传播该消息。

​	<u>由于消息是通过不同的进程传播的，并且心跳路径包含从相邻进程接收的聚合信息，因此即使两个进程之间的直接链路出现故障，我们也可以(正确地)将无法到达的进程标记为活动进程</u>。

​	心跳计数器表示系统的全局和归一化视图。这个视图捕获了心跳是如何在节点间传播的，让我们可以对进程进行比较。然而，<u>这种方法的一个缺点是，对心跳计数器进行解释可能相当棘手：**需要选择一个能够产生可靠结果的阈值**。除非我们能做到这一点，否则算法会错误地将活动进程标记为疑似死亡</u>。

### * 9.1.2 外包心跳

​	可扩展弱一致性感染式进程组成员协议(Scalable Weakly Consistent Infection-style Process Group Membership Protocol，SWIM)[GUPTA01使用的是另一种方法。它使用外包心跳(outsourced heartbeat)来提高可靠性，**<u>利用的是从其相邻进程的角度查看到的进程活动性(liveness)信息。这种方法不需要进程知道网络中的所有其他进程，只需要知道其连接的对等进程的子集</u>**。

​	进程P1向进程P2发送ping消息。P2不响应该消息，因此P1继续选择多个随机成员(P3和P4)发送ping消息。这些随机成员会尝试向P2发送心跳消息，如果P2响应，则将确认转发回P1。

​	这允许我们将直接和间接可达性都考虑在内。例如，如果有进程P1、P2和P3，我们可以同时从P1和P2的角度检查P3的状态。

​	通过将决策责任分布到成员组中，外包心跳可以做到可靠的故障检测。这种方法不需要向很多对等进程广播消息。由于外包心跳请求可以并行触发，这种方法可以快速收集更多关于疑似死亡进程的信息，进而让我们做出更准确的决策。

## * 9.2 phi增量故障检测器

> 某种意义上有点类似Google TCP的BBR拥塞控制算法。
>
> [谈TCP BBR拥塞控制算法_yang_oh的博客-CSDN博客_bbr拥塞控制算法](https://blog.csdn.net/u013032097/article/details/96212770)

​	phi增量(φ-accrual)故障检测器[HAYASHIBARA04]不是将节点故障视为二元判断问题(即进程只能处于两种状态：在线或宕机)，而是<u>用连续范围来捕获被监视进程崩溃的概率</u>。它的工作方式是**维护一个滑动窗口，从对等进程收集最近心跳的到达时间**。该信息用于估算下一个心跳的到达时间，将该近似值与实际到达时间进行比较，并计算可疑程度φ：代表在给定当前网络条件下，故障检测器对故障的置信度。

​	该算法的原理是：<u>收集和采样到达时间，创建出一个可用于对节点健康状况做出可靠判断的视图，然后使用这些釆样结果计算φ的值：如果该值达到阈值，则节点被标记为宕机。**通过调整标记节点为疑似死亡的阈值，这种故障检测器能够动态地适应变化的网络条件**</u>。

​	从架构的角度来看，phi增量故障检测器可以看作三个子系统的组合。

+ **监控**

  通过ping、心跳或请求-响应采样来收集进程存活信息。

+ **解释**

  决定是否将该进程标记为疑似死亡。

+ **行动**

  每当标记进程为疑似死亡时执行的回调。

​	监控进程将数据样本(假定是正态分布)收集并储存在心跳到达时间的固定大小窗口中。新到达的心跳被添加到窗口中，同时最早的心跳数据点被丢弃。

​	<u>通过确定样本的均值和方差，可以从采样窗口估算出分布参数。该信息用于计算在前个消息到达之后t个时间单位内消息到达的概率。基于这个信息我们能计算出φ，它描述了我们对一个进程活动性做出正确决定的可能性。换句话说，有多大的可能性犯错——接收到一个与计算出的假设相矛盾的心跳</u>。

​	这种方法是由日本高级科学技术研究所的研究人员开发的，现在已用于许多分布式系统中，例如，Cassandra和Akka(连同前面提到的截止时间故障检测器)。

## * 9.3 Gossip和故障检测

> 有点像OSPF动态路由协议。
>
> [动态路由协议_百度百科 (baidu.com)](https://baike.baidu.com/item/动态路由协议/2915884?fr=aladdin)

​	另一种避免依赖单节点视图做出决策的方法是Gossip式的故障检测服务[VANRENESSE98]它使用Gossip(参见12.6节)来收集和分发相邻进程的状态。

​	<u>每个成员维护一个其他成员的列表：它们的心跳计数器(heartbeat counter)和时间戳</u>。

​	<u>时间戳列出了心跳计数器上次递增的时间。每个成员定期递增其心跳计数器，并将其列表分发给随机相邻节点。在接收到消息时，相邻节点将列表与它自己的列表进行合并，更新其他相邻节点的心跳计数器</u>。

​	节点还定期检查状态列表和心跳计数器。如果任何节点在足够长的时间里没有更新其计数器，就认为它发生了故障。超时时间应当谨慎选择，以将误报的概率降至最低。<u>成员之间通信的频率(换句话说，最坏情况下所使用的带宽)是有上限的，并且最多可以随着系统中的进程数线性增长</u>。

​	通过这样的方式，我们就可以检测出崩溃的以及任何其他集群成员都无法访问的节点。这个决策是可靠的，因为集群的视图是来自多个节点的聚合。如果两台主机之间的链路出现故障，心跳仍然可以通过其他进程传播。**使用Gossip来传播系统状态增加了系统中的消息数量，但使得信息传播更可靠**。

## * 9.4 反向故障检测

​	由于并不总是能传播故障的信息，并且通过通知每个成员来进行传播可能成本较高，因此出现了一种称为FUSE(Failure Notification Service，故障通知服务)[DUNAGANO4]的方法，它专注于可靠且廉价的故障传播，即使在网络分区的情况下也能工作。

​	<u>为了检测进程故障，该方法将所有活动进程进行分组。**如果其中一组变得不可用，则所有参与者都能检测到该故障**。换句话说，每次检测到单个进程故障时，它被转换并传播为**组故障**</u>。它可以检测任何形式的网络中断、网络分区和节点故障。

​	<u>组中的进程定期向其他成员发送ping消息，以查询它们是否仍处于活动状态。**如果其中一个成员由于崩溃、网络分区或链路故障而无法响应此消息，则发出这个ping的成员本身将停止响应ping消息**</u>。

​	展示了四个通信进程：

+ 初始状态：所有进程都处于活动状态并可以通信。
+ P2崩溃并停止响应ping消息。
+ P4检测到P2的故障并停止响应自己收到的ping消息。
+ 最终，P1和P3注意到P4和P2都没有响应，将进程故障传播到整个组。

​	**所有故障都通过系统从故障源传播到所有其他参与者**。<u>参与者逐渐停止响应ping消息，将单个节点故障转换为组故障</u>。

​	在这里，我们**<u>利用不通信作为一种传播的手段</u>**。这种方法的一个优点是保证每个成员都能了解组的故障并对其做出充分的反应。它的一个缺点是：<u>将单个进程与其他进程分开的链路故障也可能会被转换为组故障，但这其实也可以被看作是一个优点，应由具体的用例所决定</u>。应用程序可以使用其自身对故障传播的定义来应对这种情况。

## 9.5 本章小结

​	故障检测器在任何分布式系统中都是一个重要的组成部分。正如FLP不可能定理所说的，**<u>异步系统中没有协议能够保证一致性</u>**。

​	<u>故障检测器有助于扩展模型，允许我们通过在**准确性**和**完备性**之间进行权衡来解决一致性问题</u>。文献[CHANDRA96]描述了这一领域的一个重要发现，证明了故障检测器是有用的，它表明，<u>**即使使用一个犯了无限多个错误的故障检测器，解决共识问题仍然是可能的**</u>。

​	我们介绍了几种故障检测算法，每种算法都使用不同的方法：一些专注于通过直接通信来检测故障，而一些则使用广播或Gossip来传播信息，还有一些使用沉默(换句话说，不再通信)作为传播手段。现在，我们知道可以使用心跳、ping、截止时间、连续范围等方法，毎一种方法都有自己的优点:简单性、准确性或精确性。

# * 10. 领导者选举

​	同步的代价可能会非常大：如果算法的每一个步骤都需要联系其他参与者，那么结果定是产生相当显著的通信开销。在大型且地理分布的网络中尤其如此。<u>为了减少同步开销和达成决定所需消息的往返次数，一些算法依赖于**领导者(有时称为协调者)进程**的存在，该进程负责执行或协调分布式算法的各个步骤</u>。

​	一般来说，分布式系统中的进程是对等的，任何进程都可以接管领导者的角色。进程可以长期担任领导者，但这不是一个永久的角色。通常情况下，进程可以一直担任领导者直到崩溃为止。崩溃后，任何其他进程都可以开始新一轮的选举，如果其当选，就可以担任领导者并继续执行上个领导者遗留的工作。

​	选举算法的**活动性(liveness)**保证了大多数时候会有一个领导者，选举最终会完成(即<u>系统不应该无限期地处于选举状态</u>)。

​	理想情况下，我们也希望获得安全性，保证一次最多只能有一个领导者，并完全消除<u>脑裂(两个目的相同的领导者被选举出来，但彼此不知情)</u>的可能性。然而在实践中，许多领导者选举算法违反了这一协定。

​	可以使用领导者进程来实现广播中消息的全序。领导者收集并保存全局状态，接收消息，并将消息分发给各个进程。它还可以用于协调发生在系统故障后、初始化期间或重要状态变更时的系统重组。

​	系统初始化时将会触发选举，第一次选择领导者。当上一个领导者崩溃或通信失败时也会触发选举。选举必须是确定性的：<u>选举过程中必须只产生一个领导者。这一决定需要对所有参与者都有效</u>。

​	<u>尽管领导者选举和分布式锁(即对共享资源的独占所有权)从理论角度看可能很相似，但它们略有不同。如果一个进程因为执行**临界区**而持有锁，那么对于其他进程来说，知道现在到底是谁持有锁并不重要，只要满足活动性(即锁最终将被释放并允许其他人获得它)就可以了。**相比之下，选出的进程具有一些特殊性质，必须让所有其他参与者都知道，因此新当选的领导者必须将其角色通知所有对等进程**</u>。

​	<u>如果分布式锁算法对某个进程或进程组有任何偏好，不被偏好的进程最终将产生对共享资源的饥饿，这与活动性相矛盾。相比之下，领导者可以保持领导的角色直到停止或崩溃，长期存活的领导者是更好的</u>。

​	**在系统中具有稳定的领导者有助于避免远程参与者之间的状态同步，减少交换消息的数量，并能通过单进程(而不是对等进程间的协调)来驱动执行**。

​	在具有领导权的系统中，一个潜在的问题是，**领导者可能会成为瓶颈**。<u>为了克服这种情况，许多系统将数据划分在不相交的独立副本集中(参见13.6节)。每个副本集都有自己的领导者，而不是在整个系统范围上使用一个领导者</u>。使用这种方法的系统之一是Spanner(参见13.5节)。

​	因为毎一个领导者进程最终都会发生故障，所以故障必须被检测、报告和处理：系统必须选择另一个领导者来替换故障领导者。

​	一些算法，例如ZAB(参见14.2.2节)、 Multi-Paxos(参见14.34节)或Raft(参见14.4节)，<u>使用**临时领导者**来减少参与者达成一致所需的消息数量</u>。然而，这些算法都使用算法特有的手段来进行领导者选举、故障检测以及解决竞争领导者的进程之间的冲突。

## 10.1 霸道选举算法

​	有一个被称为霸道选举算法(bully algorithm)的领导者选举算法，<u>它使用进程排名来认定新的领导者。每个进程都有一个唯一的排名。在选举过程中，排名最高的进程成为领导者[MOLINA82]</u>。

​	这种算法以其简单性而闻名。之所以被命名为霸道(bully)，是因为排名最高的节点"霸道"地强迫其他节点接受它。它有时也被称为君主领导者选举：在前一个君主不复存在后，排名最高的兄弟姐妹成为君主。

​	如果一个进程注意到系统中没有领导者(该系统从未被初始化)或以前的领导者已停止响应，则按如下三个步骤进行选举：

1. 进程将选举消息发送到具有较高标识符的进程。
2. 进程等待较高排名的进程进行响应。如果没有排名更高的进程响应，则继续执行步骤3。否则，进程通知它所了解到的排名最高的进程，让它继续执行步骤3。
3. 进程假定没有排名更高的活动进程了，因此将新的领导者通知给所有排名更低的进程。

​	霸道领导者选举算法：

+ a）进程3注意到前一个领导者6已崩溃，于是向具有更高标识符的进程发送选举消息来开始新的选举。
+ b）4和5回应Alive(存活)消息，因为它们具有比3更高的排名。
+ c）3通知在此轮响应中排名最高的进程5。
+ d）5被选为新的领导者，它广播Elected(选举完成)消息，将选举结果通知排名较低的进程。

​	这种算法的一个明显问题是，在存在网络分区的情况下，它违反了安全性保证(一次最多只能选举一个领导者)。很容易出现这样的情况：节点将被分成两个或多个独立运行的子集，每个子集选举出了这个子集的领导者。这种情况被称为**脑裂(split brain)**。

​	该算法的另一个问题是对排名较高节点有强烈的偏向性，<u>如果这些节点不稳定，可能导致算法一直处于重新选举状态</u>。一个不稳定的高排名节点提议出任领导者，此后不久发生故障，稍后又再次赢得选举，然后再次发生故障，整个过程重复进行。可以通过分发主机质量的度量并在选举时将其纳入考虑因素来解决这一问题。

## 10.2 依次故障转移

​	霸道算法有许多改进其各种性质的版本。例如，我们可以使用多个依次(next-in-line)替代进程作为故障转移候选来缩短重选过程[GHOLIPOUR09]。

​	<u>每个选举出的领导者都提供一个故障转移节点的列表。当其中一个进程检测到领导者故障时，它通过向列表中排名最高的备选进程发送消息来发起新一轮选举。如果提议的备选进程中有一个是活动的，它就会成为一个新的领导者，而不必经过整个选举回合</u>。

​	如果检测到领导者故障的进程本身是列表中排名最高的进程，则它可以立即通知其他进程新领导者的信息。

优化过的流程：

+ a）6是具有指定备选进程{5，4}的领导者，它崩溃了。3注意到此故障并联系5，它是列表中排名最高的备选进程。
+ b）5响应3，它处于活动状态，以防止它联系备选进程列表中的其他节点
+ c）5通知其他节点它是新的领导者。

​	因此，如果下一个进程仍然活着，我们在选举期间需要的步骤就更少。

## 10.3 候选节点/普通节点优化

​	另一种算法试图通过将节点分成两个子集，即**候选节点(candidate)**和**普通节点(ordinary)**，来降低所需的消息数量，只有候选节点的其中之一最终可以成为领导者[MURSHED12]。

​	普通进程通过联系候选节点来发起选举，收集它们的响应，选择排名最高的活动候选节点作为新的领导者，然后通知其余节点选举结果。

​	**为了解决同时发生多个选举的问题，该算法建议使用一个特定于进程的延迟变量δ(各进程的δ差异很大)，使得其中一个节点可以在其他节点之前发起选举**。δ通常大于消息的往返时间。高优先级的节点具有较低的δ，反之则具有较高的δ。

​	选举过程的步骤（假设1、2、6都是候选节点；3、4、5都是普通节点）：

+ a）普通进程集合里的进程4注意到领导者进程6的故障。它通过联系候选节点集合里的所有剩余进程来发起新的选举回合。
+ b）候选进程发送响应通知4它们仍然活动。
+ c）4将新的领导者2通知给所有进程。

## * 10.4 邀请算法

​	邀请算法(invitation algorithm)允许进程"邀请"其他进程加入它们的组，而不是试图超越它们的排名。这种算法从定义上就**允许多个领导者存在**，因为每个组都有自己的领导者。

​	<u>每个进程一开始都是一个新组的领导者，组内唯一的成员是这个进程本身。组领导者联系不属于该组的对等进程，邀请其加入。如果对等进程本身是领导者，则合并两个组。否则，被联系的进程会回复组领导者ID，从而让两个组的领导者以较少的步骤建立联系并合并两个组</u>。

​	邀请算法的执行步骤：

+ a）开始时，有四个进程成为各有一名成员的组的领导者。1邀请2加入它所在组，3邀请4加入它所在组。
+ b）2加入进程1的组，4加入进程3的组。1作为第一组的领导者去联系另一组的领导者3。随后，该组剩余的成员(在本例中为4)被通知关于新的组领导的信息。
+ c）两个组被合并，并且1成为扩展后组的领导者。

​	因为组被合并了，是建议组合并的进程还是另一个进程成为新的领导者都无关紧要。为了将合并组所需的消息数量保持在最低限度，较大组的领导者可以成为新组的领导者。这样，只需要把领导者变更的消息通知给较小组的进程。

​	与其他讨论过的算法类似，该算法允许各个进程处于多个组中，并允许多个领导者存在。邀请算法允许创建进程组并合并它们，而不必从头开始触发新的选举，这减少了完成选举所需的消息数量。

## 10.5 环算法

​	在环算法(ring algorithm)[CHANG79]中，系统中所有的节点形成一个环，并且知道环拓扑(即它们在环中的前驱和后继)。当进程检测到领导者故障时，它发起新的选举。选举消息沿着环向下转发：每个进程联系它的后继节点(环中离它最近的下一个节点)。如果该节点不可用，则进程跳过不可达的节点，并尝试联系环中之后的节点，直到最终有一个节点响应为止。

​	节点联系其兄弟节点，沿着环收集活动节点的集合，并在将集合传递到下一个节点之前将其自身添加到该集合中，类似于9.1.1节中描述的故障检测算法那样，节点在将其传递到下一个节点之前将其标识符附加到路径中。

​	该算法通过完全遍历环来进行。当消息返回到开始选举的节点时，从活动节点集中选择排名最高的节点作为领导者。一个遍历的例子（假设1-2-3-4-5-6～1成环，6故障后，5和1成相邻节点）：

+ a）先前的领导者6发生了故障，并且每个进程都具有该环的视图。
+ b）3通过开始遍历来发起选举。每一步中，都维护一个在路径上遍历过的节点集合。5无法达到6，所以它跳过6直接联系1。
+ c）由于5是排名最高的节点，3会发起另一轮消息传递来广播关于新领导者的信息。

​	**这种算法的变体包括收集单个排名最高的标识符，而不是一组活动节点，以节省空间**：因为max函数是可交换的，所以知道当前遍历过的节点的排名最大值就足够了。当算法返回到已经开始选举的节点时，最后已知的最高标识符会再次在环上循环传递。

​	**由于环可以划分为两个或更多的部分，因此可能出现脑裂这种不安全的情况**。

​	正如你所看到的，一个具有领导者的系统要正确行使职能，我们就需要知道目前领导者的状况(它是否还活着)，因为，要想将进程组织起来继续执行算法，领导者必须是活动且可达的。要检测领导者崩溃，我们可以使用故障检测算法(参见第9章)。

## 10.6 本章小结

​	领导者选举是分布式系统中的一个重要课题，使用特定领导者可以减少协调开销并提高算法的性能。每轮选举的成本可能很高，但由于它们不经常发生，因此不会对整个系统的性能产生负面影响。<u>单个领导者可能会成为瓶颈，但大多数时候可以通过对数据进行分区来解决(对不同分区或不同的动作使用不同的领导者)</u>。

​	<u>不幸的是，我们在本章中讨论的所有算法都容易出现**脑裂**的问题</u>：**我们可能会在独立的子网中选出两个领导者，它们彼此都不知道对方的存在**。<u>为了避免脑裂，我们必须获得整个集群范围内的多数票</u>。

​	<u>许多共识算法，包括Multi-paxos和Raft，都依赖于一个领导者来进行协调。但领导者选举不就是共识本身吗?要选举一个领导者，我们需要就其身份达成共识。如果我们能就领导者的身份达成共识，我们就能用同样的方法在其他任何事情上达成共识[ABRAHAM13]</u>。

​	一个领导者的身份可能会在其他进程不知晓的情况下发生变化，因此进程本地关于领导者的知识是否仍然有效是个问题。为了解决这个问题，<u>我们需要将领导者选举与故障检测相结合</u>。例如，稳定领导者选举(stable leader election)算法使用具有唯一稳定领导者的回合和基于超时的故障检测，以保证领导者只要不崩溃且可访问，就能维持其地位[AGUILERA01]。

​	**依赖于领导者选举的算法通常允许存在多个领导者，并试图尽可能快地解决领导者之间的冲突**。例如，对于 Multi-Paxos(参见14.3.4节)就是这样的，其中只有两个冲突的领导者(提议者)中的一个可以继续执行，这些冲突通过收集第二个Quorum的投票来解决，保证来自两个不同提议者的值不会被同时接受。

​	在Raft(参见14.4节)中，领导者可以发现其任期过时(这意味着系统中存在不同的领导者)，并将其任期更新为最新的任期。

​	在这两种情况下，拥有一个领导者是确保活动性的一种方式(如果当前的领导者发生故障，我们需要一个新的领导者)，而且其他进程不应该花费无限长的时间来了解领导者是否真的发生了故障。安全性的缺失和允许多个领导者是一种性能优化：算法可以继续进行复制阶段，而安全性则通过检测和解决冲突来保证。

​	我们将会在第14章里更深入地讨论共识和共识范畴内的领导者选举。

# 11. 复制和一致性

P187